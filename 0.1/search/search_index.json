{"config":{"indexing":"full","lang":["fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the semantic segmentation documentation project How to use this project This project can be used in 2 ways. Warning This project has only been tested with Python 3.8. First Choice A Docker image has been provided, see the Various configuration page for further details about the writing of the Dockerfile. The base image from this Dockerfile is a TensorFlow image provided by NVidia , this has the advantage to not bother you with the (quite difficult) installation of TensorFlow, CUDA, CuDNN, and other optimization softwares needed to make TensorFlow compatible with GPUs. The shell commands needed to build and run the Docker container can be found below, they are also the provided in the makefile . 1 2 3 4 5 build_docker: docker build --build-arg USER_UID = $$ ( id -u ) --build-arg USER_GID = $$ ( id -g ) --rm -f Dockerfile -t segmentation_project:v1 . run_docker: docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 -it --rm -P --mount type = bind,source = $( PWD ) ,target = /media/vorph/Datas/template_segmentation -e TF_FORCE_GPU_ALLOW_GROWTH = true -e XLA_FLAGS = '--xla_gpu_autotune_level=2' segmentation_project:v1 Note that the container rely on mounting a volume, you can see that in the run_docker command : --mount type=bind,source=$(PWD),target=/media/vorph/Datas/template_segmentation , depending on where the project is installed, you will have to : either modify the Dockerfile to copy the entirety on the project inside the container, that way you wont have to mount any volume, or to modify the address of the mounted volume to make it correspond to where you have installed it. Once you've done it, ie building and running the container, you have various requirements files provided depending of what you want to do. we'll cover these requirements files later. Second Choice If you already have a working environment with TensorFlow installed , like a virtual environement, an Azure, AWS, GCP instance, you can just install the project in this environment and use the various requirements files provided. Requirements files Warning These requirements files suppose that you already have a working environment with TensorFlow installed . Three requirements files have been provided. requirements.txt provides the minimum libraries needed to make the project work, this supposes that you do not want to make any modifications, in the files located in the src folder and you just want to use the project \"as is\". This file can be installed with the following command. 1 make install requirements-doc.txt provides the libraries needed to generate this documentation. This file, and all the files above, can be installed with the following command. 1 make install-doc requirements-dev.txt provides the libraries needed to make the project work in \"development mode\", ie you want to modify or add a file in the src folder . The libraries provided involve formatiing, linting, type hinting, refactoring, unit tests, etc. This file, and all the files above, can be installed with the following command. 1 make install-dev Steps to use it Step 0 : Create annotations for a semantic segmentation task. You first need to create labels for your semantic segmentation task. This can be done with the use of makesense.ai , once you have done the labellisation, be sure to download the annotations in the VGG json style provided by makesense. Step 1 : Put the images and annotations files in the right folder. One you have your labels and images, put them in the datas/raw_datas/images folder for the images and in the datas/raw_datas/labels folder for the VGG json file. Step 2 : Generate the segmentation masks. You need to generate masks from the raw annotations you have in your VGG json file. This is the purpose of the src/utils/utils_segmentation.py script, this script can be launched through the following shell command. 1 make segmentation_masks The prepared images and masks ready to be made into datasets will be found in the datas/raw_dataset/images and datas/raw_dataset/masks folders. Step 3 : Create train, validation, and test datasets. To generate the needed csv files to train and test a model, use the src/utils/make_dataset.py script, this script can be launched through the following shell command. 1 make prepared_dataset The csv files will be found in the datas/prepared_dataset folder. Step 4 : Train a model. To generate the configuration of the training loop, we use Hydra , see the link for further explainations. Once you're satisfied with your training loop configuration, you can launch it via the following command. 1 make train Step 5 : Track your experiments. This project use MLFlow to track the parameters of the various training loops you'll runs. To go to the mlflow web page, use the following command. 1 make mlflow","title":"Home"},{"location":"#welcome-to-the-semantic-segmentation-documentation-project","text":"","title":"Welcome to the semantic segmentation documentation project"},{"location":"#how-to-use-this-project","text":"This project can be used in 2 ways. Warning This project has only been tested with Python 3.8.","title":"How to use this project"},{"location":"#first-choice","text":"A Docker image has been provided, see the Various configuration page for further details about the writing of the Dockerfile. The base image from this Dockerfile is a TensorFlow image provided by NVidia , this has the advantage to not bother you with the (quite difficult) installation of TensorFlow, CUDA, CuDNN, and other optimization softwares needed to make TensorFlow compatible with GPUs. The shell commands needed to build and run the Docker container can be found below, they are also the provided in the makefile . 1 2 3 4 5 build_docker: docker build --build-arg USER_UID = $$ ( id -u ) --build-arg USER_GID = $$ ( id -g ) --rm -f Dockerfile -t segmentation_project:v1 . run_docker: docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 -it --rm -P --mount type = bind,source = $( PWD ) ,target = /media/vorph/Datas/template_segmentation -e TF_FORCE_GPU_ALLOW_GROWTH = true -e XLA_FLAGS = '--xla_gpu_autotune_level=2' segmentation_project:v1 Note that the container rely on mounting a volume, you can see that in the run_docker command : --mount type=bind,source=$(PWD),target=/media/vorph/Datas/template_segmentation , depending on where the project is installed, you will have to : either modify the Dockerfile to copy the entirety on the project inside the container, that way you wont have to mount any volume, or to modify the address of the mounted volume to make it correspond to where you have installed it. Once you've done it, ie building and running the container, you have various requirements files provided depending of what you want to do. we'll cover these requirements files later.","title":"First Choice"},{"location":"#second-choice","text":"If you already have a working environment with TensorFlow installed , like a virtual environement, an Azure, AWS, GCP instance, you can just install the project in this environment and use the various requirements files provided.","title":"Second Choice"},{"location":"#requirements-files","text":"Warning These requirements files suppose that you already have a working environment with TensorFlow installed . Three requirements files have been provided. requirements.txt provides the minimum libraries needed to make the project work, this supposes that you do not want to make any modifications, in the files located in the src folder and you just want to use the project \"as is\". This file can be installed with the following command. 1 make install requirements-doc.txt provides the libraries needed to generate this documentation. This file, and all the files above, can be installed with the following command. 1 make install-doc requirements-dev.txt provides the libraries needed to make the project work in \"development mode\", ie you want to modify or add a file in the src folder . The libraries provided involve formatiing, linting, type hinting, refactoring, unit tests, etc. This file, and all the files above, can be installed with the following command. 1 make install-dev","title":"Requirements files"},{"location":"#steps-to-use-it","text":"","title":"Steps to use it"},{"location":"#step-0-create-annotations-for-a-semantic-segmentation-task","text":"You first need to create labels for your semantic segmentation task. This can be done with the use of makesense.ai , once you have done the labellisation, be sure to download the annotations in the VGG json style provided by makesense.","title":"Step 0 : Create annotations for a semantic segmentation task."},{"location":"#step-1-put-the-images-and-annotations-files-in-the-right-folder","text":"One you have your labels and images, put them in the datas/raw_datas/images folder for the images and in the datas/raw_datas/labels folder for the VGG json file.","title":"Step 1 : Put the images and annotations files in the right folder."},{"location":"#step-2-generate-the-segmentation-masks","text":"You need to generate masks from the raw annotations you have in your VGG json file. This is the purpose of the src/utils/utils_segmentation.py script, this script can be launched through the following shell command. 1 make segmentation_masks The prepared images and masks ready to be made into datasets will be found in the datas/raw_dataset/images and datas/raw_dataset/masks folders.","title":"Step 2 : Generate the segmentation masks."},{"location":"#step-3-create-train-validation-and-test-datasets","text":"To generate the needed csv files to train and test a model, use the src/utils/make_dataset.py script, this script can be launched through the following shell command. 1 make prepared_dataset The csv files will be found in the datas/prepared_dataset folder.","title":"Step 3 : Create train, validation, and test datasets."},{"location":"#step-4-train-a-model","text":"To generate the configuration of the training loop, we use Hydra , see the link for further explainations. Once you're satisfied with your training loop configuration, you can launch it via the following command. 1 make train","title":"Step 4 : Train a model."},{"location":"#step-5-track-your-experiments","text":"This project use MLFlow to track the parameters of the various training loops you'll runs. To go to the mlflow web page, use the following command. 1 make mlflow","title":"Step 5 : Track your experiments."},{"location":"config_hydra/hydra/","text":"Use of Hydra to configure the training pipeline We use Hydra to compose all the parameters needed for our project. Hydra is a recently released open-source Python framework developed at Facebook AI that simplifies the development of research and other complex applications. This new framework provides a powerful ability to compose and override configuration from the command line and configuration files. The idea is that every parameters of the training pipeline of a model (the model you use, the loss you etc, the optimizer, etc) should be easy to change so that you can experiment with various configurations and select the best one. Hydra does this. Each parameter of the training pipeline is stored in a centralized way in a yaml file, once you've selected all the parameters you want for your training loop, the yaml files are hen composed to create a unique configuration file describing your training loop. To configure your training loop, you'll have to modify the yaml file called params located at configs/params.yaml . This file looks like the following one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 defaults : - backbone : mitb3 - segmentation_model : allmlp - losses : focal_loss - optimizer : cg_adam - lr_decay : cosine_decay_restart - metrics : mean_iou - datasets : datasets - pipeline : classic - training : custom_training lrd : activate : true mixed_precision : activate : false prepare : split : 0.10 seed : 42 mlflow : experiment_name : SegmentationProject run_name : ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S} log : timestamp : ${now:%Y-%m-%d_%H-%M-%S} hydra : run : dir : hydra/${now:%Y-%m-%d_%H-%M-%S} #_${hydra.job.override_dirname} # sweep: # dir: outputs/${now:%Y-%m-%d_%H-%M-%S} # subdir: ${hydra.job.override_dirname} # mlflow se charge d\u00e9j\u00e0 de faire le tracking des hyperparam\u00e8tres, inutile de l'avoir en double. # d\u00e9commenter la section au dessus pour avoir un tracking de chaque run par hydra. # hydra: # output_subdir: null Training loop configuration The defaults section is the main one, this is here that you define the parameters you'll use for your training loop. Each key ( backbone , segmentation_model , losses , etc) corresponds to a subdirectory of the configs one, the backbone directory lists all the backbones available for feature extraction, the losses one lists all the loss functions that you can use during your training loop, etc. As an example, the configuration 1 2 3 4 5 6 7 8 9 10 defaults : - backbone : mitb3 - segmentation_model : allmlp - losses : focal_loss - optimizer : cg_adam - lr_decay : cosine_decay_restart - metrics : mean_iou - datasets : datasets - pipeline : classic - training : custom_training means that for your training loop, you will use the MiT-B3 model as the backbone of your segmentation model, the segmentation head will be the allmlp one . As a loss function, you will use the focal_loss , cg_adam will be your optimizer, combined with a learning rate decay defined by cosine_decay_restart . You will monitor the mean_iou metric over the dataset datasets . The pipeline training will be the classic one and the numbers of epochs and the learning rate is defined in the custom_training . Learning rate decay 1 2 lrd : activate : true Define whether or not the learning rate decay is activated, if set to false, the learning rate used will be constant and defined as in custom_training , if set to true, the learning rate defined in custom_training will be the initial learning rate for the decay. Mixed pecision 1 2 mixed_precision : activate : false Define whether or not the mixed precision training is activated. Dataset preparation 1 2 3 prepare : split : 0.10 seed : 42 Determine the random seed and the ratio of datas used in validation and test datasets, here 10%. MLFlow experiment tracking We use MLFlow to track experiments. 1 2 3 mlflow : experiment_name : SegmentationProject run_name : ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S} Hydra logs 1 2 3 4 5 log : timestamp : ${now:%Y-%m-%d_%H-%M-%S} hydra : run : dir : hydra/${now:%Y-%m-%d_%H-%M-%S} #_${hydra.job.override_dirname Actual configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 defaults : - backbone : resnet50 - segmentation_model : fpn - losses : focal_loss - optimizer : cg_adam - lr_decay : cosine_decay_restart - metrics : mean_iou - datasets : datasets - pipeline : classic - training : custom_training start : from_saved_model : True saved_model_dir : \"hydra/2021-10-26_08-56-33/ResNet50_FPN_2021-10-26_08-56-33\" lrd : activate : true prepare : split : 0.10 seed : 42 mlflow : experiment_name : SegmentationProject run_name : ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S} log : timestamp : ${now:%Y-%m-%d_%H-%M-%S} hydra : run : dir : hydra/${now:%Y-%m-%d_%H-%M-%S} #_${hydra.job.override_dirname} # sweep: # dir: outputs/${now:%Y-%m-%d_%H-%M-%S} # subdir: ${hydra.job.override_dirname} # mlflow se charge d\u00e9j\u00e0 de faire le tracking des hyperparam\u00e8tres, inutile de l'avoir en double. # d\u00e9commenter la section au dessus pour avoir un tracking de chaque run par hydra. # hydra: # output_subdir: null","title":"Hydra configuration"},{"location":"config_hydra/hydra/#use-of-hydra-to-configure-the-training-pipeline","text":"We use Hydra to compose all the parameters needed for our project. Hydra is a recently released open-source Python framework developed at Facebook AI that simplifies the development of research and other complex applications. This new framework provides a powerful ability to compose and override configuration from the command line and configuration files. The idea is that every parameters of the training pipeline of a model (the model you use, the loss you etc, the optimizer, etc) should be easy to change so that you can experiment with various configurations and select the best one. Hydra does this. Each parameter of the training pipeline is stored in a centralized way in a yaml file, once you've selected all the parameters you want for your training loop, the yaml files are hen composed to create a unique configuration file describing your training loop. To configure your training loop, you'll have to modify the yaml file called params located at configs/params.yaml . This file looks like the following one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 defaults : - backbone : mitb3 - segmentation_model : allmlp - losses : focal_loss - optimizer : cg_adam - lr_decay : cosine_decay_restart - metrics : mean_iou - datasets : datasets - pipeline : classic - training : custom_training lrd : activate : true mixed_precision : activate : false prepare : split : 0.10 seed : 42 mlflow : experiment_name : SegmentationProject run_name : ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S} log : timestamp : ${now:%Y-%m-%d_%H-%M-%S} hydra : run : dir : hydra/${now:%Y-%m-%d_%H-%M-%S} #_${hydra.job.override_dirname} # sweep: # dir: outputs/${now:%Y-%m-%d_%H-%M-%S} # subdir: ${hydra.job.override_dirname} # mlflow se charge d\u00e9j\u00e0 de faire le tracking des hyperparam\u00e8tres, inutile de l'avoir en double. # d\u00e9commenter la section au dessus pour avoir un tracking de chaque run par hydra. # hydra: # output_subdir: null","title":"Use of Hydra to configure the training pipeline"},{"location":"config_hydra/hydra/#training-loop-configuration","text":"The defaults section is the main one, this is here that you define the parameters you'll use for your training loop. Each key ( backbone , segmentation_model , losses , etc) corresponds to a subdirectory of the configs one, the backbone directory lists all the backbones available for feature extraction, the losses one lists all the loss functions that you can use during your training loop, etc. As an example, the configuration 1 2 3 4 5 6 7 8 9 10 defaults : - backbone : mitb3 - segmentation_model : allmlp - losses : focal_loss - optimizer : cg_adam - lr_decay : cosine_decay_restart - metrics : mean_iou - datasets : datasets - pipeline : classic - training : custom_training means that for your training loop, you will use the MiT-B3 model as the backbone of your segmentation model, the segmentation head will be the allmlp one . As a loss function, you will use the focal_loss , cg_adam will be your optimizer, combined with a learning rate decay defined by cosine_decay_restart . You will monitor the mean_iou metric over the dataset datasets . The pipeline training will be the classic one and the numbers of epochs and the learning rate is defined in the custom_training .","title":"Training loop configuration"},{"location":"config_hydra/hydra/#learning-rate-decay","text":"1 2 lrd : activate : true Define whether or not the learning rate decay is activated, if set to false, the learning rate used will be constant and defined as in custom_training , if set to true, the learning rate defined in custom_training will be the initial learning rate for the decay.","title":"Learning rate decay"},{"location":"config_hydra/hydra/#mixed-pecision","text":"1 2 mixed_precision : activate : false Define whether or not the mixed precision training is activated.","title":"Mixed pecision"},{"location":"config_hydra/hydra/#dataset-preparation","text":"1 2 3 prepare : split : 0.10 seed : 42 Determine the random seed and the ratio of datas used in validation and test datasets, here 10%.","title":"Dataset preparation"},{"location":"config_hydra/hydra/#mlflow-experiment-tracking","text":"We use MLFlow to track experiments. 1 2 3 mlflow : experiment_name : SegmentationProject run_name : ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S}","title":"MLFlow experiment tracking"},{"location":"config_hydra/hydra/#hydra-logs","text":"1 2 3 4 5 log : timestamp : ${now:%Y-%m-%d_%H-%M-%S} hydra : run : dir : hydra/${now:%Y-%m-%d_%H-%M-%S} #_${hydra.job.override_dirname","title":"Hydra logs"},{"location":"config_hydra/hydra/#actual-configuration","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 defaults : - backbone : resnet50 - segmentation_model : fpn - losses : focal_loss - optimizer : cg_adam - lr_decay : cosine_decay_restart - metrics : mean_iou - datasets : datasets - pipeline : classic - training : custom_training start : from_saved_model : True saved_model_dir : \"hydra/2021-10-26_08-56-33/ResNet50_FPN_2021-10-26_08-56-33\" lrd : activate : true prepare : split : 0.10 seed : 42 mlflow : experiment_name : SegmentationProject run_name : ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S} log : timestamp : ${now:%Y-%m-%d_%H-%M-%S} hydra : run : dir : hydra/${now:%Y-%m-%d_%H-%M-%S} #_${hydra.job.override_dirname} # sweep: # dir: outputs/${now:%Y-%m-%d_%H-%M-%S} # subdir: ${hydra.job.override_dirname} # mlflow se charge d\u00e9j\u00e0 de faire le tracking des hyperparam\u00e8tres, inutile de l'avoir en double. # d\u00e9commenter la section au dessus pour avoir un tracking de chaque run par hydra. # hydra: # output_subdir: null","title":"Actual configuration"},{"location":"datasets/classic/","text":"Classic pipeline Classic pipeline for training where no other modifications except classical augmentations are made on the datasets. Tensorize Class used to create tensor datasets for TensorFlow. Inheritance object: The base class of the class hierarchy, used only to enforce WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/violations/consistency.html#consistency. Parameters: Name Type Description Default n_classes int Number of classes in the dataset. required img_shape Tuple[int,int,int] Dimension of the image, format is (H,W,C). required random_seed int Fixed random seed for reproducibility. required __init__ ( self , n_classes , img_shape , random_seed ) special Initialization of the class Tensorize. Initialize the class, the number of classes in the datasets, the shape of the images and the random seed. Source code in src/pipelines/classic.py def __init__ ( self , n_classes : int , img_shape : Tuple [ int , int , int ], random_seed : int , ) -> None : \"\"\"Initialization of the class Tensorize. Initialize the class, the number of classes in the datasets, the shape of the images and the random seed. \"\"\" self . n_classes = n_classes self . img_shape = img_shape self . random_seed = random_seed self . AUTOTUNE = tf . data . experimental . AUTOTUNE create_test_dataset ( self , data_path , batch , repet , prefetch ) Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required augment bool Does the dataset has to be augmented or no. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/classic.py def create_test_dataset ( self , data_path : str , batch : int , repet : int , prefetch : int , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. augment (bool): Does the dataset has to be augmented or no. Returns: A batch of observations and masks. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset = dataset . cache () dataset = dataset . shuffle ( len ( features ), seed = self . random_seed ) dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefetch ) create_train_dataset ( self , data_path , batch , repet , prefetch , augment ) Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required augment bool Does the dataset has to be augmented or no. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/classic.py def create_train_dataset ( self , data_path : str , batch : int , repet : int , prefetch : int , augment : bool , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. augment (bool): Does the dataset has to be augmented or no. Returns: A batch of observations and masks. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset = dataset . cache () dataset = dataset . shuffle ( len ( features ), seed = self . random_seed ) dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) if augment : dataset = dataset . map ( self . apply_augments , num_parallel_calls = self . AUTOTUNE ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefetch ) load_images ( self , data_frame , column_name ) Load the images as a list. Take the dataframe containing the observations and the masks and the return the column containing the observations as a list. Parameters: Name Type Description Default data_frame pd.DataFrame Dataframe containing the dataset. required column_name str The name of the column containing the observations. required Returns: Type Description List[str] The list of observations deduced from the dataframe. Source code in src/pipelines/classic.py def load_images ( self , data_frame : pd . DataFrame , column_name : str ) -> List [ str ]: \"\"\"Load the images as a list. Take the dataframe containing the observations and the masks and the return the column containing the observations as a list. Args: data_frame (pd.DataFrame): Dataframe containing the dataset. column_name (str): The name of the column containing the observations. Returns: The list of observations deduced from the dataframe. \"\"\" return data_frame [ column_name ] . tolist () parse_image_and_mask ( self , image , mask ) Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Parameters: Name Type Description Default filename str The path of the image to parse. required mask str The mask of the image. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] A np.ndarray corresponding to the image and the corresponding one-hot mask. Source code in src/pipelines/classic.py @tf . function def parse_image_and_mask ( self , image : str , mask : str , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Args: filename (str): The path of the image to parse. mask (str): The mask of the image. Returns: A np.ndarray corresponding to the image and the corresponding one-hot mask. \"\"\" resized_dims = [ self . img_shape [ 0 ], self . img_shape [ 1 ]] # convert the mask to one-hot encoding # decode image image = tf . io . read_file ( image ) # Don't use tf.image.decode_image, # or the output shape will be undefined image = tf . image . decode_jpeg ( image ) # This will convert to float values in [0, 1] image = tf . image . convert_image_dtype ( image , tf . float32 ) image = tf . image . resize ( image , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) mask = tf . io . read_file ( mask ) # Don't use tf.image.decode_image, # or the output shape will be undefined mask = tf . io . decode_png ( mask , channels = 1 ) mask = tf . image . resize ( mask , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) return image , mask train_preprocess ( self , image , mask ) Augmentation preprocess, if needed. Parameters: Name Type Description Default image np.ndarray The image to augment. required mask np.ndarray The corresponding mask. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] The augmented pair. Source code in src/pipelines/classic.py def train_preprocess ( self , image : np . ndarray , mask : np . ndarray , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Augmentation preprocess, if needed. Args: image (np.ndarray): The image to augment. mask (np.ndarray): The corresponding mask. Returns: The augmented pair. \"\"\" aug = A . Compose ( [ A . HorizontalFlip ( p = 0.5 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . Transpose ( p = 0.5 ), ], ) augmented = aug ( image = image , mask = mask ) image = augmented [ \"image\" ] mask = augmented [ \"mask\" ] image = tf . cast ( x = image , dtype = tf . float32 ) mask = tf . cast ( x = mask , dtype = tf . float32 ) return image , mask","title":"Classic pipeline"},{"location":"datasets/classic/#classic-pipeline","text":"Classic pipeline for training where no other modifications except classical augmentations are made on the datasets.","title":"Classic pipeline"},{"location":"datasets/classic/#src.pipelines.classic.Tensorize","text":"Class used to create tensor datasets for TensorFlow. Inheritance object: The base class of the class hierarchy, used only to enforce WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/violations/consistency.html#consistency. Parameters: Name Type Description Default n_classes int Number of classes in the dataset. required img_shape Tuple[int,int,int] Dimension of the image, format is (H,W,C). required random_seed int Fixed random seed for reproducibility. required","title":"Tensorize"},{"location":"datasets/classic/#src.pipelines.classic.Tensorize.__init__","text":"Initialization of the class Tensorize. Initialize the class, the number of classes in the datasets, the shape of the images and the random seed. Source code in src/pipelines/classic.py def __init__ ( self , n_classes : int , img_shape : Tuple [ int , int , int ], random_seed : int , ) -> None : \"\"\"Initialization of the class Tensorize. Initialize the class, the number of classes in the datasets, the shape of the images and the random seed. \"\"\" self . n_classes = n_classes self . img_shape = img_shape self . random_seed = random_seed self . AUTOTUNE = tf . data . experimental . AUTOTUNE","title":"__init__()"},{"location":"datasets/classic/#src.pipelines.classic.Tensorize.create_test_dataset","text":"Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required augment bool Does the dataset has to be augmented or no. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/classic.py def create_test_dataset ( self , data_path : str , batch : int , repet : int , prefetch : int , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. augment (bool): Does the dataset has to be augmented or no. Returns: A batch of observations and masks. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset = dataset . cache () dataset = dataset . shuffle ( len ( features ), seed = self . random_seed ) dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefetch )","title":"create_test_dataset()"},{"location":"datasets/classic/#src.pipelines.classic.Tensorize.create_train_dataset","text":"Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required augment bool Does the dataset has to be augmented or no. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/classic.py def create_train_dataset ( self , data_path : str , batch : int , repet : int , prefetch : int , augment : bool , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. augment (bool): Does the dataset has to be augmented or no. Returns: A batch of observations and masks. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset = dataset . cache () dataset = dataset . shuffle ( len ( features ), seed = self . random_seed ) dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) if augment : dataset = dataset . map ( self . apply_augments , num_parallel_calls = self . AUTOTUNE ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefetch )","title":"create_train_dataset()"},{"location":"datasets/classic/#src.pipelines.classic.Tensorize.load_images","text":"Load the images as a list. Take the dataframe containing the observations and the masks and the return the column containing the observations as a list. Parameters: Name Type Description Default data_frame pd.DataFrame Dataframe containing the dataset. required column_name str The name of the column containing the observations. required Returns: Type Description List[str] The list of observations deduced from the dataframe. Source code in src/pipelines/classic.py def load_images ( self , data_frame : pd . DataFrame , column_name : str ) -> List [ str ]: \"\"\"Load the images as a list. Take the dataframe containing the observations and the masks and the return the column containing the observations as a list. Args: data_frame (pd.DataFrame): Dataframe containing the dataset. column_name (str): The name of the column containing the observations. Returns: The list of observations deduced from the dataframe. \"\"\" return data_frame [ column_name ] . tolist ()","title":"load_images()"},{"location":"datasets/classic/#src.pipelines.classic.Tensorize.parse_image_and_mask","text":"Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Parameters: Name Type Description Default filename str The path of the image to parse. required mask str The mask of the image. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] A np.ndarray corresponding to the image and the corresponding one-hot mask. Source code in src/pipelines/classic.py @tf . function def parse_image_and_mask ( self , image : str , mask : str , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Args: filename (str): The path of the image to parse. mask (str): The mask of the image. Returns: A np.ndarray corresponding to the image and the corresponding one-hot mask. \"\"\" resized_dims = [ self . img_shape [ 0 ], self . img_shape [ 1 ]] # convert the mask to one-hot encoding # decode image image = tf . io . read_file ( image ) # Don't use tf.image.decode_image, # or the output shape will be undefined image = tf . image . decode_jpeg ( image ) # This will convert to float values in [0, 1] image = tf . image . convert_image_dtype ( image , tf . float32 ) image = tf . image . resize ( image , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) mask = tf . io . read_file ( mask ) # Don't use tf.image.decode_image, # or the output shape will be undefined mask = tf . io . decode_png ( mask , channels = 1 ) mask = tf . image . resize ( mask , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) return image , mask","title":"parse_image_and_mask()"},{"location":"datasets/classic/#src.pipelines.classic.Tensorize.train_preprocess","text":"Augmentation preprocess, if needed. Parameters: Name Type Description Default image np.ndarray The image to augment. required mask np.ndarray The corresponding mask. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] The augmented pair. Source code in src/pipelines/classic.py def train_preprocess ( self , image : np . ndarray , mask : np . ndarray , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Augmentation preprocess, if needed. Args: image (np.ndarray): The image to augment. mask (np.ndarray): The corresponding mask. Returns: The augmented pair. \"\"\" aug = A . Compose ( [ A . HorizontalFlip ( p = 0.5 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . Transpose ( p = 0.5 ), ], ) augmented = aug ( image = image , mask = mask ) image = augmented [ \"image\" ] mask = augmented [ \"mask\" ] image = tf . cast ( x = image , dtype = tf . float32 ) mask = tf . cast ( x = mask , dtype = tf . float32 ) return image , mask","title":"train_preprocess()"},{"location":"datasets/config/","text":"Hydra datasets configuration The hydra configuration file for the creation of the datasets looks like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 metadatas : n_classes : 4 # n+1 for background height : 3036 width : 4024 raw_datas : images : \"datas/raw_datas/ML/images/\" labels : \"datas/raw_datas/ML/labels/\" masks : \"datas/raw_datas/ML/masks/\" raw_dataset : crop_size : 256 images : \"datas/raw_dataset/ML/images\" masks : \"datas/raw_dataset/ML/masks\" prepared_dataset : train : \"datas/prepared_dataset/train.csv\" val : \"datas/prepared_dataset/val.csv\" test : \"datas/prepared_dataset/test.csv\" params : img_shape : [ 256 , 256 , 3 ] augment : True batch_size : 16 repetitions : 1 prefetch : 1 class_dict : Background : 0 Petri_box : 1 Moisissure : 2 Levure : 3 Metadatas The metadatas contains the informations that are inherent to the semantic segmentation task we're working on : the number of classes, the original height of the images, the original width of the images. Raw datas The raw_datas part contains the location of the directories of raw datas : the directory containing the raw, unmodified images, the directory containing the labels, ie the json files in the VGG format which contain the segemntation informations, the directory containning the raw masks, generated by the SegmentationMasks class, see next page. Raw datasets The raw_datasets part contains the location of the directories of raw datasets : the directory containing the formatted images the directory containning the formatted masks, generated by the SegmentationMasks class, see next page. Prepared datasets The prepared_datasets part contains the location of the csv's of the train, val, and test datasets. tf.Data API parameters The params part contains the parameters needed to generate a tf.data pipeline. Segmentation classes The class_dict contains the chosen integer representation of the classes of the semantic segmentation task.","title":"Dataset configuration"},{"location":"datasets/config/#hydra-datasets-configuration","text":"The hydra configuration file for the creation of the datasets looks like this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 metadatas : n_classes : 4 # n+1 for background height : 3036 width : 4024 raw_datas : images : \"datas/raw_datas/ML/images/\" labels : \"datas/raw_datas/ML/labels/\" masks : \"datas/raw_datas/ML/masks/\" raw_dataset : crop_size : 256 images : \"datas/raw_dataset/ML/images\" masks : \"datas/raw_dataset/ML/masks\" prepared_dataset : train : \"datas/prepared_dataset/train.csv\" val : \"datas/prepared_dataset/val.csv\" test : \"datas/prepared_dataset/test.csv\" params : img_shape : [ 256 , 256 , 3 ] augment : True batch_size : 16 repetitions : 1 prefetch : 1 class_dict : Background : 0 Petri_box : 1 Moisissure : 2 Levure : 3","title":"Hydra datasets configuration"},{"location":"datasets/config/#metadatas","text":"The metadatas contains the informations that are inherent to the semantic segmentation task we're working on : the number of classes, the original height of the images, the original width of the images.","title":"Metadatas"},{"location":"datasets/config/#raw-datas","text":"The raw_datas part contains the location of the directories of raw datas : the directory containing the raw, unmodified images, the directory containing the labels, ie the json files in the VGG format which contain the segemntation informations, the directory containning the raw masks, generated by the SegmentationMasks class, see next page.","title":"Raw datas"},{"location":"datasets/config/#raw-datasets","text":"The raw_datasets part contains the location of the directories of raw datasets : the directory containing the formatted images the directory containning the formatted masks, generated by the SegmentationMasks class, see next page.","title":"Raw datasets"},{"location":"datasets/config/#prepared-datasets","text":"The prepared_datasets part contains the location of the csv's of the train, val, and test datasets.","title":"Prepared datasets"},{"location":"datasets/config/#tfdata-api-parameters","text":"The params part contains the parameters needed to generate a tf.data pipeline.","title":"tf.Data API parameters"},{"location":"datasets/config/#segmentation-classes","text":"The class_dict contains the chosen integer representation of the classes of the semantic segmentation task.","title":"Segmentation classes"},{"location":"datasets/cutmix/","text":"CutMix pipeline CutMix is an image data augmentation strategy. Instead of simply removing pixels as in Cutout , we replace the removed regions with a patch from another image. The ground truth masks are also mixed proportionally. The added patches further enhance localization ability by requiring the model to identify the object from a partial view. CutMix Class used to create tensor datasets for TensorFlow via the CutMix method. Inheritance object: The base class of the class hierarchy, used only to enforce WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/violations/consistency.html#consistency. Parameters: Name Type Description Default n_classes int Number of classes in the dataset. required img_shape Tuple[int,int,int] Dimension of the image, format is (H,W,C). required random_seed int Fixed random seed for reproducibility. required random_seed1 int Fixed random seed needed for cutmix method. required random_seed2 int Fixed random seed needed for cutmix method. required __init__ ( self , n_classes , img_shape , random_seed , random_seed1 , random_seed2 ) special Initialization of the class CutMix. Initialize the class, the number of classes in the datasets, the shape of the images and the random seeds. Parameters: Name Type Description Default n_classes int Number of classes in the dataset. required img_shape Tuple[int,int,int] Dimension of the image, format is (H,W,C). required random_seed int Fixed random seed for reproducibility. required random_seed1 int Fixed random seed needed for cutmix method. required random_seed2 int Fixed random seed needed for cutmix method. required Source code in src/pipelines/cutmix.py def __init__ ( self , n_classes : int , img_shape : List [ int ], random_seed : int , random_seed1 : int , random_seed2 : int , ) -> None : \"\"\"Initialization of the class CutMix. Initialize the class, the number of classes in the datasets, the shape of the images and the random seeds. Args: n_classes (int): Number of classes in the dataset. img_shape (Tuple[int,int,int]): Dimension of the image, format is (H,W,C). random_seed (int): Fixed random seed for reproducibility. random_seed1 (int): Fixed random seed needed for cutmix method. random_seed2 (int): Fixed random seed needed for cutmix method. \"\"\" self . n_classes = n_classes self . img_shape = img_shape self . random_seed = random_seed self . random_seed1 = random_seed1 self . random_seed2 = random_seed2 self . AUTOTUNE = tf . data . experimental . AUTOTUNE assert self . img_shape [ 0 ] == self . img_shape [ 1 ] self . image_size = self . img_shape [ 0 ] apply_augments ( self , image , mask ) [summary] Parameters: Name Type Description Default image [type] [description] required mask [type] [description] required Returns: Type Description [type] [description] Source code in src/pipelines/cutmix.py @tf . function def apply_augments ( self , image : np . ndarray , mask : np . ndarray , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"[summary] Args: image ([type]): [description] mask ([type]): [description] Returns: [type]: [description] \"\"\" image , mask = tf . numpy_function ( func = self . train_preprocess , inp = [ image , mask ], Tout = [ tf . float32 , tf . float32 ], ) img_shape = [ self . img_shape [ 0 ], self . img_shape [ 1 ], 3 ] mask_shape = [ self . img_shape [ 0 ], self . img_shape [ 1 ], 1 ] image = tf . ensure_shape ( image , shape = img_shape ) mask = tf . ensure_shape ( mask , shape = mask_shape ) return image , mask create_double_dataset ( self , data_path ) Creates two datasets to be used for CutMix. Parameters: Name Type Description Default data_path str The csv file containing the train datas. required Returns: Type Description Tuple[tensorflow.python.data.ops.dataset_ops.DatasetV2, tensorflow.python.data.ops.dataset_ops.DatasetV2] Two datasets. Source code in src/pipelines/cutmix.py def create_double_dataset ( self , data_path : str , ) -> Tuple [ tf . data . Dataset , tf . data . Dataset ]: \"\"\"Creates two datasets to be used for CutMix. Args: data_path (str): The csv file containing the train datas. Returns: Two datasets. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset_one = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset_one = dataset_one . shuffle ( len ( features ), seed = self . random_seed1 ) dataset_one = dataset_one . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) dataset_two = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset_two = dataset_two . shuffle ( len ( features ), seed = self . random_seed2 ) dataset_two = dataset_two . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) return tf . data . Dataset . zip (( dataset_one , dataset_two )) create_test_dataset ( self , data_path , batch , repet , prefetch ) Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/cutmix.py def create_test_dataset ( self , data_path : str , batch : int , repet : int , prefetch : int , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. Returns: A batch of observations and masks. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset = dataset . cache () dataset = dataset . shuffle ( len ( features ), seed = self . random_seed ) dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefetch ) create_train_dataset ( self , data_path , batch , repet , augment , prefecth ) Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required augment bool Does the dataset has to be augmented or no. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/cutmix.py def create_train_dataset ( self , data_path : str , batch : int , repet : int , augment : bool , prefecth : int , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. augment (bool): Does the dataset has to be augmented or no. Returns: A batch of observations and masks. \"\"\" dataset = self . create_double_dataset ( data_path ) dataset = dataset . shuffle ( len ( dataset ), seed = self . random_seed ) dataset = dataset . cache () dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . cutmix , num_parallel_calls = self . AUTOTUNE ) if augment : dataset = dataset . map ( self . apply_augments , num_parallel_calls = self . AUTOTUNE ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefecth ) cutmix ( self , train_ds_one , train_ds_two ) Define CutMix fonction. Parameters: Name Type Description Default train_ds_one tf.data.Dataset Dataset. required train_ds_two tf.data.Dataset Dataset. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] An image and a mask augmented by the CutMix function. Source code in src/pipelines/cutmix.py @tf . function def cutmix ( self , train_ds_one : tf . data . Dataset , train_ds_two : tf . data . Dataset , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Define CutMix fonction. Args: train_ds_one (tf.data.Dataset): Dataset. train_ds_two (tf.data.Dataset): Dataset. Returns: An image and a mask augmented by the CutMix function. \"\"\" ( image1 , mask1 ), ( image2 , mask2 ) = train_ds_one , train_ds_two alpha = [ 0.25 ] beta = [ 0.25 ] # Get a sample from the Beta distribution lambda_value = self . sample_beta_distribution ( 1 , alpha , beta ) # Define Lambda lambda_value = lambda_value [ 0 ][ 0 ] # Get the bounding box offsets, heights and widths boundaryx1 , boundaryy1 , target_h , target_w = self . get_box ( lambda_value ) # Get a patch from the second image (`image2`) img_crop2 = tf . image . crop_to_bounding_box ( image2 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image2` patch (`crop2`) with the same offset image2 = tf . image . pad_to_bounding_box ( img_crop2 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Get a patch from the first image (`image1`) img_crop1 = tf . image . crop_to_bounding_box ( image1 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image1` patch (`crop1`) with the same offset img1 = tf . image . pad_to_bounding_box ( img_crop1 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Modify the first image by subtracting the patch from `image1` # (before applying the `image2` patch) image1 -= img1 # Add the modified `image1` and `image2` together to get the CutMix image image = image1 + image2 # Get a patch from the second image (`image2`) mask_crop2 = tf . image . crop_to_bounding_box ( mask2 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image2` patch (`crop2`) with the same offset mask2 = tf . image . pad_to_bounding_box ( mask_crop2 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Get a patch from the first image (`image1`) mask_crop1 = tf . image . crop_to_bounding_box ( mask1 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image1` patch (`crop1`) with the same offset msk1 = tf . image . pad_to_bounding_box ( mask_crop1 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Modify the first image by subtracting the patch from `image1` # (before applying the `image2` patch) mask1 -= msk1 # Add the modified `image1` and `image2` together to get the CutMix image mask = mask1 + mask2 return image , mask get_box ( self , lambda_value ) Given a \"combination ratio lamba_value \", define the coordinates of the bounding boxes to be cut and mixed between two images and masks. The coordinates of the bounding boxes ares in the format (x_min,y_min,height,width) and chosen such that we have the ratio : \\[ \\frac{\\text{height}\\cdot \\text{width}}{H \\cdot W} = 1- \\lambda \\] where : \\(\\lambda\\) = lambda_value , \\(H\\) , \\(W\\) are the height, width of the images/masks. that is \\(\\text{height} = H \\cdot \\sqrt{1-\\lambda}\\) , and \\(\\text{width} = W \\cdot \\sqrt{1-\\lambda}\\) . x_min is chosen by sampling uniformly in \\(\\mathrm{Unif}(0, W)\\) . y_min is chosen by sampling uniformly in \\(\\mathrm{Unif}(0, H)\\) Parameters: Name Type Description Default lambda_value float The combination ratio, has to be a real number in \\((0,1)\\) . required Returns: Type Description Tuple[int, int, int, int] The coordinates of the bounding boxes tu be modified. Coordinates in format (x_min,y_min,height,width) . Source code in src/pipelines/cutmix.py @tf . function def get_box ( self , lambda_value : float ) -> Tuple [ int , int , int , int ]: \"\"\"Given a \"combination ratio `lamba_value`\", define the coordinates of the bounding boxes to be cut and mixed between two images and masks. The coordinates of the bounding boxes ares in the format `(x_min,y_min,height,width)` and chosen such that we have the ratio : \\[ \\\\frac{\\\\text{height}\\cdot \\\\text{width}}{H \\cdot W} = 1- \\lambda \\] where : * $\\lambda$ = `lambda_value`, * $H$, $W$ are the height, width of the images/masks. that is $\\\\text{height} = H \\cdot \\sqrt{1-\\lambda}$, and $\\\\text{width} = W \\cdot \\sqrt{1-\\lambda}$. * `x_min` is chosen by sampling uniformly in $\\mathrm{Unif}(0, W)$. * `y_min` is chosen by sampling uniformly in $\\mathrm{Unif}(0, H)$ Args: lambda_value (float): The combination ratio, has to be a real number in $(0,1)$. Returns: Tuple[int, int, int, int]: The coordinates of the bounding boxes tu be modified. Coordinates in format `(x_min,y_min,height,width)`. \"\"\" # define coordinates ratio cut_rat = tf . math . sqrt ( 1.0 - lambda_value ) # define bounding box width cut_w = self . image_size * cut_rat cut_w = tf . cast ( cut_w , tf . int32 ) # define bounding box height cut_h = self . image_size * cut_rat cut_h = tf . cast ( cut_h , tf . int32 ) # define bounding box c_x cut_x = tf . random . uniform ( ( 1 ,), minval = 0 , maxval = self . image_size , dtype = tf . int32 , ) # define bounding box c_y cut_y = tf . random . uniform ( ( 1 ,), minval = 0 , maxval = self . image_size , dtype = tf . int32 , ) boundaryx1 = tf . clip_by_value ( cut_x [ 0 ] - cut_w // 2 , 0 , self . image_size , ) # x_min boundaryy1 = tf . clip_by_value ( cut_y [ 0 ] - cut_h // 2 , 0 , self . image_size , ) # y_min bbx2 = tf . clip_by_value ( cut_x [ 0 ] + cut_w // 2 , 0 , self . image_size ) # x_max bby2 = tf . clip_by_value ( cut_y [ 0 ] + cut_h // 2 , 0 , self . image_size ) # y_max target_h = bby2 - boundaryy1 # y_max - y_min if target_h == 0 : target_h += 1 target_w = bbx2 - boundaryx1 # x_max - x_min if target_w == 0 : target_w += 1 return boundaryx1 , boundaryy1 , target_h , target_w load_images ( self , data_frame , column_name ) Load the images as a list. Take the dataframe containing the observations and the labels and the return the column containing the observations as a list. Parameters: Name Type Description Default data_frame pd.DataFrame Dataframe containing the dataset. required column_name str The name of the column containing the observations. required Returns: Type Description List[str] The list of observations deduced from the dataframe. Source code in src/pipelines/cutmix.py def load_images ( self , data_frame : pd . DataFrame , column_name : str ) -> List [ str ]: \"\"\"Load the images as a list. Take the dataframe containing the observations and the labels and the return the column containing the observations as a list. Args: data_frame (pd.DataFrame): Dataframe containing the dataset. column_name (str): The name of the column containing the observations. Returns: The list of observations deduced from the dataframe. \"\"\" return data_frame [ column_name ] . tolist () parse_image_and_mask ( self , image , mask ) Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Parameters: Name Type Description Default filename str The path of the image to parse. required mask str The path of the mask of the image. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] A np.ndarray corresponding to the image and the corresponding mask. Source code in src/pipelines/cutmix.py @tf . function def parse_image_and_mask ( self , image : str , mask : str , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Args: filename (str): The path of the image to parse. mask (str): The path of the mask of the image. Returns: A np.ndarray corresponding to the image and the corresponding mask. \"\"\" resized_dims = [ self . img_shape [ 0 ], self . img_shape [ 1 ]] # decode image image = tf . io . read_file ( image ) # Don't use tf.image.decode_image, # or the output shape will be undefined image = tf . image . decode_jpeg ( image ) # This will convert to float values in [0, 1] image = tf . image . convert_image_dtype ( image , tf . float32 ) image = tf . image . resize ( image , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) mask = tf . io . read_file ( mask ) # Don't use tf.image.decode_image, # or the output shape will be undefined mask = tf . io . decode_png ( mask , channels = 1 ) mask = tf . image . resize ( mask , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) return image , mask sample_beta_distribution ( self , size , concentration0 , concentration1 ) Sample a float from Beta distribution. This is done by sampling two random variables from two gamma distributions. If \\(X\\) and \\(Y\\) are two i.i.d. random variables following Gamma laws, \\(X \\sim \\mathrm{Gamma}({\\alpha}, \\theta)\\) , and \\(Y \\simeq \\mathrm{Gamma}(\\beta, \\theta)\\) , then the random variable \\(\\frac{X}{X+Y}\\) follows the \\(\\mathrm{Beta}(\\alpha, \\beta)\\) law. Note that if \\(\\alpha=\\beta=1\\) , then \\(\\mathrm{Beta}(1,1)\\) is just the Uniform distribution \\((0,1)\\) . Parameters: Name Type Description Default size int A 1-D integer Tensor or Python array. The shape of the output samples to be drawn per alpha/beta-parameterized distribution. required concentration0 List[float] A Tensor or Python value or N-D array of type dtype. concentration0 provides the shape parameter(s) describing the gamma distribution(s) to sample. required concentration1 List[float] A Tensor or Python value or N-D array of type dtype. concentration1- provides the shape parameter(s) describing the gamma distribution(s) to sample. required Returns: Type Description float A float sampled from Beta(concentration1,concentration2) distribution. Source code in src/pipelines/cutmix.py def sample_beta_distribution ( self , size : int , concentration0 : List [ float ], concentration1 : List [ float ], ) -> float : \"\"\"Sample a float from Beta distribution. This is done by sampling two random variables from two gamma distributions. If $X$ and $Y$ are two i.i.d. random variables following Gamma laws, $X \\sim \\mathrm{Gamma}({\\\\alpha}, \\\\theta)$, and $Y \\simeq \\mathrm{Gamma}(\\\\beta, \\\\theta)$, then the random variable $\\\\frac{X}{X+Y}$ follows the $\\mathrm{Beta}(\\\\alpha, \\\\beta)$ law. Note that if $\\\\alpha=\\\\beta=1$, then $\\\\mathrm{Beta}(1,1)$ is just the Uniform distribution $(0,1)$. Args: size (int): A 1-D integer Tensor or Python array. The shape of the output samples to be drawn per alpha/beta-parameterized distribution. concentration0 (List[float]): A Tensor or Python value or N-D array of type dtype. concentration0 provides the shape parameter(s) describing the gamma distribution(s) to sample. concentration1 (List[float]): A Tensor or Python value or N-D array of type dtype. concentration1- provides the shape parameter(s) describing the gamma distribution(s) to sample. Returns: float: A float sampled from Beta(concentration1,concentration2) distribution. \"\"\" gamma1sample = tf . random . gamma ( shape = [ size ], alpha = concentration1 ) gamma2sample = tf . random . gamma ( shape = [ size ], alpha = concentration0 ) return gamma1sample / ( gamma1sample + gamma2sample ) train_preprocess ( self , image , mask ) Augmentation preprocess, if needed. Parameters: Name Type Description Default image np.ndarray The image to augment. required mask np.ndarray The corresponding mask. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] The augmented pair. Source code in src/pipelines/cutmix.py def train_preprocess ( self , image : np . ndarray , mask : np . ndarray , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Augmentation preprocess, if needed. Args: image (np.ndarray): The image to augment. mask (np.ndarray): The corresponding mask. Returns: The augmented pair. \"\"\" aug = A . Compose ( [ A . HorizontalFlip ( p = 0.5 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . Transpose ( p = 0.5 ), ], ) augmented = aug ( image = image , mask = mask ) image = augmented [ \"image\" ] mask = augmented [ \"mask\" ] image = tf . cast ( x = image , dtype = tf . float32 ) mask = tf . cast ( x = mask , dtype = tf . float32 ) return image , mask","title":"CutMix pipeline"},{"location":"datasets/cutmix/#cutmix-pipeline","text":"CutMix is an image data augmentation strategy. Instead of simply removing pixels as in Cutout , we replace the removed regions with a patch from another image. The ground truth masks are also mixed proportionally. The added patches further enhance localization ability by requiring the model to identify the object from a partial view.","title":"CutMix pipeline"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix","text":"Class used to create tensor datasets for TensorFlow via the CutMix method. Inheritance object: The base class of the class hierarchy, used only to enforce WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/violations/consistency.html#consistency. Parameters: Name Type Description Default n_classes int Number of classes in the dataset. required img_shape Tuple[int,int,int] Dimension of the image, format is (H,W,C). required random_seed int Fixed random seed for reproducibility. required random_seed1 int Fixed random seed needed for cutmix method. required random_seed2 int Fixed random seed needed for cutmix method. required","title":"CutMix"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.__init__","text":"Initialization of the class CutMix. Initialize the class, the number of classes in the datasets, the shape of the images and the random seeds. Parameters: Name Type Description Default n_classes int Number of classes in the dataset. required img_shape Tuple[int,int,int] Dimension of the image, format is (H,W,C). required random_seed int Fixed random seed for reproducibility. required random_seed1 int Fixed random seed needed for cutmix method. required random_seed2 int Fixed random seed needed for cutmix method. required Source code in src/pipelines/cutmix.py def __init__ ( self , n_classes : int , img_shape : List [ int ], random_seed : int , random_seed1 : int , random_seed2 : int , ) -> None : \"\"\"Initialization of the class CutMix. Initialize the class, the number of classes in the datasets, the shape of the images and the random seeds. Args: n_classes (int): Number of classes in the dataset. img_shape (Tuple[int,int,int]): Dimension of the image, format is (H,W,C). random_seed (int): Fixed random seed for reproducibility. random_seed1 (int): Fixed random seed needed for cutmix method. random_seed2 (int): Fixed random seed needed for cutmix method. \"\"\" self . n_classes = n_classes self . img_shape = img_shape self . random_seed = random_seed self . random_seed1 = random_seed1 self . random_seed2 = random_seed2 self . AUTOTUNE = tf . data . experimental . AUTOTUNE assert self . img_shape [ 0 ] == self . img_shape [ 1 ] self . image_size = self . img_shape [ 0 ]","title":"__init__()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.apply_augments","text":"[summary] Parameters: Name Type Description Default image [type] [description] required mask [type] [description] required Returns: Type Description [type] [description] Source code in src/pipelines/cutmix.py @tf . function def apply_augments ( self , image : np . ndarray , mask : np . ndarray , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"[summary] Args: image ([type]): [description] mask ([type]): [description] Returns: [type]: [description] \"\"\" image , mask = tf . numpy_function ( func = self . train_preprocess , inp = [ image , mask ], Tout = [ tf . float32 , tf . float32 ], ) img_shape = [ self . img_shape [ 0 ], self . img_shape [ 1 ], 3 ] mask_shape = [ self . img_shape [ 0 ], self . img_shape [ 1 ], 1 ] image = tf . ensure_shape ( image , shape = img_shape ) mask = tf . ensure_shape ( mask , shape = mask_shape ) return image , mask","title":"apply_augments()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.create_double_dataset","text":"Creates two datasets to be used for CutMix. Parameters: Name Type Description Default data_path str The csv file containing the train datas. required Returns: Type Description Tuple[tensorflow.python.data.ops.dataset_ops.DatasetV2, tensorflow.python.data.ops.dataset_ops.DatasetV2] Two datasets. Source code in src/pipelines/cutmix.py def create_double_dataset ( self , data_path : str , ) -> Tuple [ tf . data . Dataset , tf . data . Dataset ]: \"\"\"Creates two datasets to be used for CutMix. Args: data_path (str): The csv file containing the train datas. Returns: Two datasets. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset_one = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset_one = dataset_one . shuffle ( len ( features ), seed = self . random_seed1 ) dataset_one = dataset_one . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) dataset_two = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset_two = dataset_two . shuffle ( len ( features ), seed = self . random_seed2 ) dataset_two = dataset_two . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) return tf . data . Dataset . zip (( dataset_one , dataset_two ))","title":"create_double_dataset()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.create_test_dataset","text":"Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/cutmix.py def create_test_dataset ( self , data_path : str , batch : int , repet : int , prefetch : int , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. Returns: A batch of observations and masks. \"\"\" df = pd . read_csv ( data_path ) features = self . load_images ( data_frame = df , column_name = \"filename\" ) masks = self . load_images ( data_frame = df , column_name = \"mask\" ) dataset = tf . data . Dataset . from_tensor_slices (( features , masks )) dataset = dataset . cache () dataset = dataset . shuffle ( len ( features ), seed = self . random_seed ) dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . parse_image_and_mask , num_parallel_calls = self . AUTOTUNE , ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefetch )","title":"create_test_dataset()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.create_train_dataset","text":"Creation of a tensor dataset for TensorFlow. Parameters: Name Type Description Default data_path str Path where the csv file containing the dataframe is located. required batch int Batch size, usually 32. required repet int How many times the dataset has to be repeated. required prefetch int How many batch the CPU has to prepare in advance for the GPU. required augment bool Does the dataset has to be augmented or no. required Returns: Type Description DatasetV2 A batch of observations and masks. Source code in src/pipelines/cutmix.py def create_train_dataset ( self , data_path : str , batch : int , repet : int , augment : bool , prefecth : int , ) -> tf . data . Dataset : \"\"\"Creation of a tensor dataset for TensorFlow. Args: data_path (str): Path where the csv file containing the dataframe is located. batch (int): Batch size, usually 32. repet (int): How many times the dataset has to be repeated. prefetch (int): How many batch the CPU has to prepare in advance for the GPU. augment (bool): Does the dataset has to be augmented or no. Returns: A batch of observations and masks. \"\"\" dataset = self . create_double_dataset ( data_path ) dataset = dataset . shuffle ( len ( dataset ), seed = self . random_seed ) dataset = dataset . cache () dataset = dataset . repeat ( repet ) dataset = dataset . map ( self . cutmix , num_parallel_calls = self . AUTOTUNE ) if augment : dataset = dataset . map ( self . apply_augments , num_parallel_calls = self . AUTOTUNE ) dataset = dataset . batch ( batch ) return dataset . prefetch ( prefecth )","title":"create_train_dataset()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.cutmix","text":"Define CutMix fonction. Parameters: Name Type Description Default train_ds_one tf.data.Dataset Dataset. required train_ds_two tf.data.Dataset Dataset. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] An image and a mask augmented by the CutMix function. Source code in src/pipelines/cutmix.py @tf . function def cutmix ( self , train_ds_one : tf . data . Dataset , train_ds_two : tf . data . Dataset , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Define CutMix fonction. Args: train_ds_one (tf.data.Dataset): Dataset. train_ds_two (tf.data.Dataset): Dataset. Returns: An image and a mask augmented by the CutMix function. \"\"\" ( image1 , mask1 ), ( image2 , mask2 ) = train_ds_one , train_ds_two alpha = [ 0.25 ] beta = [ 0.25 ] # Get a sample from the Beta distribution lambda_value = self . sample_beta_distribution ( 1 , alpha , beta ) # Define Lambda lambda_value = lambda_value [ 0 ][ 0 ] # Get the bounding box offsets, heights and widths boundaryx1 , boundaryy1 , target_h , target_w = self . get_box ( lambda_value ) # Get a patch from the second image (`image2`) img_crop2 = tf . image . crop_to_bounding_box ( image2 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image2` patch (`crop2`) with the same offset image2 = tf . image . pad_to_bounding_box ( img_crop2 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Get a patch from the first image (`image1`) img_crop1 = tf . image . crop_to_bounding_box ( image1 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image1` patch (`crop1`) with the same offset img1 = tf . image . pad_to_bounding_box ( img_crop1 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Modify the first image by subtracting the patch from `image1` # (before applying the `image2` patch) image1 -= img1 # Add the modified `image1` and `image2` together to get the CutMix image image = image1 + image2 # Get a patch from the second image (`image2`) mask_crop2 = tf . image . crop_to_bounding_box ( mask2 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image2` patch (`crop2`) with the same offset mask2 = tf . image . pad_to_bounding_box ( mask_crop2 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Get a patch from the first image (`image1`) mask_crop1 = tf . image . crop_to_bounding_box ( mask1 , boundaryy1 , boundaryx1 , target_h , target_w , ) # Pad the `image1` patch (`crop1`) with the same offset msk1 = tf . image . pad_to_bounding_box ( mask_crop1 , boundaryy1 , boundaryx1 , self . image_size , self . image_size , ) # Modify the first image by subtracting the patch from `image1` # (before applying the `image2` patch) mask1 -= msk1 # Add the modified `image1` and `image2` together to get the CutMix image mask = mask1 + mask2 return image , mask","title":"cutmix()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.get_box","text":"Given a \"combination ratio lamba_value \", define the coordinates of the bounding boxes to be cut and mixed between two images and masks. The coordinates of the bounding boxes ares in the format (x_min,y_min,height,width) and chosen such that we have the ratio : \\[ \\frac{\\text{height}\\cdot \\text{width}}{H \\cdot W} = 1- \\lambda \\] where : \\(\\lambda\\) = lambda_value , \\(H\\) , \\(W\\) are the height, width of the images/masks. that is \\(\\text{height} = H \\cdot \\sqrt{1-\\lambda}\\) , and \\(\\text{width} = W \\cdot \\sqrt{1-\\lambda}\\) . x_min is chosen by sampling uniformly in \\(\\mathrm{Unif}(0, W)\\) . y_min is chosen by sampling uniformly in \\(\\mathrm{Unif}(0, H)\\) Parameters: Name Type Description Default lambda_value float The combination ratio, has to be a real number in \\((0,1)\\) . required Returns: Type Description Tuple[int, int, int, int] The coordinates of the bounding boxes tu be modified. Coordinates in format (x_min,y_min,height,width) . Source code in src/pipelines/cutmix.py @tf . function def get_box ( self , lambda_value : float ) -> Tuple [ int , int , int , int ]: \"\"\"Given a \"combination ratio `lamba_value`\", define the coordinates of the bounding boxes to be cut and mixed between two images and masks. The coordinates of the bounding boxes ares in the format `(x_min,y_min,height,width)` and chosen such that we have the ratio : \\[ \\\\frac{\\\\text{height}\\cdot \\\\text{width}}{H \\cdot W} = 1- \\lambda \\] where : * $\\lambda$ = `lambda_value`, * $H$, $W$ are the height, width of the images/masks. that is $\\\\text{height} = H \\cdot \\sqrt{1-\\lambda}$, and $\\\\text{width} = W \\cdot \\sqrt{1-\\lambda}$. * `x_min` is chosen by sampling uniformly in $\\mathrm{Unif}(0, W)$. * `y_min` is chosen by sampling uniformly in $\\mathrm{Unif}(0, H)$ Args: lambda_value (float): The combination ratio, has to be a real number in $(0,1)$. Returns: Tuple[int, int, int, int]: The coordinates of the bounding boxes tu be modified. Coordinates in format `(x_min,y_min,height,width)`. \"\"\" # define coordinates ratio cut_rat = tf . math . sqrt ( 1.0 - lambda_value ) # define bounding box width cut_w = self . image_size * cut_rat cut_w = tf . cast ( cut_w , tf . int32 ) # define bounding box height cut_h = self . image_size * cut_rat cut_h = tf . cast ( cut_h , tf . int32 ) # define bounding box c_x cut_x = tf . random . uniform ( ( 1 ,), minval = 0 , maxval = self . image_size , dtype = tf . int32 , ) # define bounding box c_y cut_y = tf . random . uniform ( ( 1 ,), minval = 0 , maxval = self . image_size , dtype = tf . int32 , ) boundaryx1 = tf . clip_by_value ( cut_x [ 0 ] - cut_w // 2 , 0 , self . image_size , ) # x_min boundaryy1 = tf . clip_by_value ( cut_y [ 0 ] - cut_h // 2 , 0 , self . image_size , ) # y_min bbx2 = tf . clip_by_value ( cut_x [ 0 ] + cut_w // 2 , 0 , self . image_size ) # x_max bby2 = tf . clip_by_value ( cut_y [ 0 ] + cut_h // 2 , 0 , self . image_size ) # y_max target_h = bby2 - boundaryy1 # y_max - y_min if target_h == 0 : target_h += 1 target_w = bbx2 - boundaryx1 # x_max - x_min if target_w == 0 : target_w += 1 return boundaryx1 , boundaryy1 , target_h , target_w","title":"get_box()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.load_images","text":"Load the images as a list. Take the dataframe containing the observations and the labels and the return the column containing the observations as a list. Parameters: Name Type Description Default data_frame pd.DataFrame Dataframe containing the dataset. required column_name str The name of the column containing the observations. required Returns: Type Description List[str] The list of observations deduced from the dataframe. Source code in src/pipelines/cutmix.py def load_images ( self , data_frame : pd . DataFrame , column_name : str ) -> List [ str ]: \"\"\"Load the images as a list. Take the dataframe containing the observations and the labels and the return the column containing the observations as a list. Args: data_frame (pd.DataFrame): Dataframe containing the dataset. column_name (str): The name of the column containing the observations. Returns: The list of observations deduced from the dataframe. \"\"\" return data_frame [ column_name ] . tolist ()","title":"load_images()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.parse_image_and_mask","text":"Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Parameters: Name Type Description Default filename str The path of the image to parse. required mask str The path of the mask of the image. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] A np.ndarray corresponding to the image and the corresponding mask. Source code in src/pipelines/cutmix.py @tf . function def parse_image_and_mask ( self , image : str , mask : str , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Transform image and mask. Parse image and mask to go from path to a resized np.ndarray. Args: filename (str): The path of the image to parse. mask (str): The path of the mask of the image. Returns: A np.ndarray corresponding to the image and the corresponding mask. \"\"\" resized_dims = [ self . img_shape [ 0 ], self . img_shape [ 1 ]] # decode image image = tf . io . read_file ( image ) # Don't use tf.image.decode_image, # or the output shape will be undefined image = tf . image . decode_jpeg ( image ) # This will convert to float values in [0, 1] image = tf . image . convert_image_dtype ( image , tf . float32 ) image = tf . image . resize ( image , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) mask = tf . io . read_file ( mask ) # Don't use tf.image.decode_image, # or the output shape will be undefined mask = tf . io . decode_png ( mask , channels = 1 ) mask = tf . image . resize ( mask , resized_dims , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR , ) return image , mask","title":"parse_image_and_mask()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.sample_beta_distribution","text":"Sample a float from Beta distribution. This is done by sampling two random variables from two gamma distributions. If \\(X\\) and \\(Y\\) are two i.i.d. random variables following Gamma laws, \\(X \\sim \\mathrm{Gamma}({\\alpha}, \\theta)\\) , and \\(Y \\simeq \\mathrm{Gamma}(\\beta, \\theta)\\) , then the random variable \\(\\frac{X}{X+Y}\\) follows the \\(\\mathrm{Beta}(\\alpha, \\beta)\\) law. Note that if \\(\\alpha=\\beta=1\\) , then \\(\\mathrm{Beta}(1,1)\\) is just the Uniform distribution \\((0,1)\\) . Parameters: Name Type Description Default size int A 1-D integer Tensor or Python array. The shape of the output samples to be drawn per alpha/beta-parameterized distribution. required concentration0 List[float] A Tensor or Python value or N-D array of type dtype. concentration0 provides the shape parameter(s) describing the gamma distribution(s) to sample. required concentration1 List[float] A Tensor or Python value or N-D array of type dtype. concentration1- provides the shape parameter(s) describing the gamma distribution(s) to sample. required Returns: Type Description float A float sampled from Beta(concentration1,concentration2) distribution. Source code in src/pipelines/cutmix.py def sample_beta_distribution ( self , size : int , concentration0 : List [ float ], concentration1 : List [ float ], ) -> float : \"\"\"Sample a float from Beta distribution. This is done by sampling two random variables from two gamma distributions. If $X$ and $Y$ are two i.i.d. random variables following Gamma laws, $X \\sim \\mathrm{Gamma}({\\\\alpha}, \\\\theta)$, and $Y \\simeq \\mathrm{Gamma}(\\\\beta, \\\\theta)$, then the random variable $\\\\frac{X}{X+Y}$ follows the $\\mathrm{Beta}(\\\\alpha, \\\\beta)$ law. Note that if $\\\\alpha=\\\\beta=1$, then $\\\\mathrm{Beta}(1,1)$ is just the Uniform distribution $(0,1)$. Args: size (int): A 1-D integer Tensor or Python array. The shape of the output samples to be drawn per alpha/beta-parameterized distribution. concentration0 (List[float]): A Tensor or Python value or N-D array of type dtype. concentration0 provides the shape parameter(s) describing the gamma distribution(s) to sample. concentration1 (List[float]): A Tensor or Python value or N-D array of type dtype. concentration1- provides the shape parameter(s) describing the gamma distribution(s) to sample. Returns: float: A float sampled from Beta(concentration1,concentration2) distribution. \"\"\" gamma1sample = tf . random . gamma ( shape = [ size ], alpha = concentration1 ) gamma2sample = tf . random . gamma ( shape = [ size ], alpha = concentration0 ) return gamma1sample / ( gamma1sample + gamma2sample )","title":"sample_beta_distribution()"},{"location":"datasets/cutmix/#src.pipelines.cutmix.CutMix.train_preprocess","text":"Augmentation preprocess, if needed. Parameters: Name Type Description Default image np.ndarray The image to augment. required mask np.ndarray The corresponding mask. required Returns: Type Description Tuple[numpy.ndarray, numpy.ndarray] The augmented pair. Source code in src/pipelines/cutmix.py def train_preprocess ( self , image : np . ndarray , mask : np . ndarray , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Augmentation preprocess, if needed. Args: image (np.ndarray): The image to augment. mask (np.ndarray): The corresponding mask. Returns: The augmented pair. \"\"\" aug = A . Compose ( [ A . HorizontalFlip ( p = 0.5 ), A . VerticalFlip ( p = 0.5 ), A . RandomRotate90 ( p = 0.5 ), A . Transpose ( p = 0.5 ), ], ) augmented = aug ( image = image , mask = mask ) image = augmented [ \"image\" ] mask = augmented [ \"mask\" ] image = tf . cast ( x = image , dtype = tf . float32 ) mask = tf . cast ( x = mask , dtype = tf . float32 ) return image , mask","title":"train_preprocess()"},{"location":"datasets/make_dataset/","text":"Dataset initialization create_train_val_test_datasets ( raw_images , raw_labels , test_size = 0.1 ) Creation of datasets. Create three image datasets (train, validation, and test) given raw_images and raw_labels . The first step is to gather raw_images and raw_labels in a single dataset entity, then shuffle it, this is to ensure that the dataset is already well shuffled before the before using the scikit_learn module train_test_split (for example dataset could be alphabetically sorted before the shuffling). Then dataset passes into train_test_split to first get the images_train and labels_train and an intermediate images_val and labels_val . The intermediate images_val and labels_val is then again split in half to get the actual images_val , labels_val , images_test , labels_test . Parameters: Name Type Description Default raw_images List[Path] Full list of the images used for the three datasets. required raw_labels List[str] Full list of the labels used for the three datasets. required test_size Optional[float] Ratio used in the first use of train_test_split . Defaults to split. 0.1 Returns: Type Description Tuple[List[pathlib.Path], List[pathlib.Path], List[pathlib.Path], List[pathlib.Path], List[pathlib.Path], List[pathlib.Path]] The three datasets returned as lists of images, labels. Source code in src/utils/make_dataset.py def create_train_val_test_datasets ( raw_images : List [ Path ], raw_labels : List [ Path ], test_size : Optional [ float ] = reproducibility_params . prepare . split , ) -> Datasets : \"\"\"Creation of datasets. Create three image datasets (train, validation, and test) given `raw_images` and `raw_labels`. The first step is to gather `raw_images` and `raw_labels` in a single `dataset` entity, then shuffle it, this is to ensure that the dataset is already well shuffled before the before using the `scikit_learn` module `train_test_split` (for example `dataset` could be alphabetically sorted before the shuffling). Then `dataset` passes into `train_test_split` to first get the `images_train` and `labels_train` and an intermediate `images_val` and `labels_val`. The intermediate `images_val` and `labels_val` is then again split in half to get the actual `images_val`, `labels_val`, `images_test`, `labels_test`. Args: raw_images (List[Path]): Full list of the images used for the three datasets. raw_labels (List[str]): Full list of the labels used for the three datasets. test_size (Optional[float], optional): Ratio used in the first use of `train_test_split`. Defaults to split. Returns: The three datasets returned as lists of images, labels. \"\"\" set_seed ( reproducibility_params . prepare . seed ) dataset = list ( zip ( raw_images , raw_labels )) random . shuffle ( dataset ) shuffled_images , shuffled_labels = zip ( * dataset ) images_train , images_val , labels_train , labels_val = train_test_split ( shuffled_images , shuffled_labels , test_size = test_size , random_state = reproducibility_params . prepare . seed , ) images_val , images_test , labels_val , labels_test = train_test_split ( images_val , labels_val , test_size = 0.5 , random_state = reproducibility_params . prepare . seed , ) return ( images_train , labels_train , images_val , labels_val , images_test , labels_test , ) main () Main function. Source code in src/utils/make_dataset.py @app . command () def main () -> None : \"\"\"Main function.\"\"\" images_paths = get_items_list ( directory = datasets . raw_dataset . images , extension = \".jpg\" , ) masks_paths = get_items_list ( directory = datasets . raw_dataset . masks , extension = \".png\" , ) datasets_components = create_train_val_test_datasets ( images_paths , masks_paths ) save_as_csv ( datasets_components [ 0 ], datasets_components [ 1 ], datasets . prepared_dataset . train , ) save_as_csv ( datasets_components [ 2 ], datasets_components [ 3 ], datasets . prepared_dataset . val , ) save_as_csv ( datasets_components [ 4 ], datasets_components [ 5 ], datasets . prepared_dataset . test , ) save_as_csv ( filenames , labels , destination ) Save two lists of observations, labels as a csv files. Parameters: Name Type Description Default filenames List[str] List of images addresses. required labels List[str] Liste of labels addresses. required destination Path Location of the saved csv file. required Source code in src/utils/make_dataset.py def save_as_csv ( filenames : List [ Path ], labels : List [ Path ], destination : Path ) -> None : \"\"\"Save two lists of observations, labels as a csv files. Args: filenames (List[str]): List of images addresses. labels (List[str]): Liste of labels addresses. destination (Path): Location of the saved csv file. \"\"\" logger . info ( f \"Saving dataset in { destination } .\" ) header = [ \"filename\" , \"mask\" ] with open ( destination , \"w\" , newline = \"\" ) as saved_csv : writer = csv . writer ( saved_csv , delimiter = \",\" ) writer . writerow ( header ) writer . writerows ( zip ( filenames , labels ))","title":"Train-Test-Val datasets creation"},{"location":"datasets/make_dataset/#dataset-initialization","text":"","title":"Dataset initialization"},{"location":"datasets/make_dataset/#src.utils.make_dataset.create_train_val_test_datasets","text":"Creation of datasets. Create three image datasets (train, validation, and test) given raw_images and raw_labels . The first step is to gather raw_images and raw_labels in a single dataset entity, then shuffle it, this is to ensure that the dataset is already well shuffled before the before using the scikit_learn module train_test_split (for example dataset could be alphabetically sorted before the shuffling). Then dataset passes into train_test_split to first get the images_train and labels_train and an intermediate images_val and labels_val . The intermediate images_val and labels_val is then again split in half to get the actual images_val , labels_val , images_test , labels_test . Parameters: Name Type Description Default raw_images List[Path] Full list of the images used for the three datasets. required raw_labels List[str] Full list of the labels used for the three datasets. required test_size Optional[float] Ratio used in the first use of train_test_split . Defaults to split. 0.1 Returns: Type Description Tuple[List[pathlib.Path], List[pathlib.Path], List[pathlib.Path], List[pathlib.Path], List[pathlib.Path], List[pathlib.Path]] The three datasets returned as lists of images, labels. Source code in src/utils/make_dataset.py def create_train_val_test_datasets ( raw_images : List [ Path ], raw_labels : List [ Path ], test_size : Optional [ float ] = reproducibility_params . prepare . split , ) -> Datasets : \"\"\"Creation of datasets. Create three image datasets (train, validation, and test) given `raw_images` and `raw_labels`. The first step is to gather `raw_images` and `raw_labels` in a single `dataset` entity, then shuffle it, this is to ensure that the dataset is already well shuffled before the before using the `scikit_learn` module `train_test_split` (for example `dataset` could be alphabetically sorted before the shuffling). Then `dataset` passes into `train_test_split` to first get the `images_train` and `labels_train` and an intermediate `images_val` and `labels_val`. The intermediate `images_val` and `labels_val` is then again split in half to get the actual `images_val`, `labels_val`, `images_test`, `labels_test`. Args: raw_images (List[Path]): Full list of the images used for the three datasets. raw_labels (List[str]): Full list of the labels used for the three datasets. test_size (Optional[float], optional): Ratio used in the first use of `train_test_split`. Defaults to split. Returns: The three datasets returned as lists of images, labels. \"\"\" set_seed ( reproducibility_params . prepare . seed ) dataset = list ( zip ( raw_images , raw_labels )) random . shuffle ( dataset ) shuffled_images , shuffled_labels = zip ( * dataset ) images_train , images_val , labels_train , labels_val = train_test_split ( shuffled_images , shuffled_labels , test_size = test_size , random_state = reproducibility_params . prepare . seed , ) images_val , images_test , labels_val , labels_test = train_test_split ( images_val , labels_val , test_size = 0.5 , random_state = reproducibility_params . prepare . seed , ) return ( images_train , labels_train , images_val , labels_val , images_test , labels_test , )","title":"create_train_val_test_datasets()"},{"location":"datasets/make_dataset/#src.utils.make_dataset.main","text":"Main function. Source code in src/utils/make_dataset.py @app . command () def main () -> None : \"\"\"Main function.\"\"\" images_paths = get_items_list ( directory = datasets . raw_dataset . images , extension = \".jpg\" , ) masks_paths = get_items_list ( directory = datasets . raw_dataset . masks , extension = \".png\" , ) datasets_components = create_train_val_test_datasets ( images_paths , masks_paths ) save_as_csv ( datasets_components [ 0 ], datasets_components [ 1 ], datasets . prepared_dataset . train , ) save_as_csv ( datasets_components [ 2 ], datasets_components [ 3 ], datasets . prepared_dataset . val , ) save_as_csv ( datasets_components [ 4 ], datasets_components [ 5 ], datasets . prepared_dataset . test , )","title":"main()"},{"location":"datasets/make_dataset/#src.utils.make_dataset.save_as_csv","text":"Save two lists of observations, labels as a csv files. Parameters: Name Type Description Default filenames List[str] List of images addresses. required labels List[str] Liste of labels addresses. required destination Path Location of the saved csv file. required Source code in src/utils/make_dataset.py def save_as_csv ( filenames : List [ Path ], labels : List [ Path ], destination : Path ) -> None : \"\"\"Save two lists of observations, labels as a csv files. Args: filenames (List[str]): List of images addresses. labels (List[str]): Liste of labels addresses. destination (Path): Location of the saved csv file. \"\"\" logger . info ( f \"Saving dataset in { destination } .\" ) header = [ \"filename\" , \"mask\" ] with open ( destination , \"w\" , newline = \"\" ) as saved_csv : writer = csv . writer ( saved_csv , delimiter = \",\" ) writer . writerow ( header ) writer . writerows ( zip ( filenames , labels ))","title":"save_as_csv()"},{"location":"datasets/utils_segmentation/","text":"Masks creation SegmentationMasks Description of Segmentation. Class used to generate semantic segmentation masks, and apply various operations on them. Attributes: Name Type Description segmentation_config Dict[str, Any] The loaded yaml file containing the configuration parameters for the datasets. Inheritance object: The base class of the class hierarchy, used only to enforce WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/violations/consistency.html#consistency. __init__ ( self ) special Initialization of the class. Source code in src/utils/utils_segmentation.py def __init__ ( self ): \"\"\"Initialization of the class.\"\"\" self . segmentation_config = OmegaConf . load ( \"configs/datasets/datasets.yaml\" ) crop ( self , image_path , mask_path , stride , overlap ) Given a image and a segmentation mask, generate tiles from them by cropping. Given an image and a segmentation mask, resize them and them decompose them in tiles of size \\(H=W=\\) stride . The first iteration of the loop creates non overlapping tiles, then we restart the cropping processus but this time at the coordinates ( overlap , overlap ) of the resized original images and masks, the second loop starts at (2* overlap , 2* overlap ), etc. Then save all the tiles in .jpg format for the images and .png format for the masks. Parameters: Name Type Description Default image_path Path The path of the image to open to start the tilling processus. required mask_path Path The path of the mask to open to start the tilling processus. required stride int Height, width of the cropped image, mask. required overlap int How much pixels you want to overlap between each iteration. required Source code in src/utils/utils_segmentation.py def crop ( self , image_path : Path , mask_path : Path , stride : int , overlap : int ): \"\"\"Given a image and a segmentation mask, generate tiles from them by cropping. Given an image and a segmentation mask, resize them and them decompose them in tiles of size $H=W=$`stride`. The first iteration of the loop creates non overlapping tiles, then we restart the cropping processus but this time at the coordinates (`overlap`, `overlap`) of the resized original images and masks, the second loop starts at (2*`overlap`, 2*`overlap`), etc. Then save all the tiles in `.jpg` format for the images and `.png` format for the masks. Args: image_path (Path): The path of the image to open to start the tilling processus. mask_path (Path): The path of the mask to open to start the tilling processus. stride (int): Height, width of the cropped image, mask. overlap (int): How much pixels you want to overlap between each iteration. \"\"\" image = Image . open ( image_path ) . resize (( 1024 , 1024 )) mask = Image . open ( mask_path ) . resize (( 1024 , 1024 ), resample = Image . NEAREST ) image_name = Path ( image_path ) . stem mask_name = Path ( mask_path ) . stem width , height = image . size overlap = height // overlap for mult in range ( overlap - 1 ): grid = list ( product ( range ( mult * overlap , height - height % stride , stride ), range ( mult * overlap , width - width % stride , stride ), ), ) for idy , idx in grid : box = ( idx , idy , idx + stride , idy + stride ) dir_out_image = Path ( self . segmentation_config . raw_dataset . images ) dir_out_mask = Path ( self . segmentation_config . raw_dataset . masks ) image_name_cropped = f \" { image_name } _ { idy } _ { idx }{ '.jpg' } \" mask_name_cropped = f \" { mask_name } _ { idy } _ { idx }{ '.png' } \" image_out = Path ( dir_out_image ) / Path ( image_name_cropped ) mask_out = Path ( dir_out_mask ) / Path ( mask_name_cropped ) image . crop ( box ) . save ( image_out ) mask . crop ( box ) . save ( mask_out ) logger . info ( f \"Done for { image_name } , { mask_name } with start at { mult } .\" ) generate_masks ( self ) Main function, list all json files in VGG format containig segmentation informations and generates masks. List json files. For each of them, generate segmentation masks. Then apply tilling processus on all the generated masks and images. Source code in src/utils/utils_segmentation.py def generate_masks ( self ): \"\"\"Main function, list all json files in VGG format containig segmentation informations and generates masks. 1. List json files. 2. For each of them, generate segmentation masks. 1. Then apply tilling processus on all the generated masks and images. \"\"\" json_files = get_items_list ( directory = self . segmentation_config . raw_datas . labels , extension = \".json\" , ) logger . info ( f \"Found { len ( json_files ) } json files.\" ) for json_file in json_files : self . get_masks_from_json ( json_file = json_file ) self . tile () get_data ( self , image_name , coordinates_and_labels ) Return the coordinates of polygons vertices and corresponding labels. Given an image_name , parse the segmentation_datas contained in the VGG json file all_coordinates_and_labels and return them in 3 lists : Two lists of vertices. One list of labels. The first two lists correspond to the lists of vertices of the polygons defining the segmentation regions. For each of them, an element is a list containing the coordinates ( \\(x\\) or \\(y\\) ) of the vertices of a polygon. I.e. if X_coordinates and Y_coordinates look like the following. 1 2 X_coordinates = [[ 0 , 4 , 6 ], [ 12 , 3 , 9 ]] Y_coordinates = [[ 0 , 1 , 2 ], [ 10 , 12 , 15 ]] That means we have two poygons inthe given image, each of them having three vertices (ie two triangles), the vertices of the first triangles being : \\((0,0)\\) , \\((4,1)\\) , and \\((6,2)\\) . Parameters: Name Type Description Default image_name str The name of the image for which we parse the VGG json file to collect polygons coordinates an labels. required coordinates_and_labels JsonDict The loaded VGG json file to parse. required Returns: Type Description Tuple[List[PolygonVertices], List[PolygonVertices], List[str]] The lists of polygon vertices and labels of the given image. Source code in src/utils/utils_segmentation.py def get_data ( self , image_name : str , coordinates_and_labels : JsonDict , ) -> Tuple [ List [ PolygonVertices ], List [ PolygonVertices ], List [ str ]]: \"\"\"Return the coordinates of polygons vertices and corresponding labels. Given an `image_name`, parse the `segmentation_datas` contained in the VGG json file `all_coordinates_and_labels` and return them in 3 lists : * Two lists of vertices. * One list of labels. The first two lists correspond to the lists of vertices of the polygons defining the segmentation regions. For each of them, an element is a list containing the coordinates ($x$ or $y$) of the vertices of a polygon. I.e. if `X_coordinates` and `Y_coordinates` look like the following. ```python X_coordinates = [[0,4,6], [12,3,9]] Y_coordinates = [[0,1,2], [10,12,15]] ``` That means we have two poygons inthe given image, each of them having three vertices (ie two triangles), the vertices of the first triangles being : $(0,0)$, $(4,1)$, and $(6,2)$. Args: image_name (str): The name of the image for which we parse the VGG json file to collect polygons coordinates an labels. coordinates_and_labels (JsonDict, optional): The loaded VGG json file to parse. Returns: Tuple[List[PolygonVertices], List[PolygonVertices], List[str]]: The lists of polygon vertices and labels of the given image. \"\"\" X_coordinates = [] Y_coordinates = [] labels = [] # pre-allocate lists to fill in a for loop segmentation_datas = coordinates_and_labels [ image_name ] for polygon in segmentation_datas [ \"regions\" ]: # cycle through each polygon of the given image # get the x and y points from the dictionary X_coordinates . append ( segmentation_datas [ \"regions\" ][ polygon ][ \"shape_attributes\" ][ \"all_points_x\" ], ) Y_coordinates . append ( segmentation_datas [ \"regions\" ][ polygon ][ \"shape_attributes\" ][ \"all_points_y\" ], ) # get the labels corresponding to the polygons labels . append ( segmentation_datas [ \"regions\" ][ polygon ][ \"region_attributes\" ][ \"label\" ], ) assert len ( X_coordinates ) == len ( Y_coordinates ) assert len ( Y_coordinates ) == len ( labels ) logger . info ( f \"Found { len ( labels ) } segmentation masks for { image_name } \" ) return ( Y_coordinates , X_coordinates , labels , ) # image coordinates are flipped relative to json coordinates get_masks_from_json ( self , json_file , save = True ) Given a json file containing segmentation masks information in VGG format, generate the masks. Open the json file json_file to retreive the coordinates and labels of the segmentation masks and the associated images. Then loop over each images and applys get_data and get_polygon_masks to obtain for each image a \\((P,H,W)\\) numpy array where \\(P\\) is the number of segmentation polygons and \\(H,W\\) the height and width. Apply np.max on each \\((P,H,W)\\) array to get a unique segmentation mask of size \\((H,W)\\) , then save it in .png format. Parameters: Name Type Description Default json_file [type] Json file containing segmentation masks information in VGG format. required save bool Determine if you want to save in .png format the generated mask. Defaults to True. True Source code in src/utils/utils_segmentation.py def get_masks_from_json ( self , json_file , save : bool = True , ) -> None : \"\"\"Given a json file containing segmentation masks information in VGG format, generate the masks. Open the json file `json_file` to retreive the coordinates and labels of the segmentation masks and the associated images. Then loop over each images and applys `get_data` and `get_polygon_masks` to obtain for each image a $(P,H,W)$ numpy array where $P$ is the number of segmentation polygons and $H,W$ the height and width. Apply `np.max` on each $(P,H,W)$ array to get a unique segmentation mask of size $(H,W)$, then save it in `.png` format. Args: json_file ([type]): Json file containing segmentation masks information in VGG format. save (bool, optional): Determine if you want to save in `.png` format the generated mask. Defaults to True. \"\"\" masks = [] self . json_file = json_file with open ( self . json_file ) as vgg_json : coordinates_and_labels = json . load ( vgg_json ) image_names_list = sorted ( coordinates_and_labels . keys ()) for image_name in image_names_list : X_coordinates , Y_coordinates , labels = self . get_data ( image_name , coordinates_and_labels , ) polygon_masks = self . get_polygon_masks ( X_coordinates , Y_coordinates , labels , self . segmentation_config . metadatas . height , self . segmentation_config . metadatas . width , self . segmentation_config . class_dict , ) mask = np . array ( polygon_masks ) logger . info ( f \"Made polygon masks of shape { mask . shape } .\" ) segmentation_mask = np . max ( mask , axis = 0 ) if save : image_name_stem = Path ( f \" { image_name } \" ) . stem address = Path ( self . segmentation_config . raw_datas . masks ) / Path ( f \" { image_name_stem } _mask.png\" , ) segmentation_mask = Image . fromarray ( segmentation_mask ) . convert ( \"L\" ) segmentation_mask . save ( address ) masks . append ( segmentation_mask ) assert len ( image_names_list ) == len ( masks ) get_polygon_masks ( self , X_coordinates , Y_coordinates , labels , img_height , img_width , classes ) Transforms triplets (Y_coordinates,X_coordinates,labels) into numpy arrays. given a triplet (Y_coordinates,X_coordinates,labels) for a given image, transforms it into a \\((P,H,W)\\) numpy array where \\(P\\) is the number of segmentation polygons and \\(H,W\\) the height and width. Parameters: Name Type Description Default X_coordinates PolygonVertices x-axis coordinates of the polygon vertices. required Y_coordinates PolygonVertices y-axis coordinates of the polygon vertices. required labels List[str] Labels corresponding to the polygons. required img_height int Height of the image. required img_width int Width of the image. required class_dict Dict[str, int] Man-made dictionnary to convert the labels of the VGG json file (str) into integers. required Returns: Type Description List[np.ndarray] \\((P,H,W)\\) numpy array containing the polygons. Source code in src/utils/utils_segmentation.py def get_polygon_masks ( self , X_coordinates : List [ PolygonVertices ], Y_coordinates : List [ PolygonVertices ], labels : List [ str ], img_height : int , img_width : int , classes : Dict [ str , int ], ) -> List [ np . ndarray ]: \"\"\"Transforms triplets (Y_coordinates,X_coordinates,labels) into numpy arrays. given a triplet (Y_coordinates,X_coordinates,labels) for a given image, transforms it into a $(P,H,W)$ numpy array where $P$ is the number of segmentation polygons and $H,W$ the height and width. Args: X_coordinates (PolygonVertices): x-axis coordinates of the polygon vertices. Y_coordinates (PolygonVertices): y-axis coordinates of the polygon vertices. labels (List[str]): Labels corresponding to the polygons. img_height (int): Height of the image. img_width (int): Width of the image. class_dict (Dict[str, int]): Man-made dictionnary to convert the labels of the VGG json file (str) into integers. Returns: List[np.ndarray]: $(P,H,W)$ numpy array containing the polygons. \"\"\" masks = [] for y , x , label in zip ( X_coordinates , Y_coordinates , labels ): mask = np . zeros (( img_height , img_width )) # the ImageDraw.Draw().polygon function we will use to create the mask # requires the x's and y's are interweaved, which is what the following # one-liner does polygon = np . vstack (( x , y )) . reshape (( - 1 ,), order = \"F\" ) . tolist () # create a mask image of the right size and infill according to the polygon if img_height > img_width : x , y = y , x img = Image . new ( \"L\" , ( img_height , img_width ), 0 ) elif img_width > img_height : x , y = y , x img = Image . new ( \"L\" , ( img_width , img_height ), 0 ) else : img = Image . new ( \"L\" , ( img_height , img_width ), 0 ) ImageDraw . Draw ( img ) . polygon ( polygon , outline = 0 , fill = 1 ) # turn into a numpy array m = np . flipud ( np . rot90 ( np . array ( img ))) try : mask [ m == 1 ] = classes [ label ] except Exception : mask [ m . T == 1 ] = classes [ label ] masks . append ( mask ) return masks tile ( self ) Apply tilling processus on a list of images, masks. Source code in src/utils/utils_segmentation.py def tile ( self , ): \"\"\"Apply tilling processus on a list of images, masks.\"\"\" stride = self . segmentation_config . raw_dataset . crop_size overlap = 128 logger . info ( \"Searching for images and corresponding masks.\" ) images_paths = get_items_list ( directory = self . segmentation_config . raw_datas . images , extension = \".jpg\" , ) masks_paths = get_items_list ( directory = self . segmentation_config . raw_datas . masks , extension = \".png\" , ) assert len ( images_paths ) == len ( masks_paths ) logger . info ( \"Looping through images and masks for cropping.\" ) for image_path , mask_path in zip ( images_paths , masks_paths ): self . crop ( image_path , mask_path , stride , overlap )","title":"Masks creation"},{"location":"datasets/utils_segmentation/#masks-creation","text":"","title":"Masks creation"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks","text":"Description of Segmentation. Class used to generate semantic segmentation masks, and apply various operations on them. Attributes: Name Type Description segmentation_config Dict[str, Any] The loaded yaml file containing the configuration parameters for the datasets. Inheritance object: The base class of the class hierarchy, used only to enforce WPS306. See https://wemake-python-stylegui.de/en/latest/pages/usage/violations/consistency.html#consistency.","title":"SegmentationMasks"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks.__init__","text":"Initialization of the class. Source code in src/utils/utils_segmentation.py def __init__ ( self ): \"\"\"Initialization of the class.\"\"\" self . segmentation_config = OmegaConf . load ( \"configs/datasets/datasets.yaml\" )","title":"__init__()"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks.crop","text":"Given a image and a segmentation mask, generate tiles from them by cropping. Given an image and a segmentation mask, resize them and them decompose them in tiles of size \\(H=W=\\) stride . The first iteration of the loop creates non overlapping tiles, then we restart the cropping processus but this time at the coordinates ( overlap , overlap ) of the resized original images and masks, the second loop starts at (2* overlap , 2* overlap ), etc. Then save all the tiles in .jpg format for the images and .png format for the masks. Parameters: Name Type Description Default image_path Path The path of the image to open to start the tilling processus. required mask_path Path The path of the mask to open to start the tilling processus. required stride int Height, width of the cropped image, mask. required overlap int How much pixels you want to overlap between each iteration. required Source code in src/utils/utils_segmentation.py def crop ( self , image_path : Path , mask_path : Path , stride : int , overlap : int ): \"\"\"Given a image and a segmentation mask, generate tiles from them by cropping. Given an image and a segmentation mask, resize them and them decompose them in tiles of size $H=W=$`stride`. The first iteration of the loop creates non overlapping tiles, then we restart the cropping processus but this time at the coordinates (`overlap`, `overlap`) of the resized original images and masks, the second loop starts at (2*`overlap`, 2*`overlap`), etc. Then save all the tiles in `.jpg` format for the images and `.png` format for the masks. Args: image_path (Path): The path of the image to open to start the tilling processus. mask_path (Path): The path of the mask to open to start the tilling processus. stride (int): Height, width of the cropped image, mask. overlap (int): How much pixels you want to overlap between each iteration. \"\"\" image = Image . open ( image_path ) . resize (( 1024 , 1024 )) mask = Image . open ( mask_path ) . resize (( 1024 , 1024 ), resample = Image . NEAREST ) image_name = Path ( image_path ) . stem mask_name = Path ( mask_path ) . stem width , height = image . size overlap = height // overlap for mult in range ( overlap - 1 ): grid = list ( product ( range ( mult * overlap , height - height % stride , stride ), range ( mult * overlap , width - width % stride , stride ), ), ) for idy , idx in grid : box = ( idx , idy , idx + stride , idy + stride ) dir_out_image = Path ( self . segmentation_config . raw_dataset . images ) dir_out_mask = Path ( self . segmentation_config . raw_dataset . masks ) image_name_cropped = f \" { image_name } _ { idy } _ { idx }{ '.jpg' } \" mask_name_cropped = f \" { mask_name } _ { idy } _ { idx }{ '.png' } \" image_out = Path ( dir_out_image ) / Path ( image_name_cropped ) mask_out = Path ( dir_out_mask ) / Path ( mask_name_cropped ) image . crop ( box ) . save ( image_out ) mask . crop ( box ) . save ( mask_out ) logger . info ( f \"Done for { image_name } , { mask_name } with start at { mult } .\" )","title":"crop()"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks.generate_masks","text":"Main function, list all json files in VGG format containig segmentation informations and generates masks. List json files. For each of them, generate segmentation masks. Then apply tilling processus on all the generated masks and images. Source code in src/utils/utils_segmentation.py def generate_masks ( self ): \"\"\"Main function, list all json files in VGG format containig segmentation informations and generates masks. 1. List json files. 2. For each of them, generate segmentation masks. 1. Then apply tilling processus on all the generated masks and images. \"\"\" json_files = get_items_list ( directory = self . segmentation_config . raw_datas . labels , extension = \".json\" , ) logger . info ( f \"Found { len ( json_files ) } json files.\" ) for json_file in json_files : self . get_masks_from_json ( json_file = json_file ) self . tile ()","title":"generate_masks()"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks.get_data","text":"Return the coordinates of polygons vertices and corresponding labels. Given an image_name , parse the segmentation_datas contained in the VGG json file all_coordinates_and_labels and return them in 3 lists : Two lists of vertices. One list of labels. The first two lists correspond to the lists of vertices of the polygons defining the segmentation regions. For each of them, an element is a list containing the coordinates ( \\(x\\) or \\(y\\) ) of the vertices of a polygon. I.e. if X_coordinates and Y_coordinates look like the following. 1 2 X_coordinates = [[ 0 , 4 , 6 ], [ 12 , 3 , 9 ]] Y_coordinates = [[ 0 , 1 , 2 ], [ 10 , 12 , 15 ]] That means we have two poygons inthe given image, each of them having three vertices (ie two triangles), the vertices of the first triangles being : \\((0,0)\\) , \\((4,1)\\) , and \\((6,2)\\) . Parameters: Name Type Description Default image_name str The name of the image for which we parse the VGG json file to collect polygons coordinates an labels. required coordinates_and_labels JsonDict The loaded VGG json file to parse. required Returns: Type Description Tuple[List[PolygonVertices], List[PolygonVertices], List[str]] The lists of polygon vertices and labels of the given image. Source code in src/utils/utils_segmentation.py def get_data ( self , image_name : str , coordinates_and_labels : JsonDict , ) -> Tuple [ List [ PolygonVertices ], List [ PolygonVertices ], List [ str ]]: \"\"\"Return the coordinates of polygons vertices and corresponding labels. Given an `image_name`, parse the `segmentation_datas` contained in the VGG json file `all_coordinates_and_labels` and return them in 3 lists : * Two lists of vertices. * One list of labels. The first two lists correspond to the lists of vertices of the polygons defining the segmentation regions. For each of them, an element is a list containing the coordinates ($x$ or $y$) of the vertices of a polygon. I.e. if `X_coordinates` and `Y_coordinates` look like the following. ```python X_coordinates = [[0,4,6], [12,3,9]] Y_coordinates = [[0,1,2], [10,12,15]] ``` That means we have two poygons inthe given image, each of them having three vertices (ie two triangles), the vertices of the first triangles being : $(0,0)$, $(4,1)$, and $(6,2)$. Args: image_name (str): The name of the image for which we parse the VGG json file to collect polygons coordinates an labels. coordinates_and_labels (JsonDict, optional): The loaded VGG json file to parse. Returns: Tuple[List[PolygonVertices], List[PolygonVertices], List[str]]: The lists of polygon vertices and labels of the given image. \"\"\" X_coordinates = [] Y_coordinates = [] labels = [] # pre-allocate lists to fill in a for loop segmentation_datas = coordinates_and_labels [ image_name ] for polygon in segmentation_datas [ \"regions\" ]: # cycle through each polygon of the given image # get the x and y points from the dictionary X_coordinates . append ( segmentation_datas [ \"regions\" ][ polygon ][ \"shape_attributes\" ][ \"all_points_x\" ], ) Y_coordinates . append ( segmentation_datas [ \"regions\" ][ polygon ][ \"shape_attributes\" ][ \"all_points_y\" ], ) # get the labels corresponding to the polygons labels . append ( segmentation_datas [ \"regions\" ][ polygon ][ \"region_attributes\" ][ \"label\" ], ) assert len ( X_coordinates ) == len ( Y_coordinates ) assert len ( Y_coordinates ) == len ( labels ) logger . info ( f \"Found { len ( labels ) } segmentation masks for { image_name } \" ) return ( Y_coordinates , X_coordinates , labels , ) # image coordinates are flipped relative to json coordinates","title":"get_data()"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks.get_masks_from_json","text":"Given a json file containing segmentation masks information in VGG format, generate the masks. Open the json file json_file to retreive the coordinates and labels of the segmentation masks and the associated images. Then loop over each images and applys get_data and get_polygon_masks to obtain for each image a \\((P,H,W)\\) numpy array where \\(P\\) is the number of segmentation polygons and \\(H,W\\) the height and width. Apply np.max on each \\((P,H,W)\\) array to get a unique segmentation mask of size \\((H,W)\\) , then save it in .png format. Parameters: Name Type Description Default json_file [type] Json file containing segmentation masks information in VGG format. required save bool Determine if you want to save in .png format the generated mask. Defaults to True. True Source code in src/utils/utils_segmentation.py def get_masks_from_json ( self , json_file , save : bool = True , ) -> None : \"\"\"Given a json file containing segmentation masks information in VGG format, generate the masks. Open the json file `json_file` to retreive the coordinates and labels of the segmentation masks and the associated images. Then loop over each images and applys `get_data` and `get_polygon_masks` to obtain for each image a $(P,H,W)$ numpy array where $P$ is the number of segmentation polygons and $H,W$ the height and width. Apply `np.max` on each $(P,H,W)$ array to get a unique segmentation mask of size $(H,W)$, then save it in `.png` format. Args: json_file ([type]): Json file containing segmentation masks information in VGG format. save (bool, optional): Determine if you want to save in `.png` format the generated mask. Defaults to True. \"\"\" masks = [] self . json_file = json_file with open ( self . json_file ) as vgg_json : coordinates_and_labels = json . load ( vgg_json ) image_names_list = sorted ( coordinates_and_labels . keys ()) for image_name in image_names_list : X_coordinates , Y_coordinates , labels = self . get_data ( image_name , coordinates_and_labels , ) polygon_masks = self . get_polygon_masks ( X_coordinates , Y_coordinates , labels , self . segmentation_config . metadatas . height , self . segmentation_config . metadatas . width , self . segmentation_config . class_dict , ) mask = np . array ( polygon_masks ) logger . info ( f \"Made polygon masks of shape { mask . shape } .\" ) segmentation_mask = np . max ( mask , axis = 0 ) if save : image_name_stem = Path ( f \" { image_name } \" ) . stem address = Path ( self . segmentation_config . raw_datas . masks ) / Path ( f \" { image_name_stem } _mask.png\" , ) segmentation_mask = Image . fromarray ( segmentation_mask ) . convert ( \"L\" ) segmentation_mask . save ( address ) masks . append ( segmentation_mask ) assert len ( image_names_list ) == len ( masks )","title":"get_masks_from_json()"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks.get_polygon_masks","text":"Transforms triplets (Y_coordinates,X_coordinates,labels) into numpy arrays. given a triplet (Y_coordinates,X_coordinates,labels) for a given image, transforms it into a \\((P,H,W)\\) numpy array where \\(P\\) is the number of segmentation polygons and \\(H,W\\) the height and width. Parameters: Name Type Description Default X_coordinates PolygonVertices x-axis coordinates of the polygon vertices. required Y_coordinates PolygonVertices y-axis coordinates of the polygon vertices. required labels List[str] Labels corresponding to the polygons. required img_height int Height of the image. required img_width int Width of the image. required class_dict Dict[str, int] Man-made dictionnary to convert the labels of the VGG json file (str) into integers. required Returns: Type Description List[np.ndarray] \\((P,H,W)\\) numpy array containing the polygons. Source code in src/utils/utils_segmentation.py def get_polygon_masks ( self , X_coordinates : List [ PolygonVertices ], Y_coordinates : List [ PolygonVertices ], labels : List [ str ], img_height : int , img_width : int , classes : Dict [ str , int ], ) -> List [ np . ndarray ]: \"\"\"Transforms triplets (Y_coordinates,X_coordinates,labels) into numpy arrays. given a triplet (Y_coordinates,X_coordinates,labels) for a given image, transforms it into a $(P,H,W)$ numpy array where $P$ is the number of segmentation polygons and $H,W$ the height and width. Args: X_coordinates (PolygonVertices): x-axis coordinates of the polygon vertices. Y_coordinates (PolygonVertices): y-axis coordinates of the polygon vertices. labels (List[str]): Labels corresponding to the polygons. img_height (int): Height of the image. img_width (int): Width of the image. class_dict (Dict[str, int]): Man-made dictionnary to convert the labels of the VGG json file (str) into integers. Returns: List[np.ndarray]: $(P,H,W)$ numpy array containing the polygons. \"\"\" masks = [] for y , x , label in zip ( X_coordinates , Y_coordinates , labels ): mask = np . zeros (( img_height , img_width )) # the ImageDraw.Draw().polygon function we will use to create the mask # requires the x's and y's are interweaved, which is what the following # one-liner does polygon = np . vstack (( x , y )) . reshape (( - 1 ,), order = \"F\" ) . tolist () # create a mask image of the right size and infill according to the polygon if img_height > img_width : x , y = y , x img = Image . new ( \"L\" , ( img_height , img_width ), 0 ) elif img_width > img_height : x , y = y , x img = Image . new ( \"L\" , ( img_width , img_height ), 0 ) else : img = Image . new ( \"L\" , ( img_height , img_width ), 0 ) ImageDraw . Draw ( img ) . polygon ( polygon , outline = 0 , fill = 1 ) # turn into a numpy array m = np . flipud ( np . rot90 ( np . array ( img ))) try : mask [ m == 1 ] = classes [ label ] except Exception : mask [ m . T == 1 ] = classes [ label ] masks . append ( mask ) return masks","title":"get_polygon_masks()"},{"location":"datasets/utils_segmentation/#src.utils.utils_segmentation.SegmentationMasks.tile","text":"Apply tilling processus on a list of images, masks. Source code in src/utils/utils_segmentation.py def tile ( self , ): \"\"\"Apply tilling processus on a list of images, masks.\"\"\" stride = self . segmentation_config . raw_dataset . crop_size overlap = 128 logger . info ( \"Searching for images and corresponding masks.\" ) images_paths = get_items_list ( directory = self . segmentation_config . raw_datas . images , extension = \".jpg\" , ) masks_paths = get_items_list ( directory = self . segmentation_config . raw_datas . masks , extension = \".png\" , ) assert len ( images_paths ) == len ( masks_paths ) logger . info ( \"Looping through images and masks for cropping.\" ) for image_path , mask_path in zip ( images_paths , masks_paths ): self . crop ( image_path , mask_path , stride , overlap )","title":"tile()"},{"location":"loss/focal_loss/","text":"La perte focale Multiclass focal loss implementation. SparseCategoricalFocalLoss ( Loss ) Focal loss function for multiclass classification with integer labels. This loss function generalizes multiclass softmax cross-entropy by introducing a hyperparameter \\(\\gamma\\) (gamma), called the focusing parameter , that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples. This class is a wrapper around focal_loss.sparse_categorical_focal_loss . See the documentation there for details about this loss function. Examples: An instance of this class is a callable that takes a rank-one tensor of integer class labels y_true and a tensor of model predictions y_pred and returns a scalar tensor obtained by reducing the per-example focal loss (the default reduction is a batch-wise average). >>> from focal_loss import SparseCategoricalFocalLoss >>> loss_func = SparseCategoricalFocalLoss ( gamma = 2 ) >>> y_true = [ 0 , 1 , 2 ] >>> y_pred = [[ 0.8 , 0.1 , 0.1 ], [ 0.2 , 0.7 , 0.1 ], [ 0.2 , 0.2 , 0.6 ]] >>> loss_func ( y_true , y_pred ) < tf . Tensor : shape = (), dtype = float32 , numpy = 0.040919524 > Use this class in the tf.keras API like any other multiclass classification loss function class that accepts integer labels found in tf.keras.losses (e.g., tf.keras.losses.SparseCategoricalCrossentropy : 1 2 3 4 5 6 7 8 # Typical usage model = tf . keras . Model ( ... ) model . compile ( optimizer =... , loss = SparseCategoricalFocalLoss ( gamma = 2 ), # Used here like a tf.keras loss metrics =... , ) history = model . fit ( ... ) call ( self , y_true , y_pred ) Invokes the Loss instance. Parameters: Name Type Description Default y_true Tensor Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred Tensor The predicted values. shape = [batch_size, d0, .. dN] required Returns: Type Description Loss values with the shape [batch_size, d0, .. dN-1] . Source code in src/losses/focal_loss.py def call ( self , y_true : tf . Tensor , y_pred : tf . Tensor ): return sparse_categorical_focal_loss ( y_true = y_true , y_pred = y_pred , class_weight = self . class_weight , alpha = self . alpha , gamma = self . gamma , from_logits = self . from_logits , ) get_config ( self ) Returns the config dictionary for a Loss instance. Source code in src/losses/focal_loss.py def get_config ( self ): config = super () . get_config () config . update ( gamma = self . gamma , alpha = self . alpha , class_weight = self . class_weight , from_logits = self . from_logits , ) return config sparse_categorical_focal_loss ( y_true , y_pred , alpha , gamma , * , class_weight = None , from_logits = False , axis =- 1 ) Focal loss function for multiclass classification with integer labels. This loss function generalizes multiclass softmax cross-entropy by introducing a hyperparameter called the focusing parameter that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples. In the multiclass setting, with integer labels \\(y\\) , focal loss is defined as \\[ L(y, \\hat{\\mathbf{p}}) = -(1 - \\hat{p}_y )^\\gamma \\log(\\hat{p}_y) \\] where \\(y \\in \\{0, \\ldots, K - 1\\}\\) is an integer class label ( \\(K\\) denotes the number of classes), \\(\\hat{\\mathbf{p}} = (\\hat{p}_0, \\ldots, \\hat{p}_{K-1}) \\in [0, 1]^K\\) is a vector representing an estimated probability distribution over the \\(K\\) classes, \\(\\gamma\\) (gamma, not \\(y\\) ) is the focusing parameter that specifies how much higher-confidence correct predictions contribute to the overall loss (the higher the \\(\\gamma\\) , the higher the rate at which easy-to-classify examples are down-weighted). The usual multiclass softmax cross-entropy loss is recovered by setting \\(\\gamma = 0\\) . Parameters: Name Type Description Default y_true tf.Tensor Integer class labels. required y_pred tf.Tensor Either probabilities or logits, depending on the from_logits parameter. required alpha float required gamma float The focusing parameter \\(\\gamma\\) . Higher values of gamma make easy-to-classify examples contribute less to the loss relative to hard-to-classify examples. Must be non-negative. This can be a one-dimensional tensor, in which case it specifies a focusing parameter for each class. required class_weight Optional[Any] Weighting factor for each of the \\(k\\) classes. If not specified, then all classes are weighted equally. Defaults to None. None from_logits bool Whether y_pred contains logits or probabilities.. Defaults to False. False axis int Channel axis in the y_pred tensor.. Defaults to -1. -1 Returns: Type Description tf.Tensor The focal loss for each example. Examples: This function computes the per-example focal loss between a one-dimensional integer label vector and a two-dimensional prediction matrix: >>> import numpy as np >>> from focal_loss import sparse_categorical_focal_loss >>> y_true = [ 0 , 1 , 2 ] >>> y_pred = [[ 0.8 , 0.1 , 0.1 ], [ 0.2 , 0.7 , 0.1 ], [ 0.2 , 0.2 , 0.6 ]] >>> loss = sparse_categorical_focal_loss ( y_true , y_pred , gamma = 2 ) >>> np . set_printoptions ( precision = 3 ) >>> print ( loss . numpy ()) [ 0.009 0.032 0.082 ] Warnings This function does not reduce its output to a scalar, so it cannot be passed to tf.keras.Model.compile as a loss argument. Instead, use the wrapper class focal_loss.SparseCategoricalFocalLoss . References T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. ( DOI <https://doi.org/10.1109/TPAMI.2018.2858826> ) ( arXiv preprint <https://arxiv.org/abs/1708.02002> ) Source code in src/losses/focal_loss.py def sparse_categorical_focal_loss ( y_true : tf . Tensor , y_pred : tf . Tensor , alpha : float , gamma : float , * , class_weight : Optional [ Any ] = None , from_logits : bool = False , axis : int = - 1 , ) -> tf . Tensor : \"\"\" Focal loss function for multiclass classification with integer labels. This loss function generalizes multiclass softmax cross-entropy by introducing a hyperparameter called the *focusing parameter* that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples. In the multiclass setting, with integer labels $y$, focal loss is defined as \\[ L(y, \\hat{\\mathbf{p}}) = -(1 - \\hat{p}_y )^\\gamma \\log(\\hat{p}_y) \\] where * $y \\in \\{0, \\ldots, K - 1\\}$ is an integer class label ($K$ denotes the number of classes), * $\\hat{\\mathbf{p}} = (\\hat{p}_0, \\ldots, \\hat{p}_{K-1}) \\in [0, 1]^K$ is a vector representing an estimated probability distribution over the $K$ classes, * $\\gamma$ (gamma, not $y$) is the *focusing parameter* that specifies how much higher-confidence correct predictions contribute to the overall loss (the higher the $\\gamma$, the higher the rate at which easy-to-classify examples are down-weighted). The usual multiclass softmax cross-entropy loss is recovered by setting $\\gamma = 0$. Args: y_true (tf.Tensor): Integer class labels. y_pred (tf.Tensor): Either probabilities or logits, depending on the `from_logits` parameter. alpha (float): gamma (float): The focusing parameter $\\gamma$. Higher values of `gamma` make easy-to-classify examples contribute less to the loss relative to hard-to-classify examples. Must be non-negative. This can be a one-dimensional tensor, in which case it specifies a focusing parameter for each class. class_weight (Optional[Any], optional): Weighting factor for each of the $k$ classes. If not specified, then all classes are weighted equally. Defaults to None. from_logits (bool, optional): Whether `y_pred` contains logits or probabilities.. Defaults to False. axis (int, optional): Channel axis in the `y_pred` tensor.. Defaults to -1. Returns: tf.Tensor: The focal loss for each example. Examples: This function computes the per-example focal loss between a one-dimensional integer label vector and a two-dimensional prediction matrix: >>> import numpy as np >>> from focal_loss import sparse_categorical_focal_loss >>> y_true = [0, 1, 2] >>> y_pred = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]] >>> loss = sparse_categorical_focal_loss(y_true, y_pred, gamma=2) >>> np.set_printoptions(precision=3) >>> print(loss.numpy()) [0.009 0.032 0.082] Warnings: This function does not reduce its output to a scalar, so it cannot be passed to `tf.keras.Model.compile` as a `loss` argument. Instead, use the wrapper class `focal_loss.SparseCategoricalFocalLoss`. References: T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. (`DOI <https://doi.org/10.1109/TPAMI.2018.2858826>`__) (`arXiv preprint <https://arxiv.org/abs/1708.02002>`__) \"\"\" # Process focusing parameter gamma = tf . convert_to_tensor ( gamma , dtype = tf . dtypes . float32 ) gamma_rank = gamma . shape . rank scalar_gamma = gamma_rank == 0 # Process class weight if class_weight is not None : class_weight = tf . convert_to_tensor ( class_weight , dtype = tf . dtypes . float32 ) # Process prediction tensor y_pred = tf . convert_to_tensor ( y_pred ) y_pred_rank = y_pred . shape . rank if y_pred_rank is not None : axis %= y_pred_rank if axis != y_pred_rank - 1 : # Put channel axis last for sparse_softmax_cross_entropy_with_logits perm = list ( itertools . chain ( range ( axis ), range ( axis + 1 , y_pred_rank ), [ axis ]), ) y_pred = tf . transpose ( y_pred , perm = perm ) elif axis != - 1 : raise ValueError ( f \"Cannot compute sparse categorical focal loss with axis= { axis } on \" \"a prediction tensor with statically unknown rank.\" , ) y_pred_shape = tf . shape ( y_pred ) # Process ground truth tensor y_true = tf . dtypes . cast ( y_true , dtype = tf . dtypes . int64 ) y_true_rank = y_true . shape . rank if y_true_rank is None : raise NotImplementedError ( \"Sparse categorical focal loss not supported \" \"for target/label tensors of unknown rank\" , ) reshape_needed = ( y_true_rank is not None and y_pred_rank is not None and y_pred_rank != y_true_rank + 1 ) if reshape_needed : y_true = tf . reshape ( y_true , [ - 1 ]) y_pred = tf . reshape ( y_pred , [ - 1 , y_pred_shape [ - 1 ]]) if from_logits : logits = y_pred probs = tf . nn . softmax ( y_pred , axis =- 1 ) else : probs = y_pred logits = tf . math . log ( tf . clip_by_value ( y_pred , _EPSILON , 1 - _EPSILON )) xent_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = y_true , logits = logits , ) y_true_rank = y_true . shape . rank probs = tf . gather ( probs , y_true , axis =- 1 , batch_dims = y_true_rank ) if not scalar_gamma : gamma = tf . gather ( gamma , y_true , axis = 0 , batch_dims = y_true_rank ) focal_modulation = ( 1 - probs ) ** gamma loss = alpha * focal_modulation * xent_loss if class_weight is not None : class_weight = tf . gather ( class_weight , y_true , axis = 0 , batch_dims = y_true_rank ) loss *= class_weight if reshape_needed : loss = tf . reshape ( loss , y_pred_shape [: - 1 ]) return loss","title":"Loss"},{"location":"loss/focal_loss/#la-perte-focale","text":"Multiclass focal loss implementation.","title":"La perte focale"},{"location":"loss/focal_loss/#src.losses.focal_loss.SparseCategoricalFocalLoss","text":"Focal loss function for multiclass classification with integer labels. This loss function generalizes multiclass softmax cross-entropy by introducing a hyperparameter \\(\\gamma\\) (gamma), called the focusing parameter , that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples. This class is a wrapper around focal_loss.sparse_categorical_focal_loss . See the documentation there for details about this loss function. Examples: An instance of this class is a callable that takes a rank-one tensor of integer class labels y_true and a tensor of model predictions y_pred and returns a scalar tensor obtained by reducing the per-example focal loss (the default reduction is a batch-wise average). >>> from focal_loss import SparseCategoricalFocalLoss >>> loss_func = SparseCategoricalFocalLoss ( gamma = 2 ) >>> y_true = [ 0 , 1 , 2 ] >>> y_pred = [[ 0.8 , 0.1 , 0.1 ], [ 0.2 , 0.7 , 0.1 ], [ 0.2 , 0.2 , 0.6 ]] >>> loss_func ( y_true , y_pred ) < tf . Tensor : shape = (), dtype = float32 , numpy = 0.040919524 > Use this class in the tf.keras API like any other multiclass classification loss function class that accepts integer labels found in tf.keras.losses (e.g., tf.keras.losses.SparseCategoricalCrossentropy : 1 2 3 4 5 6 7 8 # Typical usage model = tf . keras . Model ( ... ) model . compile ( optimizer =... , loss = SparseCategoricalFocalLoss ( gamma = 2 ), # Used here like a tf.keras loss metrics =... , ) history = model . fit ( ... )","title":"SparseCategoricalFocalLoss"},{"location":"loss/focal_loss/#src.losses.focal_loss.SparseCategoricalFocalLoss.call","text":"Invokes the Loss instance. Parameters: Name Type Description Default y_true Tensor Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred Tensor The predicted values. shape = [batch_size, d0, .. dN] required Returns: Type Description Loss values with the shape [batch_size, d0, .. dN-1] . Source code in src/losses/focal_loss.py def call ( self , y_true : tf . Tensor , y_pred : tf . Tensor ): return sparse_categorical_focal_loss ( y_true = y_true , y_pred = y_pred , class_weight = self . class_weight , alpha = self . alpha , gamma = self . gamma , from_logits = self . from_logits , )","title":"call()"},{"location":"loss/focal_loss/#src.losses.focal_loss.SparseCategoricalFocalLoss.get_config","text":"Returns the config dictionary for a Loss instance. Source code in src/losses/focal_loss.py def get_config ( self ): config = super () . get_config () config . update ( gamma = self . gamma , alpha = self . alpha , class_weight = self . class_weight , from_logits = self . from_logits , ) return config","title":"get_config()"},{"location":"loss/focal_loss/#src.losses.focal_loss.sparse_categorical_focal_loss","text":"Focal loss function for multiclass classification with integer labels. This loss function generalizes multiclass softmax cross-entropy by introducing a hyperparameter called the focusing parameter that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples. In the multiclass setting, with integer labels \\(y\\) , focal loss is defined as \\[ L(y, \\hat{\\mathbf{p}}) = -(1 - \\hat{p}_y )^\\gamma \\log(\\hat{p}_y) \\] where \\(y \\in \\{0, \\ldots, K - 1\\}\\) is an integer class label ( \\(K\\) denotes the number of classes), \\(\\hat{\\mathbf{p}} = (\\hat{p}_0, \\ldots, \\hat{p}_{K-1}) \\in [0, 1]^K\\) is a vector representing an estimated probability distribution over the \\(K\\) classes, \\(\\gamma\\) (gamma, not \\(y\\) ) is the focusing parameter that specifies how much higher-confidence correct predictions contribute to the overall loss (the higher the \\(\\gamma\\) , the higher the rate at which easy-to-classify examples are down-weighted). The usual multiclass softmax cross-entropy loss is recovered by setting \\(\\gamma = 0\\) . Parameters: Name Type Description Default y_true tf.Tensor Integer class labels. required y_pred tf.Tensor Either probabilities or logits, depending on the from_logits parameter. required alpha float required gamma float The focusing parameter \\(\\gamma\\) . Higher values of gamma make easy-to-classify examples contribute less to the loss relative to hard-to-classify examples. Must be non-negative. This can be a one-dimensional tensor, in which case it specifies a focusing parameter for each class. required class_weight Optional[Any] Weighting factor for each of the \\(k\\) classes. If not specified, then all classes are weighted equally. Defaults to None. None from_logits bool Whether y_pred contains logits or probabilities.. Defaults to False. False axis int Channel axis in the y_pred tensor.. Defaults to -1. -1 Returns: Type Description tf.Tensor The focal loss for each example. Examples: This function computes the per-example focal loss between a one-dimensional integer label vector and a two-dimensional prediction matrix: >>> import numpy as np >>> from focal_loss import sparse_categorical_focal_loss >>> y_true = [ 0 , 1 , 2 ] >>> y_pred = [[ 0.8 , 0.1 , 0.1 ], [ 0.2 , 0.7 , 0.1 ], [ 0.2 , 0.2 , 0.6 ]] >>> loss = sparse_categorical_focal_loss ( y_true , y_pred , gamma = 2 ) >>> np . set_printoptions ( precision = 3 ) >>> print ( loss . numpy ()) [ 0.009 0.032 0.082 ] Warnings This function does not reduce its output to a scalar, so it cannot be passed to tf.keras.Model.compile as a loss argument. Instead, use the wrapper class focal_loss.SparseCategoricalFocalLoss . References T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. ( DOI <https://doi.org/10.1109/TPAMI.2018.2858826> ) ( arXiv preprint <https://arxiv.org/abs/1708.02002> ) Source code in src/losses/focal_loss.py def sparse_categorical_focal_loss ( y_true : tf . Tensor , y_pred : tf . Tensor , alpha : float , gamma : float , * , class_weight : Optional [ Any ] = None , from_logits : bool = False , axis : int = - 1 , ) -> tf . Tensor : \"\"\" Focal loss function for multiclass classification with integer labels. This loss function generalizes multiclass softmax cross-entropy by introducing a hyperparameter called the *focusing parameter* that allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples. In the multiclass setting, with integer labels $y$, focal loss is defined as \\[ L(y, \\hat{\\mathbf{p}}) = -(1 - \\hat{p}_y )^\\gamma \\log(\\hat{p}_y) \\] where * $y \\in \\{0, \\ldots, K - 1\\}$ is an integer class label ($K$ denotes the number of classes), * $\\hat{\\mathbf{p}} = (\\hat{p}_0, \\ldots, \\hat{p}_{K-1}) \\in [0, 1]^K$ is a vector representing an estimated probability distribution over the $K$ classes, * $\\gamma$ (gamma, not $y$) is the *focusing parameter* that specifies how much higher-confidence correct predictions contribute to the overall loss (the higher the $\\gamma$, the higher the rate at which easy-to-classify examples are down-weighted). The usual multiclass softmax cross-entropy loss is recovered by setting $\\gamma = 0$. Args: y_true (tf.Tensor): Integer class labels. y_pred (tf.Tensor): Either probabilities or logits, depending on the `from_logits` parameter. alpha (float): gamma (float): The focusing parameter $\\gamma$. Higher values of `gamma` make easy-to-classify examples contribute less to the loss relative to hard-to-classify examples. Must be non-negative. This can be a one-dimensional tensor, in which case it specifies a focusing parameter for each class. class_weight (Optional[Any], optional): Weighting factor for each of the $k$ classes. If not specified, then all classes are weighted equally. Defaults to None. from_logits (bool, optional): Whether `y_pred` contains logits or probabilities.. Defaults to False. axis (int, optional): Channel axis in the `y_pred` tensor.. Defaults to -1. Returns: tf.Tensor: The focal loss for each example. Examples: This function computes the per-example focal loss between a one-dimensional integer label vector and a two-dimensional prediction matrix: >>> import numpy as np >>> from focal_loss import sparse_categorical_focal_loss >>> y_true = [0, 1, 2] >>> y_pred = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]] >>> loss = sparse_categorical_focal_loss(y_true, y_pred, gamma=2) >>> np.set_printoptions(precision=3) >>> print(loss.numpy()) [0.009 0.032 0.082] Warnings: This function does not reduce its output to a scalar, so it cannot be passed to `tf.keras.Model.compile` as a `loss` argument. Instead, use the wrapper class `focal_loss.SparseCategoricalFocalLoss`. References: T. Lin, P. Goyal, R. Girshick, K. He and P. Doll\u00e1r. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018. (`DOI <https://doi.org/10.1109/TPAMI.2018.2858826>`__) (`arXiv preprint <https://arxiv.org/abs/1708.02002>`__) \"\"\" # Process focusing parameter gamma = tf . convert_to_tensor ( gamma , dtype = tf . dtypes . float32 ) gamma_rank = gamma . shape . rank scalar_gamma = gamma_rank == 0 # Process class weight if class_weight is not None : class_weight = tf . convert_to_tensor ( class_weight , dtype = tf . dtypes . float32 ) # Process prediction tensor y_pred = tf . convert_to_tensor ( y_pred ) y_pred_rank = y_pred . shape . rank if y_pred_rank is not None : axis %= y_pred_rank if axis != y_pred_rank - 1 : # Put channel axis last for sparse_softmax_cross_entropy_with_logits perm = list ( itertools . chain ( range ( axis ), range ( axis + 1 , y_pred_rank ), [ axis ]), ) y_pred = tf . transpose ( y_pred , perm = perm ) elif axis != - 1 : raise ValueError ( f \"Cannot compute sparse categorical focal loss with axis= { axis } on \" \"a prediction tensor with statically unknown rank.\" , ) y_pred_shape = tf . shape ( y_pred ) # Process ground truth tensor y_true = tf . dtypes . cast ( y_true , dtype = tf . dtypes . int64 ) y_true_rank = y_true . shape . rank if y_true_rank is None : raise NotImplementedError ( \"Sparse categorical focal loss not supported \" \"for target/label tensors of unknown rank\" , ) reshape_needed = ( y_true_rank is not None and y_pred_rank is not None and y_pred_rank != y_true_rank + 1 ) if reshape_needed : y_true = tf . reshape ( y_true , [ - 1 ]) y_pred = tf . reshape ( y_pred , [ - 1 , y_pred_shape [ - 1 ]]) if from_logits : logits = y_pred probs = tf . nn . softmax ( y_pred , axis =- 1 ) else : probs = y_pred logits = tf . math . log ( tf . clip_by_value ( y_pred , _EPSILON , 1 - _EPSILON )) xent_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = y_true , logits = logits , ) y_true_rank = y_true . shape . rank probs = tf . gather ( probs , y_true , axis =- 1 , batch_dims = y_true_rank ) if not scalar_gamma : gamma = tf . gather ( gamma , y_true , axis = 0 , batch_dims = y_true_rank ) focal_modulation = ( 1 - probs ) ** gamma loss = alpha * focal_modulation * xent_loss if class_weight is not None : class_weight = tf . gather ( class_weight , y_true , axis = 0 , batch_dims = y_true_rank ) loss *= class_weight if reshape_needed : loss = tf . reshape ( loss , y_pred_shape [: - 1 ]) return loss","title":"sparse_categorical_focal_loss()"},{"location":"metrics/mean_iou/","text":"Mean Intersection over Union UpdatedMeanIoU ( MeanIoU ) __init__ ( self , num_classes , y_true = None , y_pred = None , name = 'mean_iou' , dtype = None ) special Modified Mean IoU (Intersection over Union) to work for sparse labels. We have to modify the definition of the Mean IoU if we want to consider sparse outputs for the segmentation. From StackOverflow . Parameters: Name Type Description Default y_true tf.Tensor The ground truth values. Defaults to None. None y_pred tf.Tensor The predicted labels. Defaults to None. None num_classes int Number of classes for the segmentation task. Defaults to None. required name str Name of the metric. Defaults to \"mean_iou\". 'mean_iou' dtype [type] . Defaults to None. None Source code in src/metrics/mean_iou.py def __init__ ( self , num_classes : int , y_true : tf . Tensor = None , y_pred : tf . Tensor = None , name : str = \"mean_iou\" , dtype = None , ): \"\"\"Modified Mean IoU (Intersection over Union) to work for sparse labels. We have to modify the definition of the Mean IoU if we want to consider sparse outputs for the segmentation. From [StackOverflow](https://stackoverflow.com/questions/61824470/dimensions-mismatch-error-when-using-tf-metrics-meaniou-with-sparsecategorical). Args: y_true (tf.Tensor, optional): The ground truth values. Defaults to None. y_pred (tf.Tensor, optional): The predicted labels. Defaults to None. num_classes (int, optional): Number of classes for the segmentation task. Defaults to None. name (str, optional): Name of the metric. Defaults to \"mean_iou\". dtype ([type], optional): . Defaults to None. \"\"\" super () . __init__ ( num_classes = num_classes , name = name , dtype = dtype ) get_config ( self ) Returns the serializable config of the metric. Source code in src/metrics/mean_iou.py def get_config ( self ): config = super () . get_config () return config update_state ( self , y_true , y_pred , sample_weight = None ) Accumulates the confusion matrix statistics. Parameters: Name Type Description Default y_true The ground truth values. required y_pred The predicted values. required sample_weight Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description Update op. Source code in src/metrics/mean_iou.py def update_state ( self , y_true , y_pred , sample_weight = None ): y_pred = tf . math . argmax ( y_pred , axis =- 1 ) return super () . update_state ( y_true , y_pred , sample_weight )","title":"Metrics"},{"location":"metrics/mean_iou/#mean-intersection-over-union","text":"","title":"Mean Intersection over Union"},{"location":"metrics/mean_iou/#src.metrics.mean_iou.UpdatedMeanIoU","text":"","title":"UpdatedMeanIoU"},{"location":"metrics/mean_iou/#src.metrics.mean_iou.UpdatedMeanIoU.__init__","text":"Modified Mean IoU (Intersection over Union) to work for sparse labels. We have to modify the definition of the Mean IoU if we want to consider sparse outputs for the segmentation. From StackOverflow . Parameters: Name Type Description Default y_true tf.Tensor The ground truth values. Defaults to None. None y_pred tf.Tensor The predicted labels. Defaults to None. None num_classes int Number of classes for the segmentation task. Defaults to None. required name str Name of the metric. Defaults to \"mean_iou\". 'mean_iou' dtype [type] . Defaults to None. None Source code in src/metrics/mean_iou.py def __init__ ( self , num_classes : int , y_true : tf . Tensor = None , y_pred : tf . Tensor = None , name : str = \"mean_iou\" , dtype = None , ): \"\"\"Modified Mean IoU (Intersection over Union) to work for sparse labels. We have to modify the definition of the Mean IoU if we want to consider sparse outputs for the segmentation. From [StackOverflow](https://stackoverflow.com/questions/61824470/dimensions-mismatch-error-when-using-tf-metrics-meaniou-with-sparsecategorical). Args: y_true (tf.Tensor, optional): The ground truth values. Defaults to None. y_pred (tf.Tensor, optional): The predicted labels. Defaults to None. num_classes (int, optional): Number of classes for the segmentation task. Defaults to None. name (str, optional): Name of the metric. Defaults to \"mean_iou\". dtype ([type], optional): . Defaults to None. \"\"\" super () . __init__ ( num_classes = num_classes , name = name , dtype = dtype )","title":"__init__()"},{"location":"metrics/mean_iou/#src.metrics.mean_iou.UpdatedMeanIoU.get_config","text":"Returns the serializable config of the metric. Source code in src/metrics/mean_iou.py def get_config ( self ): config = super () . get_config () return config","title":"get_config()"},{"location":"metrics/mean_iou/#src.metrics.mean_iou.UpdatedMeanIoU.update_state","text":"Accumulates the confusion matrix statistics. Parameters: Name Type Description Default y_true The ground truth values. required y_pred The predicted values. required sample_weight Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description Update op. Source code in src/metrics/mean_iou.py def update_state ( self , y_true , y_pred , sample_weight = None ): y_pred = tf . math . argmax ( y_pred , axis =- 1 ) return super () . update_state ( y_true , y_pred , sample_weight )","title":"update_state()"},{"location":"misc_config/docker/","text":"Dockerfile configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # FROM nvcr.io/nvidia/tensorflow:21.02-tf2-py3 # FROM nvcr.io/nvidia/tensorflow:21.09-tf2-py3 FROM nvcr.io/nvidia/tensorflow:21.06-tf2-py3 COPY requirements.txt . COPY requirements-dev.txt . ARG USERNAME = vorph ARG USER_UID = 1000 ARG USER_GID = 1000 RUN groupadd -g $USER_GID -o $USERNAME RUN useradd -m -u $USER_UID -g $USER_GID -o -s /bin/bash $USERNAME USER $USERNAME ENV PATH \" $PATH :/home/vorph/.local/bin\" RUN /usr/bin/python -m pip install --upgrade pip ENV PATH \" $PATH :/usr/lib/python3.8/dist-packages\" RUN /bin/bash -c \"pip install -r requirements.txt\" RUN /bin/bash -c \"pip install -r requirements-dev.txt\" # 5000 pour mlflow EXPOSE 5000 # 6006 pour tensorboard EXPOSE 6006 # 8001 pour mkdocs EXPOSE 8001 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // For f orma t de ta ils , see h tt ps : //aka.ms/devco nta i ner .jso n . For co nf ig op t io ns , see t he README a t : // h tt ps : //gi t hub.com/microso ft /vscode - dev - co nta i ners / tree /v 0.158.0 /co nta i ners /docker -e xis t i n g - docker f ile { \"name\" : \"AI_segmentation_project\" , // Se ts t he ru n co nte x t t o o ne level up i nstea d o f t he .devco nta i ner f older. \"context\" : \"..\" , // Upda te t he 'dockerFile' proper t y i f you are n ' t usi n g t he s tan dard 'Docker f ile' f ile na me. \"dockerFile\" : \"../Dockerfile\" , // Se t *de fault * co nta i ner speci f ic se tt i n gs.jso n values o n co nta i ner crea te . \"settings\" : { \"editor.rulers\" : [ 88 , 120 ], \"editor.fontSize\" : 12 , \"editor.bracketPairColorization.enabled\" : true , \"workbench.colorTheme\" : \"One Dark Pro\" , \"workbench.colorCustomizations\" : { \"editorRuler.foreground\" : \"#750917\" }, \"terminal.integrated.shell.linux\" : \"/bin/bash\" , }, // Add t he IDs o f ex tens io ns you wa nt i nstalle d whe n t he co nta i ner is crea te d. \"extensions\" : [ \"ms-python.python\" , \"eamodio.gitlens\" , \"esbenp.prettier-vscode\" , \"ms-azuretools.vscode-docker\" , \"njpwerner.autodocstring\" , \"mechatroner.rainbow-csv\" , \"shardulm94.trailing-spaces\" , \"visualstudioexptteam.vscodeintellicode\" , \"redhat.vscode-yaml\" , \"zhuangtongfa.material-theme\" , \"coenraads.bracket-pair-colorizer-2\" , \"emmanuelbeziat.vscode-great-icons\" , \"zainchen.json\" , \"yzhang.markdown-all-in-one\" ], //Use ' f orwardPor ts ' t o make a lis t o f por ts i ns ide t he co nta i ner available locally. \"forwardPorts\" : [ 5000 , 8001 , 6006 ], // U n comme nt t he ne x t li ne t o ru n comma n ds a fter t he co nta i ner is crea te d - f or example i nstall i n g curl. // \"postCreateCommand\" : \"apt-get update && apt-get install -y curl\" , // U n comme nt whe n usi n g a p tra ce - based debugger like C++ , Go , a n d Rus t \"runArgs\" : [ \"-it\" , \"--rm\" , \"-P\" , \"--runtime=nvidia\" ], // U n comme nt t o use t he Docker CLI fr om i ns ide t he co nta i ner . See h tt ps : //aka.ms/vscode - remo te /samples/docker - fr om - docker. \"mounts\" : [ \"type=bind,source=/media/vorph/datas/template_segmentation,target=/media/vorph/datas/template_segmentation\" ], // U n comme nt t o co nne c t as a n o n - roo t user i f you've added o ne . See h tt ps : //aka.ms/vscode - remo te /co nta i ners / n o n - roo t . // \"remoteUser\" : \"vorph\" }","title":"Docker config"},{"location":"misc_config/docker/#dockerfile-configuration","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # FROM nvcr.io/nvidia/tensorflow:21.02-tf2-py3 # FROM nvcr.io/nvidia/tensorflow:21.09-tf2-py3 FROM nvcr.io/nvidia/tensorflow:21.06-tf2-py3 COPY requirements.txt . COPY requirements-dev.txt . ARG USERNAME = vorph ARG USER_UID = 1000 ARG USER_GID = 1000 RUN groupadd -g $USER_GID -o $USERNAME RUN useradd -m -u $USER_UID -g $USER_GID -o -s /bin/bash $USERNAME USER $USERNAME ENV PATH \" $PATH :/home/vorph/.local/bin\" RUN /usr/bin/python -m pip install --upgrade pip ENV PATH \" $PATH :/usr/lib/python3.8/dist-packages\" RUN /bin/bash -c \"pip install -r requirements.txt\" RUN /bin/bash -c \"pip install -r requirements-dev.txt\" # 5000 pour mlflow EXPOSE 5000 # 6006 pour tensorboard EXPOSE 6006 # 8001 pour mkdocs EXPOSE 8001 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // For f orma t de ta ils , see h tt ps : //aka.ms/devco nta i ner .jso n . For co nf ig op t io ns , see t he README a t : // h tt ps : //gi t hub.com/microso ft /vscode - dev - co nta i ners / tree /v 0.158.0 /co nta i ners /docker -e xis t i n g - docker f ile { \"name\" : \"AI_segmentation_project\" , // Se ts t he ru n co nte x t t o o ne level up i nstea d o f t he .devco nta i ner f older. \"context\" : \"..\" , // Upda te t he 'dockerFile' proper t y i f you are n ' t usi n g t he s tan dard 'Docker f ile' f ile na me. \"dockerFile\" : \"../Dockerfile\" , // Se t *de fault * co nta i ner speci f ic se tt i n gs.jso n values o n co nta i ner crea te . \"settings\" : { \"editor.rulers\" : [ 88 , 120 ], \"editor.fontSize\" : 12 , \"editor.bracketPairColorization.enabled\" : true , \"workbench.colorTheme\" : \"One Dark Pro\" , \"workbench.colorCustomizations\" : { \"editorRuler.foreground\" : \"#750917\" }, \"terminal.integrated.shell.linux\" : \"/bin/bash\" , }, // Add t he IDs o f ex tens io ns you wa nt i nstalle d whe n t he co nta i ner is crea te d. \"extensions\" : [ \"ms-python.python\" , \"eamodio.gitlens\" , \"esbenp.prettier-vscode\" , \"ms-azuretools.vscode-docker\" , \"njpwerner.autodocstring\" , \"mechatroner.rainbow-csv\" , \"shardulm94.trailing-spaces\" , \"visualstudioexptteam.vscodeintellicode\" , \"redhat.vscode-yaml\" , \"zhuangtongfa.material-theme\" , \"coenraads.bracket-pair-colorizer-2\" , \"emmanuelbeziat.vscode-great-icons\" , \"zainchen.json\" , \"yzhang.markdown-all-in-one\" ], //Use ' f orwardPor ts ' t o make a lis t o f por ts i ns ide t he co nta i ner available locally. \"forwardPorts\" : [ 5000 , 8001 , 6006 ], // U n comme nt t he ne x t li ne t o ru n comma n ds a fter t he co nta i ner is crea te d - f or example i nstall i n g curl. // \"postCreateCommand\" : \"apt-get update && apt-get install -y curl\" , // U n comme nt whe n usi n g a p tra ce - based debugger like C++ , Go , a n d Rus t \"runArgs\" : [ \"-it\" , \"--rm\" , \"-P\" , \"--runtime=nvidia\" ], // U n comme nt t o use t he Docker CLI fr om i ns ide t he co nta i ner . See h tt ps : //aka.ms/vscode - remo te /samples/docker - fr om - docker. \"mounts\" : [ \"type=bind,source=/media/vorph/datas/template_segmentation,target=/media/vorph/datas/template_segmentation\" ], // U n comme nt t o co nne c t as a n o n - roo t user i f you've added o ne . See h tt ps : //aka.ms/vscode - remo te /co nta i ners / n o n - roo t . // \"remoteUser\" : \"vorph\" }","title":"Dockerfile configuration"},{"location":"misc_config/make/","text":"Makefile configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 # Makefile .PHONY : help help : @echo \"Commands:\" @echo \"rm_dataset : remove datas from raw_dataset directory.\" @echo \"prepared_dataset : create train, test, & validation datasets from raw datas in raw_dataset directory.\" @echo \"train : launch training loop for a given set of parameters from configs/params.yaml.\" @echo \"install : installs project requirements.\" @echo \"install-dev : installs development requirements.\" @echo \"install-docs : installs docs requirements.\" @echo \"clean : cleans all unecessary files.\" @echo \"build_docker : build the docker image of the project, to train in a docker container.\" @echo \"run_docker : run the docker container to train inside it.\" @echo \"mlflow : launch mlflow ui for monitoring training experiments.\" @echo \"tensorboard : launch tensorboard ui for monitoring training experiments.\" @echo \"docs : serve generated documentation from mkdocs.\" @echo \"tests : run unit tests.\" @echo \"mypy : run mypy in the src folder for type hinting checking.\" @echo \"cc_report : run radon in the src folder for code complexity report.\" @echo \"raw_report : run radon in the src folder for raw report.\" @echo \"mi_report : run radon in the src folder for maintainability index report.\" @echo \"hal_report : run radon in the src folder for hal report.\" @echo \"install_precommit : installs precommit.\" @echo \"check_precommit : check precommit.\" # Datas and training rm_dataset : rm -r ./datas/raw_dataset .PHONY : segmentation_masks segmentation_masks : python src/utils/utils_segmentation.py .PHONY : segmentation_info segmentation_info : python src/utils/utils_weight_sampling.py .PHONY : prepared_dataset prepared_dataset : python src/utils/make_dataset.py .PHONY : train train : python src/train.py # Installation .PHONY : install install : python -m pip install -e . --no-cache-dir .PHONY : install - dev install-dev : python -m pip install -e \".[dev]\" --no-cache-dir pre-commit install pre-commit autoupdate .PHONY : install - docs install-docs : python -m pip install -e \".[docs]\" --no-cache-dir # Cleaning .PHONY : clean clean : bash shell/clean_pycache.sh ../template_segmentation find . -type f -name \"*.DS_Store\" -ls -delete find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf find . | grep -E \".pytest_cache\" | xargs rm -rf find . | grep -E \".mypy_cache\" | xargs rm -rf find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf find . | grep -E \"htmlcov/*\" | xargs rm -rf rm -f .coverage .PHONY : clean_project clean_project : find . | grep -E \"mlruns/*\" | xargs rm -rf find . | grep -E \"hydra/*\" | xargs rm -rf # Docker .PHONY : build_docker build_docker : docker build --build-arg USER_UID = $$ ( id -u ) --build-arg USER_GID = $$ ( id -g ) --rm -f Dockerfile -t segmentation_project:v1 . .PHONY : run_docker run_docker : docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 -it --rm -P --mount type = bind,source = $( PWD ) ,target = /media/vorph/Datas/template_segmentation -e TF_FORCE_GPU_ALLOW_GROWTH = true -e XLA_FLAGS = '--xla_gpu_autotune_level=2' segmentation_project:v1 # Docker .PHONY : build_prod build_prod : docker build --build-arg USER_UID = $$ ( id -u ) --build-arg USER_GID = $$ ( id -g ) --rm -f Dockerfile.prod -t segmentation_project:v1_prod . .PHONY : run_prod run_prod : docker run --gpus all \\ --shm-size = 2g \\ --ulimit memlock = -1 \\ --ulimit stack = 67108864 \\ -it \\ --rm \\ -P \\ --mount type = bind,source = $( PWD ) /configs/,target = /home/vorph/configs \\ --mount type = bind,source = $( PWD ) /datas/,target = /home/vorph/datas \\ --mount type = bind,source = $( PWD ) /hydra/,target = /home/vorph/hydra/ \\ --mount type = bind,source = $( PWD ) /mlruns/,target = /home/vorph/mlruns/ \\ -e TF_FORCE_GPU_ALLOW_GROWTH = true \\ -e TF_ENABLE_ONEDNN_OPTS = true \\ -e XLA_FLAGS = '--xla_gpu_autotune_level=2' \\ segmentation_project:v1_prod # https://stackoverflow.com/questions/43133670/getting-docker-container-id-in-makefile-to-use-in-another-command # I ran into the same problem and realised that makefiles take output from shell variables with the use of $$. # Experiments monitoring mlflow : mlflow server -h 0 .0.0.0 -p 5000 --backend-store-uri $( PWD ) /mlruns/ tensorboard : tensorboard --logdir $( PWD ) /mlruns/ # Documentation .PHONY : docs docs : mkdocs serve # Tests .PHONY : tests tests : python -m pytest -v --cov # Reporting mypy : mypy --show-error-codes src/ cc_report : radon cc src/ raw_report : radon raw --summary src/ mi_report : radon mi src/ hal_report : radon hal src/ # Precommit install_precommit : pre-commit install check_precommit : pre-commit run --all","title":"Makefile"},{"location":"misc_config/make/#makefile-configuration","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 # Makefile .PHONY : help help : @echo \"Commands:\" @echo \"rm_dataset : remove datas from raw_dataset directory.\" @echo \"prepared_dataset : create train, test, & validation datasets from raw datas in raw_dataset directory.\" @echo \"train : launch training loop for a given set of parameters from configs/params.yaml.\" @echo \"install : installs project requirements.\" @echo \"install-dev : installs development requirements.\" @echo \"install-docs : installs docs requirements.\" @echo \"clean : cleans all unecessary files.\" @echo \"build_docker : build the docker image of the project, to train in a docker container.\" @echo \"run_docker : run the docker container to train inside it.\" @echo \"mlflow : launch mlflow ui for monitoring training experiments.\" @echo \"tensorboard : launch tensorboard ui for monitoring training experiments.\" @echo \"docs : serve generated documentation from mkdocs.\" @echo \"tests : run unit tests.\" @echo \"mypy : run mypy in the src folder for type hinting checking.\" @echo \"cc_report : run radon in the src folder for code complexity report.\" @echo \"raw_report : run radon in the src folder for raw report.\" @echo \"mi_report : run radon in the src folder for maintainability index report.\" @echo \"hal_report : run radon in the src folder for hal report.\" @echo \"install_precommit : installs precommit.\" @echo \"check_precommit : check precommit.\" # Datas and training rm_dataset : rm -r ./datas/raw_dataset .PHONY : segmentation_masks segmentation_masks : python src/utils/utils_segmentation.py .PHONY : segmentation_info segmentation_info : python src/utils/utils_weight_sampling.py .PHONY : prepared_dataset prepared_dataset : python src/utils/make_dataset.py .PHONY : train train : python src/train.py # Installation .PHONY : install install : python -m pip install -e . --no-cache-dir .PHONY : install - dev install-dev : python -m pip install -e \".[dev]\" --no-cache-dir pre-commit install pre-commit autoupdate .PHONY : install - docs install-docs : python -m pip install -e \".[docs]\" --no-cache-dir # Cleaning .PHONY : clean clean : bash shell/clean_pycache.sh ../template_segmentation find . -type f -name \"*.DS_Store\" -ls -delete find . | grep -E \"(__pycache__|\\.pyc|\\.pyo)\" | xargs rm -rf find . | grep -E \".pytest_cache\" | xargs rm -rf find . | grep -E \".mypy_cache\" | xargs rm -rf find . | grep -E \".ipynb_checkpoints\" | xargs rm -rf find . | grep -E \"htmlcov/*\" | xargs rm -rf rm -f .coverage .PHONY : clean_project clean_project : find . | grep -E \"mlruns/*\" | xargs rm -rf find . | grep -E \"hydra/*\" | xargs rm -rf # Docker .PHONY : build_docker build_docker : docker build --build-arg USER_UID = $$ ( id -u ) --build-arg USER_GID = $$ ( id -g ) --rm -f Dockerfile -t segmentation_project:v1 . .PHONY : run_docker run_docker : docker run --gpus all --shm-size = 2g --ulimit memlock = -1 --ulimit stack = 67108864 -it --rm -P --mount type = bind,source = $( PWD ) ,target = /media/vorph/Datas/template_segmentation -e TF_FORCE_GPU_ALLOW_GROWTH = true -e XLA_FLAGS = '--xla_gpu_autotune_level=2' segmentation_project:v1 # Docker .PHONY : build_prod build_prod : docker build --build-arg USER_UID = $$ ( id -u ) --build-arg USER_GID = $$ ( id -g ) --rm -f Dockerfile.prod -t segmentation_project:v1_prod . .PHONY : run_prod run_prod : docker run --gpus all \\ --shm-size = 2g \\ --ulimit memlock = -1 \\ --ulimit stack = 67108864 \\ -it \\ --rm \\ -P \\ --mount type = bind,source = $( PWD ) /configs/,target = /home/vorph/configs \\ --mount type = bind,source = $( PWD ) /datas/,target = /home/vorph/datas \\ --mount type = bind,source = $( PWD ) /hydra/,target = /home/vorph/hydra/ \\ --mount type = bind,source = $( PWD ) /mlruns/,target = /home/vorph/mlruns/ \\ -e TF_FORCE_GPU_ALLOW_GROWTH = true \\ -e TF_ENABLE_ONEDNN_OPTS = true \\ -e XLA_FLAGS = '--xla_gpu_autotune_level=2' \\ segmentation_project:v1_prod # https://stackoverflow.com/questions/43133670/getting-docker-container-id-in-makefile-to-use-in-another-command # I ran into the same problem and realised that makefiles take output from shell variables with the use of $$. # Experiments monitoring mlflow : mlflow server -h 0 .0.0.0 -p 5000 --backend-store-uri $( PWD ) /mlruns/ tensorboard : tensorboard --logdir $( PWD ) /mlruns/ # Documentation .PHONY : docs docs : mkdocs serve # Tests .PHONY : tests tests : python -m pytest -v --cov # Reporting mypy : mypy --show-error-codes src/ cc_report : radon cc src/ raw_report : radon raw --summary src/ mi_report : radon mi src/ hal_report : radon hal src/ # Precommit install_precommit : pre-commit install check_precommit : pre-commit run --all","title":"Makefile configuration"},{"location":"misc_config/requirements/","text":"Requirements configuration requirements.txt 1 2 3 4 5 6 7 8 9 albumentations == 1.0.3 hydra_core == 1.1.1 loguru == 0.5.3 mlflow == 1.18.0 omegaconf == 2.1.1 pandas == 1.3.4 PyYAML == 6.0 scikit_learn == 1.0.1 typer == 0.4.0 requirements-dev.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 bandit == 1.7.0 # linting, formating, typing black == 21.9 b0 darglint == 1.8.1 flake8 == 3.9.0 isort == 5.9.3 mypy == 0.910 # requirements writing pipreqs == 0.4.10 # precommit action pre - commit == 2.15.0 # unit tests pytest == 6.2.5 pytest - cov == 2.11.1 pytest - xdist == 2.3.0 # code quality infos radon == 4.5.0 # refactoring rope == 0.19.0 wemake - python - styleguide == 0.15.3 requirements-doc.txt 1 2 3 4 mkdocs == 1.2.3 mkdocs - material == 7.3.4 mkdocstrings == 0.16.2 mike == 1.1.2","title":"Requirements"},{"location":"misc_config/requirements/#requirements-configuration","text":"","title":"Requirements configuration"},{"location":"misc_config/requirements/#requirementstxt","text":"1 2 3 4 5 6 7 8 9 albumentations == 1.0.3 hydra_core == 1.1.1 loguru == 0.5.3 mlflow == 1.18.0 omegaconf == 2.1.1 pandas == 1.3.4 PyYAML == 6.0 scikit_learn == 1.0.1 typer == 0.4.0","title":"requirements.txt"},{"location":"misc_config/requirements/#requirements-devtxt","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 bandit == 1.7.0 # linting, formating, typing black == 21.9 b0 darglint == 1.8.1 flake8 == 3.9.0 isort == 5.9.3 mypy == 0.910 # requirements writing pipreqs == 0.4.10 # precommit action pre - commit == 2.15.0 # unit tests pytest == 6.2.5 pytest - cov == 2.11.1 pytest - xdist == 2.3.0 # code quality infos radon == 4.5.0 # refactoring rope == 0.19.0 wemake - python - styleguide == 0.15.3","title":"requirements-dev.txt"},{"location":"misc_config/requirements/#requirements-doctxt","text":"1 2 3 4 mkdocs == 1.2.3 mkdocs - material == 7.3.4 mkdocstrings == 0.16.2 mike == 1.1.2","title":"requirements-doc.txt"},{"location":"models/common_layers/","text":"Common layers used for all models InvertedResidualBottleneck2D ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/common_layers.py def build ( self , input_shape ) -> None : # *_, channels = input_shape self . conv1 = Conv2D ( filters = self . expansion_rate * self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , ) self . conv2 = Conv2D ( filters = self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , ) self . act = ReLU ( max_value = 6 ) self . bn1 = BatchNormalization () self . bn2 = BatchNormalization () self . bn3 = BatchNormalization () self . dwconv = DepthwiseConv2D ( kernel_size = 3 , strides = self . strides , padding = \"same\" , depth_multiplier = 1 , depthwise_initializer = \"he_normal\" , use_bias = False , ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/common_layers.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . conv1 ( inputs ) fmap = self . bn1 ( fmap ) fmap = self . act ( fmap ) fmap = self . dwconv ( fmap ) fmap = self . bn2 ( fmap ) fmap = self . act ( fmap ) fmap = self . conv2 ( fmap ) fmap = self . bn3 ( fmap ) if self . skip_connection : fmap += inputs return fmap from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/common_layers.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/common_layers.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"expansion_rate\" : self . expansion_rate , \"filters\" : self . filters , \"strides\" : self . strides , \"skip_connection\" : self . skip_connection , \"l2_regul\" : self . l2_regul , }, ) return config bn_relu_conv ( tensor , filters , kernel_size , name , padding = 'same' , strides = 1 , dilation_rate = 1 , w_init = 'he_normal' , l2_regul = 0.0001 ) BatchNormalization - ReLU - Conv2D module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the Conv2D layer. required kernel_size int Size of the convolution kernels used in the Conv2D layer. required name str Name of the module. required padding str Padding parameter of the Conv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the Conv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the Conv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th Conv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def bn_relu_conv ( tensor : tf . Tensor , filters : int , kernel_size : int , name : str , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" BatchNormalization - ReLU - Conv2D module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `Conv2D` layer. kernel_size (int): Size of the convolution kernels used in the `Conv2D` layer. name (str): Name of the module. padding (str, optional): Padding parameter of the `Conv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `Conv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `Conv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `Conv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = BatchNormalization ( name = f \"bn_ { name } \" )( tensor ) fmap = ReLU ( name = f \"relu_ { name } \" )( fmap ) return Conv2D ( filters = filters , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), kernel_initializer = w_init , use_bias = False , name = f \"conv_ { name } \" , )( fmap ) conv_bn_relu ( tensor , filters , kernel_size , name , padding = 'same' , strides = 1 , dilation_rate = 1 , w_init = 'he_normal' , l2_regul = 0.0001 ) Conv2D - BatchNormalization - ReLU module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the Conv2D layer. required kernel_size int Size of the convolution kernels used in the Conv2D layer. required name str Name of the module. required padding str Padding parameter of the Conv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the Conv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the Conv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th Conv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def conv_bn_relu ( tensor : tf . Tensor , filters : int , kernel_size : int , name : str , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" Conv2D - BatchNormalization - ReLU module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `Conv2D` layer. kernel_size (int): Size of the convolution kernels used in the `Conv2D` layer. name (str): Name of the module. padding (str, optional): Padding parameter of the `Conv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `Conv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `Conv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `Conv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = Conv2D ( filters = filters , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), kernel_initializer = w_init , use_bias = False , name = f \" { name } \" , )( tensor ) fmap = BatchNormalization ( name = f \"bn_ { name } \" )( fmap ) return ReLU ( name = f \"relu_ { name } \" )( fmap ) conv_gn_relu ( tensor , filters , kernel_size , padding = 'same' , strides = 1 , dilation_rate = 1 , w_init = 'he_normal' , l2_regul = 0.0001 ) Conv2D - GroupNormalization - ReLU module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the Conv2D layer. required kernel_size int Size of the convolution kernels used in the Conv2D layer. required padding str Padding parameter of the Conv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the Conv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the Conv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th Conv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def conv_gn_relu ( tensor : tf . Tensor , filters : int , kernel_size : int , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" Conv2D - GroupNormalization - ReLU module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `Conv2D` layer. kernel_size (int): Size of the convolution kernels used in the `Conv2D` layer. padding (str, optional): Padding parameter of the `Conv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `Conv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `Conv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `Conv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = Conv2D ( filters = filters , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), kernel_initializer = w_init , use_bias = False , )( tensor ) fmap = tfa . layers . GroupNormalization ()( fmap ) return ReLU ()( fmap ) residual_bottleneck ( tensor , filters , name , strides = 1 , dilation_rate = 1 , shortcut = False , l2_regul = 0.0001 ) [summary] Parameters: Name Type Description Default tensor tf.Tensor [description] required filters int [description] required kernel_size int [description] required name str [description] required padding str [description]. Defaults to \"same\". required strides int [description]. Defaults to 1. 1 dilation_rate int [description]. Defaults to 1. 1 w_init str [description]. Defaults to \"he_normal\". required l2_regul float [description]. Defaults to 1e-4. 0.0001 shortcut bool [description]. Defaults to False. False Returns: Type Description tf.Tensor [description] Source code in src/model/layers/common_layers.py def residual_bottleneck ( tensor : tf . Tensor , filters : int , name : str , strides : int = 1 , dilation_rate : int = 1 , shortcut : bool = False , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"[summary] Args: tensor (tf.Tensor): [description] filters (int): [description] kernel_size (int): [description] name (str): [description] padding (str, optional): [description]. Defaults to \"same\". strides (int, optional): [description]. Defaults to 1. dilation_rate (int, optional): [description]. Defaults to 1. w_init (str, optional): [description]. Defaults to \"he_normal\". l2_regul (float, optional): [description]. Defaults to 1e-4. shortcut (bool, optional): [description]. Defaults to False. Returns: tf.Tensor: [description] \"\"\" inner_filters = filters // 4 img = BatchNormalization ( name = f \"bn_bottleneck_ { name } \" )( tensor ) out = ReLU ( name = f \"relu_bottleneck_ { name } \" )( img ) residual_fmap = out # main stream fmap = bn_relu_conv ( out , inner_filters , kernel_size = 1 , strides = strides , name = f \"1_ { name } \" , ) fmap = bn_relu_conv ( fmap , inner_filters , kernel_size = 3 , strides = 1 , dilation_rate = dilation_rate , name = f \"2_ { name } \" , ) fmap = bn_relu_conv ( fmap , filters , kernel_size = 1 , strides = 1 , name = f \"3_ { name } \" ) # shortcut if shortcut : residual_fmap = Conv2D ( filters = filters , kernel_size = 1 , padding = \"same\" , strides = strides , kernel_initializer = \"he_normal\" , use_bias = False , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"shortcut_ { name } \" , )( out ) residual_fmap = BatchNormalization ( name = f \"shortcut_bn_ { name } \" )( residual_fmap ) return Add ( name = f \"add_ { name } \" )([ fmap , residual_fmap ]) sepconv_bn_relu ( tensor , filters , kernel_size , padding = 'same' , strides = 1 , dilation_rate = 1 , w_init = 'he_normal' , l2_regul = 0.0001 ) SeparableConv2D - BatchNormalization - ReLU module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the SeparableConv2D layer. required kernel_size int Size of the convolution kernels used in the SeparableConv2D layer. required padding str Padding parameter of the SeparableConv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the SeparableConv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the SeparableConv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th SeparableConv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def sepconv_bn_relu ( tensor : tf . Tensor , filters : int , kernel_size : int , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" SeparableConv2D - BatchNormalization - ReLU module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `SeparableConv2D` layer. kernel_size (int): Size of the convolution kernels used in the `SeparableConv2D` layer. padding (str, optional): Padding parameter of the `SeparableConv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `SeparableConv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `SeparableConv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `SeparableConv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = SeparableConv2D ( filters = filters , depth_multiplier = 1 , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , depthwise_initializer = w_init , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), use_bias = False , )( tensor ) fmap = BatchNormalization ()( fmap ) return ReLU ()( fmap )","title":"Common layers"},{"location":"models/common_layers/#common-layers-used-for-all-models","text":"","title":"Common layers used for all models"},{"location":"models/common_layers/#src.model.layers.common_layers.InvertedResidualBottleneck2D","text":"","title":"InvertedResidualBottleneck2D"},{"location":"models/common_layers/#src.model.layers.common_layers.InvertedResidualBottleneck2D.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/common_layers.py def build ( self , input_shape ) -> None : # *_, channels = input_shape self . conv1 = Conv2D ( filters = self . expansion_rate * self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , ) self . conv2 = Conv2D ( filters = self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , ) self . act = ReLU ( max_value = 6 ) self . bn1 = BatchNormalization () self . bn2 = BatchNormalization () self . bn3 = BatchNormalization () self . dwconv = DepthwiseConv2D ( kernel_size = 3 , strides = self . strides , padding = \"same\" , depth_multiplier = 1 , depthwise_initializer = \"he_normal\" , use_bias = False , )","title":"build()"},{"location":"models/common_layers/#src.model.layers.common_layers.InvertedResidualBottleneck2D.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/common_layers.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . conv1 ( inputs ) fmap = self . bn1 ( fmap ) fmap = self . act ( fmap ) fmap = self . dwconv ( fmap ) fmap = self . bn2 ( fmap ) fmap = self . act ( fmap ) fmap = self . conv2 ( fmap ) fmap = self . bn3 ( fmap ) if self . skip_connection : fmap += inputs return fmap","title":"call()"},{"location":"models/common_layers/#src.model.layers.common_layers.InvertedResidualBottleneck2D.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/common_layers.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/common_layers/#src.model.layers.common_layers.InvertedResidualBottleneck2D.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/common_layers.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"expansion_rate\" : self . expansion_rate , \"filters\" : self . filters , \"strides\" : self . strides , \"skip_connection\" : self . skip_connection , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/common_layers/#src.model.layers.common_layers.bn_relu_conv","text":"BatchNormalization - ReLU - Conv2D module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the Conv2D layer. required kernel_size int Size of the convolution kernels used in the Conv2D layer. required name str Name of the module. required padding str Padding parameter of the Conv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the Conv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the Conv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th Conv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def bn_relu_conv ( tensor : tf . Tensor , filters : int , kernel_size : int , name : str , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" BatchNormalization - ReLU - Conv2D module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `Conv2D` layer. kernel_size (int): Size of the convolution kernels used in the `Conv2D` layer. name (str): Name of the module. padding (str, optional): Padding parameter of the `Conv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `Conv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `Conv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `Conv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = BatchNormalization ( name = f \"bn_ { name } \" )( tensor ) fmap = ReLU ( name = f \"relu_ { name } \" )( fmap ) return Conv2D ( filters = filters , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), kernel_initializer = w_init , use_bias = False , name = f \"conv_ { name } \" , )( fmap )","title":"bn_relu_conv()"},{"location":"models/common_layers/#src.model.layers.common_layers.conv_bn_relu","text":"Conv2D - BatchNormalization - ReLU module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the Conv2D layer. required kernel_size int Size of the convolution kernels used in the Conv2D layer. required name str Name of the module. required padding str Padding parameter of the Conv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the Conv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the Conv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th Conv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def conv_bn_relu ( tensor : tf . Tensor , filters : int , kernel_size : int , name : str , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" Conv2D - BatchNormalization - ReLU module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `Conv2D` layer. kernel_size (int): Size of the convolution kernels used in the `Conv2D` layer. name (str): Name of the module. padding (str, optional): Padding parameter of the `Conv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `Conv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `Conv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `Conv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = Conv2D ( filters = filters , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), kernel_initializer = w_init , use_bias = False , name = f \" { name } \" , )( tensor ) fmap = BatchNormalization ( name = f \"bn_ { name } \" )( fmap ) return ReLU ( name = f \"relu_ { name } \" )( fmap )","title":"conv_bn_relu()"},{"location":"models/common_layers/#src.model.layers.common_layers.conv_gn_relu","text":"Conv2D - GroupNormalization - ReLU module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the Conv2D layer. required kernel_size int Size of the convolution kernels used in the Conv2D layer. required padding str Padding parameter of the Conv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the Conv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the Conv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th Conv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def conv_gn_relu ( tensor : tf . Tensor , filters : int , kernel_size : int , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" Conv2D - GroupNormalization - ReLU module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `Conv2D` layer. kernel_size (int): Size of the convolution kernels used in the `Conv2D` layer. padding (str, optional): Padding parameter of the `Conv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `Conv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `Conv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `Conv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = Conv2D ( filters = filters , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), kernel_initializer = w_init , use_bias = False , )( tensor ) fmap = tfa . layers . GroupNormalization ()( fmap ) return ReLU ()( fmap )","title":"conv_gn_relu()"},{"location":"models/common_layers/#src.model.layers.common_layers.residual_bottleneck","text":"[summary] Parameters: Name Type Description Default tensor tf.Tensor [description] required filters int [description] required kernel_size int [description] required name str [description] required padding str [description]. Defaults to \"same\". required strides int [description]. Defaults to 1. 1 dilation_rate int [description]. Defaults to 1. 1 w_init str [description]. Defaults to \"he_normal\". required l2_regul float [description]. Defaults to 1e-4. 0.0001 shortcut bool [description]. Defaults to False. False Returns: Type Description tf.Tensor [description] Source code in src/model/layers/common_layers.py def residual_bottleneck ( tensor : tf . Tensor , filters : int , name : str , strides : int = 1 , dilation_rate : int = 1 , shortcut : bool = False , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"[summary] Args: tensor (tf.Tensor): [description] filters (int): [description] kernel_size (int): [description] name (str): [description] padding (str, optional): [description]. Defaults to \"same\". strides (int, optional): [description]. Defaults to 1. dilation_rate (int, optional): [description]. Defaults to 1. w_init (str, optional): [description]. Defaults to \"he_normal\". l2_regul (float, optional): [description]. Defaults to 1e-4. shortcut (bool, optional): [description]. Defaults to False. Returns: tf.Tensor: [description] \"\"\" inner_filters = filters // 4 img = BatchNormalization ( name = f \"bn_bottleneck_ { name } \" )( tensor ) out = ReLU ( name = f \"relu_bottleneck_ { name } \" )( img ) residual_fmap = out # main stream fmap = bn_relu_conv ( out , inner_filters , kernel_size = 1 , strides = strides , name = f \"1_ { name } \" , ) fmap = bn_relu_conv ( fmap , inner_filters , kernel_size = 3 , strides = 1 , dilation_rate = dilation_rate , name = f \"2_ { name } \" , ) fmap = bn_relu_conv ( fmap , filters , kernel_size = 1 , strides = 1 , name = f \"3_ { name } \" ) # shortcut if shortcut : residual_fmap = Conv2D ( filters = filters , kernel_size = 1 , padding = \"same\" , strides = strides , kernel_initializer = \"he_normal\" , use_bias = False , dilation_rate = dilation_rate , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"shortcut_ { name } \" , )( out ) residual_fmap = BatchNormalization ( name = f \"shortcut_bn_ { name } \" )( residual_fmap ) return Add ( name = f \"add_ { name } \" )([ fmap , residual_fmap ])","title":"residual_bottleneck()"},{"location":"models/common_layers/#src.model.layers.common_layers.sepconv_bn_relu","text":"SeparableConv2D - BatchNormalization - ReLU module. Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the SeparableConv2D layer. required kernel_size int Size of the convolution kernels used in the SeparableConv2D layer. required padding str Padding parameter of the SeparableConv2D layer. Defaults to \"same\". 'same' strides int Strides parameter of the SeparableConv2D layer. Defaults to 1. 1 dilation_rate int Dilation rate of the SeparableConv2D layer. Defaults to 1. 1 w_init str Kernel initialization method used in th SeparableConv2D layer. Defaults to \"he_normal\". 'he_normal' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/layers/common_layers.py def sepconv_bn_relu ( tensor : tf . Tensor , filters : int , kernel_size : int , padding : str = \"same\" , strides : int = 1 , dilation_rate : int = 1 , w_init : str = \"he_normal\" , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\" SeparableConv2D - BatchNormalization - ReLU module. Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the `SeparableConv2D` layer. kernel_size (int): Size of the convolution kernels used in the `SeparableConv2D` layer. padding (str, optional): Padding parameter of the `SeparableConv2D` layer. Defaults to \"same\". strides (int, optional): Strides parameter of the `SeparableConv2D` layer. Defaults to 1. dilation_rate (int, optional): Dilation rate of the `SeparableConv2D` layer. Defaults to 1. w_init (str, optional): Kernel initialization method used in th `SeparableConv2D` layer. Defaults to \"he_normal\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" fmap = SeparableConv2D ( filters = filters , depth_multiplier = 1 , kernel_size = kernel_size , padding = padding , strides = strides , dilation_rate = dilation_rate , depthwise_initializer = w_init , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), use_bias = False , )( tensor ) fmap = BatchNormalization ()( fmap ) return ReLU ()( fmap )","title":"sepconv_bn_relu()"},{"location":"models/description/","text":"Architecture description and backbone/segmentations heads compatibility Modern, State-of-the-art semantic segmentation models rely on the combination of multiple (most of the time 2) networks : The network responsible of features extractions, usually a CNN like ResNet. This network is known as the backbone of the segmentation model. The networks responsible of computing the mask, given the outputs of the backbone. This network is known as the segmentation head of the segmentation model. Compared to the case where a backbone network like ResNet is used for a classification task and only outputs probabilities. Backbone networks in segmentation task are setup to ouptuts one of more feature maps. Definition The ratio between the height/width of the input image and the heights/widths of the outputs feature maps is called the output stride . It is usually denoted by OS. \\[ \\mathrm{OS} := \\frac{\\text{height-width input}}{\\text{height-width output}} \\] The segmentation head available in this project are the following ones. For example, OS 4-8-16-32 here means that the segmentation head takes as inputs 4 feature maps coming from the backbone, and denotes the different output strides of the inputs : the first input is of output stride 4, meaning that the feature map is 4 times smaller than the orginal input image of the backbone. the second input is of output stride 8, meaning that the feature map is 8 times smaller than the orginal input image of the backbone. the third input is of output stride 16, meaning that the feature map is 16 times smaller than the orginal input image of the backbone. the fourth input is of output stride 32, meaning that the feature map is 32 times smaller than the orginal input image of the backbone. Segmentation Heads FPN JPU KSAC OCNet ASPP ASPP_OCNet Output stride inputs OS 4-8-16-32 OS 8-16-32 OS 4 OS 8 OS 4 OS 4 Output stride output OS 4 OS 4 OS 4 OS 8 OS 4 OS 4 The backbones available are the following ones. GhostNet MobileNetv2 ResNet50 ResNet50v2 ResNet101 ResNet101v2 VoVNet27 VoVNet39 VoVNet57 They are all setup to outputs features maps of outputs strides 4, 8, 16, and 32. The outputs-inputs correspodances are automatically made within the script.","title":"Architecture description"},{"location":"models/description/#architecture-description-and-backbonesegmentations-heads-compatibility","text":"Modern, State-of-the-art semantic segmentation models rely on the combination of multiple (most of the time 2) networks : The network responsible of features extractions, usually a CNN like ResNet. This network is known as the backbone of the segmentation model. The networks responsible of computing the mask, given the outputs of the backbone. This network is known as the segmentation head of the segmentation model. Compared to the case where a backbone network like ResNet is used for a classification task and only outputs probabilities. Backbone networks in segmentation task are setup to ouptuts one of more feature maps. Definition The ratio between the height/width of the input image and the heights/widths of the outputs feature maps is called the output stride . It is usually denoted by OS. \\[ \\mathrm{OS} := \\frac{\\text{height-width input}}{\\text{height-width output}} \\] The segmentation head available in this project are the following ones. For example, OS 4-8-16-32 here means that the segmentation head takes as inputs 4 feature maps coming from the backbone, and denotes the different output strides of the inputs : the first input is of output stride 4, meaning that the feature map is 4 times smaller than the orginal input image of the backbone. the second input is of output stride 8, meaning that the feature map is 8 times smaller than the orginal input image of the backbone. the third input is of output stride 16, meaning that the feature map is 16 times smaller than the orginal input image of the backbone. the fourth input is of output stride 32, meaning that the feature map is 32 times smaller than the orginal input image of the backbone. Segmentation Heads FPN JPU KSAC OCNet ASPP ASPP_OCNet Output stride inputs OS 4-8-16-32 OS 8-16-32 OS 4 OS 8 OS 4 OS 4 Output stride output OS 4 OS 4 OS 4 OS 8 OS 4 OS 4 The backbones available are the following ones. GhostNet MobileNetv2 ResNet50 ResNet50v2 ResNet101 ResNet101v2 VoVNet27 VoVNet39 VoVNet57 They are all setup to outputs features maps of outputs strides 4, 8, 16, and 32. The outputs-inputs correspodances are automatically made within the script.","title":"Architecture description and backbone/segmentations heads compatibility"},{"location":"models/backbone/convmlp/","text":"ConvMLP: Hierarchical Convolutional MLPs for Vision Abstract MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for visual recognition, which is a light-weight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. ArXiv link BasicStage ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/convmlp.py def build ( self , input_shape ) -> None : dpr = [ rates for rates in np . linspace ( 0 , self . stochastic_depth_rate , self . num_blocks ) ] self . blocks = [ ConvMLPStage ( expansion_units = int ( self . units * self . mlp_ratio ), units = self . units , stochastic_depth_rate = dpr [ idx ], ) for idx in range ( self . num_blocks ) ] self . downsample_mlp = ( ConvDownsample ( filters = int ( self . units * 2 )) if self . downsample else Identity () ) call ( self , inputs , trainable = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , trainable = None ) -> tf . Tensor : for blk in self . blocks : inputs = blk ( inputs ) return self . downsample_mlp ( inputs ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"num_blocks\" : self . num_blocks , \"units\" : self . units , \"mlp_ratio\" : self . mlp_ratio , \"stochastic_depth_rate\" : self . stochastic_depth_rate , \"downsample\" : self . downsample , }, ) return config ConvDownsample ( Layer ) call ( self , inputs ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs ) -> tf . Tensor : return self . downsample ( inputs ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , }, ) return config ConvMLPStage ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/convmlp.py def build ( self , input_shape ) -> None : self . channel_mlp1 = Mlp ( fc1_units = self . expansion_units , fc2_units = self . units ) self . channel_mlp2 = Mlp ( fc1_units = self . expansion_units , fc2_units = self . units ) self . stochastic_drop = ( StochasticDepth ( drop_prop = self . stochastic_depth_rate ) if self . stochastic_depth_rate > 0 else Identity () ) self . depth_conv = DepthwiseConv2D ( depth_multiplier = 1 , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = inputs + self . stochastic_drop ( self . channel_mlp1 ( self . norm1 ( inputs ))) fmap = self . depth_conv ( self . connect_norm ( fmap )) return fmap + self . stochastic_drop ( self . channel_mlp2 ( self . norm2 ( inputs ))) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"expansion_units\" : self . expansion_units , \"units\" : self . units , \"stochastic_depth_rate\" : self . stochastic_depth_rate , \"l2_regularization\" : self . l2_regul , }, ) return config ConvStage ( Layer ) call ( self , inputs , trainable = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , trainable = None ) -> tf . Tensor : fmap = inputs for block in self . conv_blocks : fmap = fmap + block ( fmap ) return self . downsample ( fmap ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"num_blocks\" : self . num_blocks , \"filters_in\" : self . filters_in , \"filters_out\" : self . filters_out , \"filters_downsample\" : self . filters_downsample , \"l2_regularization\" : self . l2_regul , }, ) return config ConvTokenizer ( Layer ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : return self . block ( inputs ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , }, ) return config Identity ( Layer ) call ( self , inputs ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs ) -> tf . Tensor : return inputs Mlp ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/convmlp.py def build ( self , input_shape ) -> None : self . fc1 = Dense ( units = self . fc1_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc2 = Dense ( units = self . fc2_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . fc1 ( inputs ) fmap = self . gelu ( fmap ) return self . fc2 ( fmap ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"fc1_units\" : self . fc1_units , \"fc2_units\" : self . fc2_units , \"l2_regularization\" : self . l2_regul , }, ) return config StochasticDepth ( Layer ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : if training : keep_prob = tf . cast ( 1 - self . drop_prob , dtype = inputs . dtype ) shape = ( tf . shape ( inputs )[ 0 ],) + ( 1 ,) * ( len ( tf . shape ( inputs )) - 1 ) random_tensor = keep_prob + tf . random . uniform ( shape , 0 , 1 , dtype = inputs . dtype , ) random_tensor = tf . floor ( random_tensor ) return ( inputs / keep_prob ) * random_tensor return inputs get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"drop_prob\" : self . drop_prob }) return config get_backbone ( img_shape , channels , n_conv_blocks , num_blocks , units , mlp_ratios , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] [description] required channels int [description] required n_conv_blocks int [description] required num_blocks List[int] [description] required units List[int] [description] required mlp_ratios List[int] [description] required backbone_name str [description] required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/convmlp.py def get_backbone ( img_shape : List [ int ], channels : int , n_conv_blocks : int , num_blocks : List [ int ], units : List [ int ], mlp_ratios : List [ int ], backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): [description] channels (int): [description] n_conv_blocks (int): [description] num_blocks (List[int]): [description] units (List[int]): [description] mlp_ratios (List[int]): [description] backbone_name (str): [description] Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , channels = channels , n_conv_blocks = n_conv_blocks , num_blocks = num_blocks , units = units , mlp_ratios = mlp_ratios , ) endpoint_layers = [ \"tokenizer\" , \"conv\" , \"mlp1\" , \"mlp3\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , ) Various configurations available as backbones ConvMLP-XS 1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 64 n_conv_blocks : 2 num_blocks : [ 2 , 2 , 2 ] units : [ 128 , 128 , 256 , 512 ] mlp_ratios : [ 2 , 2 , 2 ] backbone_name : ConvMLP-XS ConvMLP-S 1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 64 n_conv_blocks : 2 num_blocks : [ 2 , 4 , 2 ] units : [ 128 , 128 , 256 , 512 ] mlp_ratios : [ 2 , 2 , 2 ] backbone_name : ConvMLP-S ConvMLP-M 1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 64 n_conv_blocks : 3 num_blocks : [ 3 , 6 , 3 ] units : [ 128 , 128 , 256 , 512 ] mlp_ratios : [ 3 , 3 , 3 ] backbone_name : ConvMLP-M ConvMLP-L 1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 96 n_conv_blocks : 3 num_blocks : [ 4 , 8 , 3 ] units : [ 192 , 192 , 384 , 768 ] mlp_ratios : [ 3 , 3 , 3 ] backbone_name : ConvMLP-L","title":"ConvMLP"},{"location":"models/backbone/convmlp/#convmlp-hierarchical-convolutional-mlps-for-vision","text":"Abstract MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for visual recognition, which is a light-weight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. ArXiv link","title":"ConvMLP: Hierarchical Convolutional MLPs for Vision"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.BasicStage","text":"","title":"BasicStage"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.BasicStage.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/convmlp.py def build ( self , input_shape ) -> None : dpr = [ rates for rates in np . linspace ( 0 , self . stochastic_depth_rate , self . num_blocks ) ] self . blocks = [ ConvMLPStage ( expansion_units = int ( self . units * self . mlp_ratio ), units = self . units , stochastic_depth_rate = dpr [ idx ], ) for idx in range ( self . num_blocks ) ] self . downsample_mlp = ( ConvDownsample ( filters = int ( self . units * 2 )) if self . downsample else Identity () )","title":"build()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.BasicStage.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , trainable = None ) -> tf . Tensor : for blk in self . blocks : inputs = blk ( inputs ) return self . downsample_mlp ( inputs )","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.BasicStage.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"num_blocks\" : self . num_blocks , \"units\" : self . units , \"mlp_ratio\" : self . mlp_ratio , \"stochastic_depth_rate\" : self . stochastic_depth_rate , \"downsample\" : self . downsample , }, ) return config","title":"get_config()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvDownsample","text":"","title":"ConvDownsample"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvDownsample.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs ) -> tf . Tensor : return self . downsample ( inputs )","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvDownsample.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvMLPStage","text":"","title":"ConvMLPStage"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvMLPStage.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/convmlp.py def build ( self , input_shape ) -> None : self . channel_mlp1 = Mlp ( fc1_units = self . expansion_units , fc2_units = self . units ) self . channel_mlp2 = Mlp ( fc1_units = self . expansion_units , fc2_units = self . units ) self . stochastic_drop = ( StochasticDepth ( drop_prop = self . stochastic_depth_rate ) if self . stochastic_depth_rate > 0 else Identity () ) self . depth_conv = DepthwiseConv2D ( depth_multiplier = 1 , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), )","title":"build()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvMLPStage.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = inputs + self . stochastic_drop ( self . channel_mlp1 ( self . norm1 ( inputs ))) fmap = self . depth_conv ( self . connect_norm ( fmap )) return fmap + self . stochastic_drop ( self . channel_mlp2 ( self . norm2 ( inputs )))","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvMLPStage.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"expansion_units\" : self . expansion_units , \"units\" : self . units , \"stochastic_depth_rate\" : self . stochastic_depth_rate , \"l2_regularization\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvStage","text":"","title":"ConvStage"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvStage.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , trainable = None ) -> tf . Tensor : fmap = inputs for block in self . conv_blocks : fmap = fmap + block ( fmap ) return self . downsample ( fmap )","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvStage.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"num_blocks\" : self . num_blocks , \"filters_in\" : self . filters_in , \"filters_out\" : self . filters_out , \"filters_downsample\" : self . filters_downsample , \"l2_regularization\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvTokenizer","text":"","title":"ConvTokenizer"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvTokenizer.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : return self . block ( inputs )","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.ConvTokenizer.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.Identity","text":"","title":"Identity"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.Identity.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs ) -> tf . Tensor : return inputs","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.Mlp","text":"","title":"Mlp"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.Mlp.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/convmlp.py def build ( self , input_shape ) -> None : self . fc1 = Dense ( units = self . fc1_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc2 = Dense ( units = self . fc2_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), )","title":"build()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.Mlp.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . fc1 ( inputs ) fmap = self . gelu ( fmap ) return self . fc2 ( fmap )","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.Mlp.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"fc1_units\" : self . fc1_units , \"fc2_units\" : self . fc2_units , \"l2_regularization\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.StochasticDepth","text":"","title":"StochasticDepth"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.StochasticDepth.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/convmlp.py def call ( self , inputs , training = None ) -> tf . Tensor : if training : keep_prob = tf . cast ( 1 - self . drop_prob , dtype = inputs . dtype ) shape = ( tf . shape ( inputs )[ 0 ],) + ( 1 ,) * ( len ( tf . shape ( inputs )) - 1 ) random_tensor = keep_prob + tf . random . uniform ( shape , 0 , 1 , dtype = inputs . dtype , ) random_tensor = tf . floor ( random_tensor ) return ( inputs / keep_prob ) * random_tensor return inputs","title":"call()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.StochasticDepth.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/convmlp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"drop_prob\" : self . drop_prob }) return config","title":"get_config()"},{"location":"models/backbone/convmlp/#src.model.backbone.convmlp.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] [description] required channels int [description] required n_conv_blocks int [description] required num_blocks List[int] [description] required units List[int] [description] required mlp_ratios List[int] [description] required backbone_name str [description] required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/convmlp.py def get_backbone ( img_shape : List [ int ], channels : int , n_conv_blocks : int , num_blocks : List [ int ], units : List [ int ], mlp_ratios : List [ int ], backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): [description] channels (int): [description] n_conv_blocks (int): [description] num_blocks (List[int]): [description] units (List[int]): [description] mlp_ratios (List[int]): [description] backbone_name (str): [description] Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , channels = channels , n_conv_blocks = n_conv_blocks , num_blocks = num_blocks , units = units , mlp_ratios = mlp_ratios , ) endpoint_layers = [ \"tokenizer\" , \"conv\" , \"mlp1\" , \"mlp3\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/convmlp/#various-configurations-available-as-backbones","text":"","title":"Various configurations available as backbones"},{"location":"models/backbone/convmlp/#convmlp-xs","text":"1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 64 n_conv_blocks : 2 num_blocks : [ 2 , 2 , 2 ] units : [ 128 , 128 , 256 , 512 ] mlp_ratios : [ 2 , 2 , 2 ] backbone_name : ConvMLP-XS","title":"ConvMLP-XS"},{"location":"models/backbone/convmlp/#convmlp-s","text":"1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 64 n_conv_blocks : 2 num_blocks : [ 2 , 4 , 2 ] units : [ 128 , 128 , 256 , 512 ] mlp_ratios : [ 2 , 2 , 2 ] backbone_name : ConvMLP-S","title":"ConvMLP-S"},{"location":"models/backbone/convmlp/#convmlp-m","text":"1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 64 n_conv_blocks : 3 num_blocks : [ 3 , 6 , 3 ] units : [ 128 , 128 , 256 , 512 ] mlp_ratios : [ 3 , 3 , 3 ] backbone_name : ConvMLP-M","title":"ConvMLP-M"},{"location":"models/backbone/convmlp/#convmlp-l","text":"1 2 3 4 5 6 7 8 _target_ : model.backbone.convmlp.get_backbone img_shape : ${datasets.params.img_shape} channels : 96 n_conv_blocks : 3 num_blocks : [ 4 , 8 , 3 ] units : [ 192 , 192 , 384 , 768 ] mlp_ratios : [ 3 , 3 , 3 ] backbone_name : ConvMLP-L","title":"ConvMLP-L"},{"location":"models/backbone/ghostnet/","text":"GhostNet: More Features from Cheap Operations Abstract Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. ArXiv link get_backbone ( img_shape , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/ghostnet.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , ) endpoint_layers = [ \"concat_ghost_module_2_gbneck_module_1_2\" , \"concat_ghost_module_2_gbneck_module_2_2\" , \"concat_ghost_module_2_gbneck_module_3_2\" , \"concat_ghost_module_2_gbneck_module_5_4\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , ) get_feature_extractor ( img_shape ) Instantiate a GhostNet model. Parameters: Name Type Description Default img_shape List[int] Input shape of the images in the dataset. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/ghostnet.py def get_feature_extractor ( img_shape : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a GhostNet model. Args: img_shape (List[int]): Input shape of the images in the dataset. Returns: A `tf.keras` model. \"\"\" dwkernels = [ 3 , 3 , 3 , 5 , 5 , 3 , 3 , 3 , 3 , 3 , 3 , 5 , 5 , 5 , 5 , 5 ] strides = [ 1 , 2 , 1 , 2 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 2 , 1 , 1 , 1 , 1 ] exps = [ 16 , 48 , 72 , 72 , 120 , 240 , 200 , 184 , 184 , 480 , 672 , 672 , 960 , 960 , 960 , 960 ] outs = [ 16 , 24 , 24 , 40 , 40 , 80 , 80 , 80 , 80 , 112 , 112 , 160 , 160 , 160 , 160 , 160 ] ratios = [ 2 ] * 16 use_ses = [ False , False , False , True , True , False , False , False , False , True , True , True , False , True , False , True , False , ] l2_reguls = [ 1e-4 ] * 16 img_input = Input ( img_shape ) fmap = Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , )( img_input ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 0 ], strides = strides [ 0 ], exp = exps [ 0 ], out = outs [ 0 ], ratio = ratios [ 0 ], use_se = use_ses [ 0 ], l2_regul = l2_reguls [ 0 ], name = \"1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 1 ], strides = strides [ 1 ], exp = exps [ 1 ], out = outs [ 1 ], ratio = ratios [ 1 ], use_se = use_ses [ 1 ], l2_regul = l2_reguls [ 1 ], name = \"1_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 2 ], strides = strides [ 2 ], exp = exps [ 2 ], out = outs [ 2 ], ratio = ratios [ 2 ], use_se = use_ses [ 2 ], l2_regul = l2_reguls [ 2 ], name = \"2_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 3 ], strides = strides [ 3 ], exp = exps [ 3 ], out = outs [ 3 ], ratio = ratios [ 3 ], use_se = use_ses [ 3 ], l2_regul = l2_reguls [ 3 ], name = \"2_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 4 ], strides = strides [ 4 ], exp = exps [ 4 ], out = outs [ 4 ], ratio = ratios [ 4 ], use_se = use_ses [ 4 ], l2_regul = l2_reguls [ 4 ], name = \"3_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 5 ], strides = strides [ 5 ], exp = exps [ 5 ], out = outs [ 5 ], ratio = ratios [ 5 ], use_se = use_ses [ 5 ], l2_regul = l2_reguls [ 5 ], name = \"3_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 6 ], strides = strides [ 6 ], exp = exps [ 6 ], out = outs [ 6 ], ratio = ratios [ 6 ], use_se = use_ses [ 6 ], l2_regul = l2_reguls [ 6 ], name = \"4_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 7 ], strides = strides [ 7 ], exp = exps [ 7 ], out = outs [ 7 ], ratio = ratios [ 7 ], use_se = use_ses [ 7 ], l2_regul = l2_reguls [ 7 ], name = \"4_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 8 ], strides = strides [ 8 ], exp = exps [ 8 ], out = outs [ 8 ], ratio = ratios [ 8 ], use_se = use_ses [ 8 ], l2_regul = l2_reguls [ 8 ], name = \"4_3\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 9 ], strides = strides [ 9 ], exp = exps [ 9 ], out = outs [ 9 ], ratio = ratios [ 9 ], use_se = use_ses [ 9 ], l2_regul = l2_reguls [ 9 ], name = \"4_4\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 10 ], strides = strides [ 10 ], exp = exps [ 10 ], out = outs [ 10 ], ratio = ratios [ 10 ], use_se = use_ses [ 10 ], l2_regul = l2_reguls [ 10 ], name = \"4_5\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 11 ], strides = strides [ 11 ], exp = exps [ 11 ], out = outs [ 11 ], ratio = ratios [ 11 ], use_se = use_ses [ 11 ], l2_regul = l2_reguls [ 11 ], name = \"4_6\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 12 ], strides = strides [ 12 ], exp = exps [ 12 ], out = outs [ 12 ], ratio = ratios [ 12 ], use_se = use_ses [ 12 ], l2_regul = l2_reguls [ 12 ], name = \"5_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 13 ], strides = strides [ 13 ], exp = exps [ 13 ], out = outs [ 13 ], ratio = ratios [ 13 ], use_se = use_ses [ 13 ], l2_regul = l2_reguls [ 13 ], name = \"5_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 14 ], strides = strides [ 14 ], exp = exps [ 14 ], out = outs [ 14 ], ratio = ratios [ 14 ], use_se = use_ses [ 14 ], l2_regul = l2_reguls [ 14 ], name = \"5_3\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 15 ], strides = strides [ 15 ], exp = exps [ 15 ], out = outs [ 15 ], ratio = ratios [ 15 ], use_se = use_ses [ 15 ], l2_regul = l2_reguls [ 15 ], name = \"5_4\" , ) return Model ( img_input , fmap ) ghost_bottleneck_module ( fmap_in , dwkernel , strides , exp , out , ratio , use_se , name , l2_regul = 0.0001 ) Ghost Bottleneck Module, the backbone of the GhostNet model. Parameters: Name Type Description Default fmap_in tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required dwkernel int Number of convolution kernels in the DepthwiseConv2D layer. required strides int Stride used in the DepthwiseConv2D layers. required exp int Number of filters used as an expansion operation in the first ghost_module . required out int Number of filters/channels of the output feature map. required ratio int Define the ratio in the ghost_module between the number of filters of the Conv2D layer and the number of filters of the DepthwiseConv2D in the last Concatenate layer. depth_multiplier of the DepthwiseConv2D layer is also defined as ratio-1 . required use_se bool Determine whether or not use a squeeze-and-excitation module before the last ghost_module layer. required name str Name of the module. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,\\mathrm{out})\\) . Source code in src/model/backbone/ghostnet.py def ghost_bottleneck_module ( fmap_in : tf . Tensor , dwkernel : int , strides : int , exp : int , out : int , ratio : int , use_se : bool , name : str , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"Ghost Bottleneck Module, the backbone of the GhostNet model. Args: fmap_in (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. dwkernel (int): Number of convolution kernels in the `DepthwiseConv2D` layer. strides (int): Stride used in the `DepthwiseConv2D` layers. exp (int): Number of filters used as an expansion operation in the first `ghost_module`. out (int): Number of filters/channels of the output feature map. ratio (int): Define the ratio in the `ghost_module` between the number of filters of the Conv2D layer and the number of filters of the `DepthwiseConv2D` in the last `Concatenate` layer. `depth_multiplier` of the `DepthwiseConv2D` layer is also defined as `ratio-1`. use_se (bool): Determine whether or not use a squeeze-and-excitation module before the last `ghost_module` layer. name (str): Name of the module. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,\\mathrm{out})$. \"\"\" fmap_shortcut = DepthwiseConv2D ( kernel_size = dwkernel , strides = strides , padding = \"same\" , depth_multiplier = ratio - 1 , activation = None , use_bias = False , depthwise_initializer = \"he_uniform\" , depthwise_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"depthconv_gbneck_module_ { name } \" , )( fmap_in ) fmap_shortcut = BatchNormalization ( name = f \"bn1_gbneck_module_ { name } \" )( fmap_shortcut ) fmap_shortcut = Conv2D ( filters = out , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , activation = None , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv_gbneck_module_ { name } \" , )( fmap_shortcut ) fmap_shortcut = BatchNormalization ( name = f \"bn2_gbneck_module_ { name } \" )( fmap_shortcut ) fmap = ghost_module ( fmap = fmap_in , out = exp , ratio = ratio , convkernel = ( 1 , 1 ), dwkernel = ( 3 , 3 ), name = f \"1_gbneck_module_ { name } \" , ) fmap = BatchNormalization ( name = f \"bn3_gbneck_module_ { name } \" )( fmap ) fmap = ReLU ( name = f \"relu_gbneck_module_ { name } \" )( fmap ) if strides > 1 : fmap = DepthwiseConv2D ( kernel_size = dwkernel , strides = strides , padding = \"same\" , depth_multiplier = ratio - 1 , activation = None , use_bias = False , depthwise_initializer = \"he_uniform\" , depthwise_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"depthconv_s2_gbneck_module_ { name } \" , )( fmap ) fmap = BatchNormalization ( name = f \"bn4_gbneck_module_ { name } \" )( fmap ) if use_se : fmap = se_module ( fmap_in = fmap , filters = exp , ratio = ratio , name = f \"gbneck_module_ { name } \" , ) fmap = ghost_module ( fmap = fmap , out = out , ratio = ratio , convkernel = ( 1 , 1 ), dwkernel = ( 3 , 3 ), name = f \"2_gbneck_module_ { name } \" , ) fmap = BatchNormalization ( name = f \"bn5_gbneck_module_ { name } \" )( fmap ) return Add ( name = f \"add_gbneck_module_ { name } \" )([ fmap_shortcut , fmap ]) ghost_module ( fmap , out , ratio , convkernel , dwkernel , name , l2_regul = 0.0001 ) Primary module of the GhostNet architecture. Parameters: Name Type Description Default fmap tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required out int Number of channels of the output feature map. required ratio int Define the ratio between the number of filters of the Conv2D layer and the number of filters of the DepthwiseConv2D in the last Concatenate layer. depth_multiplier of the DepthwiseConv2D layer is also defined as ratio-1 . required convkernel Tuple[int, int] Number of convolution kernels in the Conv2D layer. required dwkernel Tuple[int, int] Number of convolution kernels in the DepthwiseConv2D layer. required name str Name of the module. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,\\mathrm{out})\\) Source code in src/model/backbone/ghostnet.py def ghost_module ( fmap : tf . Tensor , out : int , ratio : int , convkernel : Tuple [ int , int ], dwkernel : Tuple [ int , int ], name : str , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"Primary module of the GhostNet architecture. Args: fmap (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. out (int): Number of channels of the output feature map. ratio (int): Define the ratio between the number of filters of the Conv2D layer and the number of filters of the `DepthwiseConv2D` in the last `Concatenate` layer. `depth_multiplier` of the `DepthwiseConv2D` layer is also defined as `ratio-1`. convkernel (Tuple[int, int]): Number of convolution kernels in the `Conv2D` layer. dwkernel (Tuple[int, int]): Number of convolution kernels in the `DepthwiseConv2D` layer. name (str): Name of the module. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,\\mathrm{out})$ \"\"\" filters = int ( np . ceil ( out / ratio )) channels = int ( out - filters ) fmap = Conv2D ( filters = filters , kernel_size = convkernel , strides = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv_ghost_module_ { name } \" , )( fmap ) dwfmap = DepthwiseConv2D ( kernel_size = dwkernel , strides = ( 1 , 1 ), padding = \"same\" , depth_multiplier = ratio - 1 , use_bias = False , depthwise_initializer = \"he_uniform\" , depthwise_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"depthconv_ghost_module_ { name } \" , )( fmap ) return Concatenate ( axis =- 1 , name = f \"concat_ghost_module_ { name } \" )( [ fmap , dwfmap [:, :, :, : channels ]], ) se_module ( fmap_in , ratio , filters , name , l2_regul = 0.0001 ) Squeeze-and-Excitation Module. Architecture Source : ArXiv link Parameters: Name Type Description Default fmap_in tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required ratio int Define the ratio of filters used in the squeeze operation of the modle (the first Conv2D). required filters int Numbers of filters used in the excitation operation of the module (the second Conv2D). required name str Name of the module. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/backbone/ghostnet.py def se_module ( fmap_in : tf . Tensor , ratio : int , filters : int , name : str , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"Squeeze-and-Excitation Module. Architecture: ![architecture](./images/se_module.svg) Source : [ArXiv link](https://arxiv.org/abs/1709.01507) Args: fmap_in (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. ratio (int): Define the ratio of filters used in the squeeze operation of the modle (the first Conv2D). filters (int): Numbers of filters used in the excitation operation of the module (the second Conv2D). name (str): Name of the module. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" channels = int ( fmap_in . shape [ - 1 ]) fmap = GlobalAveragePooling2D ( name = f \"gap_se_module_ { name } \" )( fmap_in ) fmap = Reshape (( 1 , 1 , channels ), name = f \"reshape_se_module_ { name } \" )( fmap ) fmap = Conv2D ( filters = int ( filters / ratio ), kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv1_se_module_ { name } \" , )( fmap ) fmap = ReLU ( name = f \"relu_se_module_ { name } \" )( fmap ) fmap = Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv2_se_module_ { name } \" , )( fmap ) excitation = Activation ( \"sigmoid\" , name = f \"sigmoid_se_module_ { name } \" )( fmap ) return fmap_in * excitation","title":"GhostNet"},{"location":"models/backbone/ghostnet/#ghostnet-more-features-from-cheap-operations","text":"Abstract Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. 75.7% top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. ArXiv link","title":"GhostNet: More Features from Cheap Operations"},{"location":"models/backbone/ghostnet/#src.model.backbone.ghostnet.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/ghostnet.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , ) endpoint_layers = [ \"concat_ghost_module_2_gbneck_module_1_2\" , \"concat_ghost_module_2_gbneck_module_2_2\" , \"concat_ghost_module_2_gbneck_module_3_2\" , \"concat_ghost_module_2_gbneck_module_5_4\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/ghostnet/#src.model.backbone.ghostnet.get_feature_extractor","text":"Instantiate a GhostNet model. Parameters: Name Type Description Default img_shape List[int] Input shape of the images in the dataset. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/ghostnet.py def get_feature_extractor ( img_shape : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a GhostNet model. Args: img_shape (List[int]): Input shape of the images in the dataset. Returns: A `tf.keras` model. \"\"\" dwkernels = [ 3 , 3 , 3 , 5 , 5 , 3 , 3 , 3 , 3 , 3 , 3 , 5 , 5 , 5 , 5 , 5 ] strides = [ 1 , 2 , 1 , 2 , 1 , 2 , 1 , 1 , 1 , 1 , 1 , 2 , 1 , 1 , 1 , 1 ] exps = [ 16 , 48 , 72 , 72 , 120 , 240 , 200 , 184 , 184 , 480 , 672 , 672 , 960 , 960 , 960 , 960 ] outs = [ 16 , 24 , 24 , 40 , 40 , 80 , 80 , 80 , 80 , 112 , 112 , 160 , 160 , 160 , 160 , 160 ] ratios = [ 2 ] * 16 use_ses = [ False , False , False , True , True , False , False , False , False , True , True , True , False , True , False , True , False , ] l2_reguls = [ 1e-4 ] * 16 img_input = Input ( img_shape ) fmap = Conv2D ( filters = 16 , kernel_size = ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , )( img_input ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 0 ], strides = strides [ 0 ], exp = exps [ 0 ], out = outs [ 0 ], ratio = ratios [ 0 ], use_se = use_ses [ 0 ], l2_regul = l2_reguls [ 0 ], name = \"1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 1 ], strides = strides [ 1 ], exp = exps [ 1 ], out = outs [ 1 ], ratio = ratios [ 1 ], use_se = use_ses [ 1 ], l2_regul = l2_reguls [ 1 ], name = \"1_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 2 ], strides = strides [ 2 ], exp = exps [ 2 ], out = outs [ 2 ], ratio = ratios [ 2 ], use_se = use_ses [ 2 ], l2_regul = l2_reguls [ 2 ], name = \"2_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 3 ], strides = strides [ 3 ], exp = exps [ 3 ], out = outs [ 3 ], ratio = ratios [ 3 ], use_se = use_ses [ 3 ], l2_regul = l2_reguls [ 3 ], name = \"2_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 4 ], strides = strides [ 4 ], exp = exps [ 4 ], out = outs [ 4 ], ratio = ratios [ 4 ], use_se = use_ses [ 4 ], l2_regul = l2_reguls [ 4 ], name = \"3_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 5 ], strides = strides [ 5 ], exp = exps [ 5 ], out = outs [ 5 ], ratio = ratios [ 5 ], use_se = use_ses [ 5 ], l2_regul = l2_reguls [ 5 ], name = \"3_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 6 ], strides = strides [ 6 ], exp = exps [ 6 ], out = outs [ 6 ], ratio = ratios [ 6 ], use_se = use_ses [ 6 ], l2_regul = l2_reguls [ 6 ], name = \"4_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 7 ], strides = strides [ 7 ], exp = exps [ 7 ], out = outs [ 7 ], ratio = ratios [ 7 ], use_se = use_ses [ 7 ], l2_regul = l2_reguls [ 7 ], name = \"4_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 8 ], strides = strides [ 8 ], exp = exps [ 8 ], out = outs [ 8 ], ratio = ratios [ 8 ], use_se = use_ses [ 8 ], l2_regul = l2_reguls [ 8 ], name = \"4_3\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 9 ], strides = strides [ 9 ], exp = exps [ 9 ], out = outs [ 9 ], ratio = ratios [ 9 ], use_se = use_ses [ 9 ], l2_regul = l2_reguls [ 9 ], name = \"4_4\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 10 ], strides = strides [ 10 ], exp = exps [ 10 ], out = outs [ 10 ], ratio = ratios [ 10 ], use_se = use_ses [ 10 ], l2_regul = l2_reguls [ 10 ], name = \"4_5\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 11 ], strides = strides [ 11 ], exp = exps [ 11 ], out = outs [ 11 ], ratio = ratios [ 11 ], use_se = use_ses [ 11 ], l2_regul = l2_reguls [ 11 ], name = \"4_6\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 12 ], strides = strides [ 12 ], exp = exps [ 12 ], out = outs [ 12 ], ratio = ratios [ 12 ], use_se = use_ses [ 12 ], l2_regul = l2_reguls [ 12 ], name = \"5_1\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 13 ], strides = strides [ 13 ], exp = exps [ 13 ], out = outs [ 13 ], ratio = ratios [ 13 ], use_se = use_ses [ 13 ], l2_regul = l2_reguls [ 13 ], name = \"5_2\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 14 ], strides = strides [ 14 ], exp = exps [ 14 ], out = outs [ 14 ], ratio = ratios [ 14 ], use_se = use_ses [ 14 ], l2_regul = l2_reguls [ 14 ], name = \"5_3\" , ) fmap = ghost_bottleneck_module ( fmap_in = fmap , dwkernel = dwkernels [ 15 ], strides = strides [ 15 ], exp = exps [ 15 ], out = outs [ 15 ], ratio = ratios [ 15 ], use_se = use_ses [ 15 ], l2_regul = l2_reguls [ 15 ], name = \"5_4\" , ) return Model ( img_input , fmap )","title":"get_feature_extractor()"},{"location":"models/backbone/ghostnet/#src.model.backbone.ghostnet.ghost_bottleneck_module","text":"Ghost Bottleneck Module, the backbone of the GhostNet model. Parameters: Name Type Description Default fmap_in tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required dwkernel int Number of convolution kernels in the DepthwiseConv2D layer. required strides int Stride used in the DepthwiseConv2D layers. required exp int Number of filters used as an expansion operation in the first ghost_module . required out int Number of filters/channels of the output feature map. required ratio int Define the ratio in the ghost_module between the number of filters of the Conv2D layer and the number of filters of the DepthwiseConv2D in the last Concatenate layer. depth_multiplier of the DepthwiseConv2D layer is also defined as ratio-1 . required use_se bool Determine whether or not use a squeeze-and-excitation module before the last ghost_module layer. required name str Name of the module. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,\\mathrm{out})\\) . Source code in src/model/backbone/ghostnet.py def ghost_bottleneck_module ( fmap_in : tf . Tensor , dwkernel : int , strides : int , exp : int , out : int , ratio : int , use_se : bool , name : str , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"Ghost Bottleneck Module, the backbone of the GhostNet model. Args: fmap_in (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. dwkernel (int): Number of convolution kernels in the `DepthwiseConv2D` layer. strides (int): Stride used in the `DepthwiseConv2D` layers. exp (int): Number of filters used as an expansion operation in the first `ghost_module`. out (int): Number of filters/channels of the output feature map. ratio (int): Define the ratio in the `ghost_module` between the number of filters of the Conv2D layer and the number of filters of the `DepthwiseConv2D` in the last `Concatenate` layer. `depth_multiplier` of the `DepthwiseConv2D` layer is also defined as `ratio-1`. use_se (bool): Determine whether or not use a squeeze-and-excitation module before the last `ghost_module` layer. name (str): Name of the module. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,\\mathrm{out})$. \"\"\" fmap_shortcut = DepthwiseConv2D ( kernel_size = dwkernel , strides = strides , padding = \"same\" , depth_multiplier = ratio - 1 , activation = None , use_bias = False , depthwise_initializer = \"he_uniform\" , depthwise_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"depthconv_gbneck_module_ { name } \" , )( fmap_in ) fmap_shortcut = BatchNormalization ( name = f \"bn1_gbneck_module_ { name } \" )( fmap_shortcut ) fmap_shortcut = Conv2D ( filters = out , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , activation = None , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv_gbneck_module_ { name } \" , )( fmap_shortcut ) fmap_shortcut = BatchNormalization ( name = f \"bn2_gbneck_module_ { name } \" )( fmap_shortcut ) fmap = ghost_module ( fmap = fmap_in , out = exp , ratio = ratio , convkernel = ( 1 , 1 ), dwkernel = ( 3 , 3 ), name = f \"1_gbneck_module_ { name } \" , ) fmap = BatchNormalization ( name = f \"bn3_gbneck_module_ { name } \" )( fmap ) fmap = ReLU ( name = f \"relu_gbneck_module_ { name } \" )( fmap ) if strides > 1 : fmap = DepthwiseConv2D ( kernel_size = dwkernel , strides = strides , padding = \"same\" , depth_multiplier = ratio - 1 , activation = None , use_bias = False , depthwise_initializer = \"he_uniform\" , depthwise_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"depthconv_s2_gbneck_module_ { name } \" , )( fmap ) fmap = BatchNormalization ( name = f \"bn4_gbneck_module_ { name } \" )( fmap ) if use_se : fmap = se_module ( fmap_in = fmap , filters = exp , ratio = ratio , name = f \"gbneck_module_ { name } \" , ) fmap = ghost_module ( fmap = fmap , out = out , ratio = ratio , convkernel = ( 1 , 1 ), dwkernel = ( 3 , 3 ), name = f \"2_gbneck_module_ { name } \" , ) fmap = BatchNormalization ( name = f \"bn5_gbneck_module_ { name } \" )( fmap ) return Add ( name = f \"add_gbneck_module_ { name } \" )([ fmap_shortcut , fmap ])","title":"ghost_bottleneck_module()"},{"location":"models/backbone/ghostnet/#src.model.backbone.ghostnet.ghost_module","text":"Primary module of the GhostNet architecture. Parameters: Name Type Description Default fmap tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required out int Number of channels of the output feature map. required ratio int Define the ratio between the number of filters of the Conv2D layer and the number of filters of the DepthwiseConv2D in the last Concatenate layer. depth_multiplier of the DepthwiseConv2D layer is also defined as ratio-1 . required convkernel Tuple[int, int] Number of convolution kernels in the Conv2D layer. required dwkernel Tuple[int, int] Number of convolution kernels in the DepthwiseConv2D layer. required name str Name of the module. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,\\mathrm{out})\\) Source code in src/model/backbone/ghostnet.py def ghost_module ( fmap : tf . Tensor , out : int , ratio : int , convkernel : Tuple [ int , int ], dwkernel : Tuple [ int , int ], name : str , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"Primary module of the GhostNet architecture. Args: fmap (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. out (int): Number of channels of the output feature map. ratio (int): Define the ratio between the number of filters of the Conv2D layer and the number of filters of the `DepthwiseConv2D` in the last `Concatenate` layer. `depth_multiplier` of the `DepthwiseConv2D` layer is also defined as `ratio-1`. convkernel (Tuple[int, int]): Number of convolution kernels in the `Conv2D` layer. dwkernel (Tuple[int, int]): Number of convolution kernels in the `DepthwiseConv2D` layer. name (str): Name of the module. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,\\mathrm{out})$ \"\"\" filters = int ( np . ceil ( out / ratio )) channels = int ( out - filters ) fmap = Conv2D ( filters = filters , kernel_size = convkernel , strides = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv_ghost_module_ { name } \" , )( fmap ) dwfmap = DepthwiseConv2D ( kernel_size = dwkernel , strides = ( 1 , 1 ), padding = \"same\" , depth_multiplier = ratio - 1 , use_bias = False , depthwise_initializer = \"he_uniform\" , depthwise_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"depthconv_ghost_module_ { name } \" , )( fmap ) return Concatenate ( axis =- 1 , name = f \"concat_ghost_module_ { name } \" )( [ fmap , dwfmap [:, :, :, : channels ]], )","title":"ghost_module()"},{"location":"models/backbone/ghostnet/#src.model.backbone.ghostnet.se_module","text":"Squeeze-and-Excitation Module. Architecture Source : ArXiv link Parameters: Name Type Description Default fmap_in tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required ratio int Define the ratio of filters used in the squeeze operation of the modle (the first Conv2D). required filters int Numbers of filters used in the excitation operation of the module (the second Conv2D). required name str Name of the module. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Returns: Type Description Tensor Output feature map, size = \\((H,W,C)\\) . Source code in src/model/backbone/ghostnet.py def se_module ( fmap_in : tf . Tensor , ratio : int , filters : int , name : str , l2_regul : float = 1e-4 , ) -> tf . Tensor : \"\"\"Squeeze-and-Excitation Module. Architecture: ![architecture](./images/se_module.svg) Source : [ArXiv link](https://arxiv.org/abs/1709.01507) Args: fmap_in (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. ratio (int): Define the ratio of filters used in the squeeze operation of the modle (the first Conv2D). filters (int): Numbers of filters used in the excitation operation of the module (the second Conv2D). name (str): Name of the module. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. Returns: Output feature map, size = $(H,W,C)$. \"\"\" channels = int ( fmap_in . shape [ - 1 ]) fmap = GlobalAveragePooling2D ( name = f \"gap_se_module_ { name } \" )( fmap_in ) fmap = Reshape (( 1 , 1 , channels ), name = f \"reshape_se_module_ { name } \" )( fmap ) fmap = Conv2D ( filters = int ( filters / ratio ), kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv1_se_module_ { name } \" , )( fmap ) fmap = ReLU ( name = f \"relu_se_module_ { name } \" )( fmap ) fmap = Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), name = f \"conv2_se_module_ { name } \" , )( fmap ) excitation = Activation ( \"sigmoid\" , name = f \"sigmoid_se_module_ { name } \" )( fmap ) return fmap_in * excitation","title":"se_module()"},{"location":"models/backbone/mit/","text":"MixTransformer CustomAttention ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , tensors , _ = input_shape height = int ( tf . sqrt ( float ( tensors ))) width = int ( tf . sqrt ( float ( tensors ))) reduction_height = height // self . attn_reduction_ratio reduction_width = width // self . attn_reduction_ratio self . heads_reshape = Reshape ( target_shape = ( tensors , self . num_heads , - 1 )) self . square_reshape = Reshape ( target_shape = ( height , width , - 1 )) self . wide_reshape = Reshape ( target_shape = ( tensors , - 1 )) self . wide_reduction_reshape = Reshape ( target_shape = ( reduction_height * reduction_width , - 1 ), ) self . kv_reshape = Reshape ( target_shape = ( - 1 , 2 , self . num_heads , int ( self . head_dims )), ) self . query = Dense ( self . units , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . key_value = Dense ( self . units * 2 , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . proj = Dense ( self . units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . attn_drop = Dropout ( rate = self . attn_drop_prob ) self . proj_drop = Dropout ( rate = self . proj_drop_prob ) self . permute = Permute (( 2 , 1 , 3 )) if self . attn_reduction_ratio > 1 : self . attn_conv = Conv2D ( filters = self . units , kernel_size = self . attn_reduction_ratio , strides = self . attn_reduction_ratio , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . norm = LayerNormalization () call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : queries = self . query ( inputs ) queries = self . heads_reshape ( queries ) queries = self . permute ( queries ) fmap = inputs if self . attn_reduction_ratio > 1 : fmap = self . square_reshape ( fmap ) fmap = self . attn_conv ( fmap ) fmap = self . wide_reduction_reshape ( fmap ) fmap = self . norm ( fmap ) fmap = self . key_value ( fmap ) fmap = self . kv_reshape ( fmap ) fmap = tf . transpose ( fmap , perm = [ 2 , 0 , 3 , 1 , 4 ]) keys , values = tf . split ( fmap , num_or_size_splits = 2 ) keys = tf . squeeze ( keys , axis = 0 ) values = tf . squeeze ( values , axis = 0 ) attn = tf . matmul ( queries , keys , transpose_b = True ) * self . scale attn = self . softmax ( attn ) attn = self . attn_drop ( attn ) x = tf . matmul ( attn , values ) x = tf . transpose ( x , perm = [ 0 , 2 , 1 , 3 ]) x = self . wide_reshape ( x ) x = self . proj ( x ) return self . proj_drop ( x ) from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"units\" : self . units , \"num_heads\" : self . num_heads , \"attn_drop_prob\" : self . attn_drop_prob , \"proj_drop_prob\" : self . proj_drop_prob , \"attn_reduction_ratio\" : self . attn_reduction_ratio , \"l2_regul\" : self . l2_regul , }, ) return config FFNAttentionBlock ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : self . attn = CustomAttention ( units = self . units , num_heads = self . num_heads , attn_drop_prob = self . attn_drop_prob , proj_drop_prob = self . proj_drop_prob , attn_reduction_ratio = self . attn_reduction_ratio , ) self . stochastic_drop = ( StochasticDepth ( drop_prop = self . stochastic_depth_rate ) if self . stochastic_depth_rate > 0 else Identity () ) self . mlp = Mlp ( fc1_units = self . units * self . mlp_ratio , fc2_units = self . units , ) self . norm1 = LayerNormalization () self . norm2 = LayerNormalization () call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . stochastic_drop ( self . attn ( self . norm1 ( inputs ))) fmap = inputs + fmap fmap = fmap + self . stochastic_drop ( self . mlp ( self . norm2 ( fmap ))) return fmap from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"units\" : self . units , \"num_heads\" : self . num_heads , \"mlp_ratio\" : self . mlp_ratio , \"attn_drop_prob\" : self . attn_drop_prob , \"proj_drop_prob\" : self . proj_drop_prob , \"attn_reduction_ratio\" : self . attn_reduction_ratio , \"stochastic_depth_rate\" : self . stochastic_depth_rate , }, ) return config Identity ( Layer ) call ( self , inputs ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs ) -> tf . Tensor : return inputs from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () return config Mlp ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , tensors , _ = input_shape height = int ( tf . sqrt ( float ( tensors ))) width = int ( tf . sqrt ( float ( tensors ))) self . square_reshape = Reshape ( target_shape = ( height , width , - 1 )) self . wide_reshape = Reshape ( target_shape = ( tensors , - 1 )) self . fc1 = Dense ( self . fc1_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc2 = Dense ( self . fc2_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . depth_conv = DepthwiseConv2D ( depth_multiplier = 1 , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . fc1 ( inputs ) fmap = self . square_reshape ( fmap ) fmap = self . depth_conv ( fmap ) fmap = self . wide_reshape ( fmap ) fmap = self . gelu ( fmap ) return self . fc2 ( fmap ) from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"fc1_units\" : self . fc1_units , \"fc2_units\" : self . fc2_units , \"l2_regularization\" : self . l2_regul , }, ) return config OverlapPatchEmbed ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , height , width , channels = input_shape self . H = height // self . strides self . W = width // self . strides self . proj = Conv2D ( self . emb_dim , kernel_size = self . patch_size , strides = self . strides , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . reshape = Reshape ( target_shape = ( self . H * self . W , - 1 )) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . proj ( inputs ) fmap = self . reshape ( fmap ) return self . norm ( fmap ) from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"patch_size\" : self . patch_size , \"strides\" : self . strides , \"emb_dim\" : self . emb_dim , \"l2_regul\" : self . l2_regul , }, ) return config SquareReshape ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , tensors , _ = input_shape height = int ( tf . sqrt ( float ( tensors ))) width = int ( tf . sqrt ( float ( tensors ))) self . square_reshape = Reshape ( target_shape = ( height , width , - 1 )) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : return self . square_reshape ( inputs ) from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () return config StochasticDepth ( Layer ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : if training : keep_prob = tf . cast ( 1 - self . drop_prob , dtype = inputs . dtype ) shape = ( tf . shape ( inputs )[ 0 ],) + ( 1 ,) * ( len ( tf . shape ( inputs )) - 1 ) random_tensor = keep_prob + tf . random . uniform ( shape , 0 , 1 , dtype = inputs . dtype , ) random_tensor = tf . floor ( random_tensor ) return ( inputs / keep_prob ) * random_tensor return inputs from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"drop_prob\" : self . drop_prob }) return config get_backbone ( img_shape , patch_size , strides , emb_dims , num_heads , mlp_ratios , proj_drop_prob , attn_drop_prob , stochastic_depth_rate , attn_reduction_ratios , depths , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] [description] required patch_size List[int] [description] required strides List[int] [description] required emb_dims List[int] [description] required num_heads List[int] [description] required mlp_ratios List[int] [description] required proj_drop_prob float [description] required attn_drop_prob float [description] required stochastic_depth_rate float [description] required attn_reduction_ratios List[int] [description] required depths List[int] [description] required backbone_name str [description] required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mit.py def get_backbone ( img_shape : List [ int ], patch_size : List [ int ], strides : List [ int ], emb_dims : List [ int ], num_heads : List [ int ], mlp_ratios : List [ int ], proj_drop_prob : float , attn_drop_prob : float , stochastic_depth_rate : float , attn_reduction_ratios : List [ int ], depths : List [ int ], backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): [description] patch_size (List[int]): [description] strides (List[int]): [description] emb_dims (List[int]): [description] num_heads (List[int]): [description] mlp_ratios (List[int]): [description] proj_drop_prob (float): [description] attn_drop_prob (float): [description] stochastic_depth_rate (float): [description] attn_reduction_ratios (List[int]): [description] depths (List[int]): [description] backbone_name (str): [description] Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , patch_size = patch_size , strides = strides , emb_dims = emb_dims , num_heads = num_heads , mlp_ratios = mlp_ratios , proj_drop_prob = proj_drop_prob , attn_drop_prob = attn_drop_prob , stochastic_depth_rate = stochastic_depth_rate , attn_reduction_ratios = attn_reduction_ratios , depths = depths , ) endpoint_layers = [ \"reshape_stage1\" , \"reshape_stage2\" , \"reshape_stage3\" , \"reshape_stage4\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , ) get_feature_extractor ( img_shape , patch_size , strides , emb_dims , num_heads , mlp_ratios , proj_drop_prob , attn_drop_prob , stochastic_depth_rate , attn_reduction_ratios , depths ) Instantiate a MiT model. Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mit.py def get_feature_extractor ( img_shape : List [ int ], patch_size : List [ int ], strides : List [ int ], emb_dims : List [ int ], num_heads : List [ int ], mlp_ratios : List [ int ], proj_drop_prob : float , attn_drop_prob : float , stochastic_depth_rate : float , attn_reduction_ratios : List [ int ], depths : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a MiT model. Returns: A `tf.keras` model. \"\"\" dpr = [ rates for rates in np . linspace ( 0 , stochastic_depth_rate , np . sum ( depths ))] img_input = Input ( img_shape ) fmap = OverlapPatchEmbed ( patch_size = patch_size [ 0 ], strides = strides [ 0 ], emb_dim = emb_dims [ 0 ], )( img_input ) # stage 1 cur = 0 for idx0 in range ( depths [ 0 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 0 ], num_heads = num_heads [ 0 ], mlp_ratio = mlp_ratios [ 0 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 0 ], stochastic_depth_rate = dpr [ cur + idx0 ], name = f \"block_ { idx0 } _stage_1\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage1\" )( fmap ) # stage 2 fmap = OverlapPatchEmbed ( patch_size = patch_size [ 1 ], strides = strides [ 1 ], emb_dim = emb_dims [ 1 ], )( fmap ) cur += depths [ 0 ] for idx1 in range ( depths [ 1 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 1 ], num_heads = num_heads [ 1 ], mlp_ratio = mlp_ratios [ 1 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 1 ], stochastic_depth_rate = dpr [ cur + idx1 ], name = f \"block_ { idx1 } _stage_2\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage2\" )( fmap ) # stage 3 fmap = OverlapPatchEmbed ( patch_size = patch_size [ 2 ], strides = strides [ 2 ], emb_dim = emb_dims [ 2 ], )( fmap ) cur += depths [ 1 ] for idx2 in range ( depths [ 2 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 2 ], num_heads = num_heads [ 2 ], mlp_ratio = mlp_ratios [ 2 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 2 ], stochastic_depth_rate = dpr [ cur + idx2 ], name = f \"block_ { idx2 } _stage_3\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage3\" )( fmap ) # stage 4 fmap = OverlapPatchEmbed ( patch_size = patch_size [ 3 ], strides = strides [ 3 ], emb_dim = emb_dims [ 3 ], )( fmap ) cur += depths [ 2 ] for idx3 in range ( depths [ 3 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 3 ], num_heads = num_heads [ 3 ], mlp_ratio = mlp_ratios [ 3 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 3 ], stochastic_depth_rate = dpr [ cur + idx3 ], name = f \"block_ { idx3 } _stage_4\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage4\" )( fmap ) return Model ( img_input , fmap )","title":"MixTransformer"},{"location":"models/backbone/mit/#mixtransformer","text":"","title":"MixTransformer"},{"location":"models/backbone/mit/#src.model.backbone.mit.CustomAttention","text":"","title":"CustomAttention"},{"location":"models/backbone/mit/#src.model.backbone.mit.CustomAttention.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , tensors , _ = input_shape height = int ( tf . sqrt ( float ( tensors ))) width = int ( tf . sqrt ( float ( tensors ))) reduction_height = height // self . attn_reduction_ratio reduction_width = width // self . attn_reduction_ratio self . heads_reshape = Reshape ( target_shape = ( tensors , self . num_heads , - 1 )) self . square_reshape = Reshape ( target_shape = ( height , width , - 1 )) self . wide_reshape = Reshape ( target_shape = ( tensors , - 1 )) self . wide_reduction_reshape = Reshape ( target_shape = ( reduction_height * reduction_width , - 1 ), ) self . kv_reshape = Reshape ( target_shape = ( - 1 , 2 , self . num_heads , int ( self . head_dims )), ) self . query = Dense ( self . units , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . key_value = Dense ( self . units * 2 , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . proj = Dense ( self . units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . attn_drop = Dropout ( rate = self . attn_drop_prob ) self . proj_drop = Dropout ( rate = self . proj_drop_prob ) self . permute = Permute (( 2 , 1 , 3 )) if self . attn_reduction_ratio > 1 : self . attn_conv = Conv2D ( filters = self . units , kernel_size = self . attn_reduction_ratio , strides = self . attn_reduction_ratio , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . norm = LayerNormalization ()","title":"build()"},{"location":"models/backbone/mit/#src.model.backbone.mit.CustomAttention.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : queries = self . query ( inputs ) queries = self . heads_reshape ( queries ) queries = self . permute ( queries ) fmap = inputs if self . attn_reduction_ratio > 1 : fmap = self . square_reshape ( fmap ) fmap = self . attn_conv ( fmap ) fmap = self . wide_reduction_reshape ( fmap ) fmap = self . norm ( fmap ) fmap = self . key_value ( fmap ) fmap = self . kv_reshape ( fmap ) fmap = tf . transpose ( fmap , perm = [ 2 , 0 , 3 , 1 , 4 ]) keys , values = tf . split ( fmap , num_or_size_splits = 2 ) keys = tf . squeeze ( keys , axis = 0 ) values = tf . squeeze ( values , axis = 0 ) attn = tf . matmul ( queries , keys , transpose_b = True ) * self . scale attn = self . softmax ( attn ) attn = self . attn_drop ( attn ) x = tf . matmul ( attn , values ) x = tf . transpose ( x , perm = [ 0 , 2 , 1 , 3 ]) x = self . wide_reshape ( x ) x = self . proj ( x ) return self . proj_drop ( x )","title":"call()"},{"location":"models/backbone/mit/#src.model.backbone.mit.CustomAttention.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.CustomAttention.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"units\" : self . units , \"num_heads\" : self . num_heads , \"attn_drop_prob\" : self . attn_drop_prob , \"proj_drop_prob\" : self . proj_drop_prob , \"attn_reduction_ratio\" : self . attn_reduction_ratio , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.FFNAttentionBlock","text":"","title":"FFNAttentionBlock"},{"location":"models/backbone/mit/#src.model.backbone.mit.FFNAttentionBlock.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : self . attn = CustomAttention ( units = self . units , num_heads = self . num_heads , attn_drop_prob = self . attn_drop_prob , proj_drop_prob = self . proj_drop_prob , attn_reduction_ratio = self . attn_reduction_ratio , ) self . stochastic_drop = ( StochasticDepth ( drop_prop = self . stochastic_depth_rate ) if self . stochastic_depth_rate > 0 else Identity () ) self . mlp = Mlp ( fc1_units = self . units * self . mlp_ratio , fc2_units = self . units , ) self . norm1 = LayerNormalization () self . norm2 = LayerNormalization ()","title":"build()"},{"location":"models/backbone/mit/#src.model.backbone.mit.FFNAttentionBlock.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . stochastic_drop ( self . attn ( self . norm1 ( inputs ))) fmap = inputs + fmap fmap = fmap + self . stochastic_drop ( self . mlp ( self . norm2 ( fmap ))) return fmap","title":"call()"},{"location":"models/backbone/mit/#src.model.backbone.mit.FFNAttentionBlock.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.FFNAttentionBlock.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"units\" : self . units , \"num_heads\" : self . num_heads , \"mlp_ratio\" : self . mlp_ratio , \"attn_drop_prob\" : self . attn_drop_prob , \"proj_drop_prob\" : self . proj_drop_prob , \"attn_reduction_ratio\" : self . attn_reduction_ratio , \"stochastic_depth_rate\" : self . stochastic_depth_rate , }, ) return config","title":"get_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.Identity","text":"","title":"Identity"},{"location":"models/backbone/mit/#src.model.backbone.mit.Identity.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs ) -> tf . Tensor : return inputs","title":"call()"},{"location":"models/backbone/mit/#src.model.backbone.mit.Identity.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.Identity.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () return config","title":"get_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.Mlp","text":"","title":"Mlp"},{"location":"models/backbone/mit/#src.model.backbone.mit.Mlp.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , tensors , _ = input_shape height = int ( tf . sqrt ( float ( tensors ))) width = int ( tf . sqrt ( float ( tensors ))) self . square_reshape = Reshape ( target_shape = ( height , width , - 1 )) self . wide_reshape = Reshape ( target_shape = ( tensors , - 1 )) self . fc1 = Dense ( self . fc1_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc2 = Dense ( self . fc2_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . depth_conv = DepthwiseConv2D ( depth_multiplier = 1 , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), )","title":"build()"},{"location":"models/backbone/mit/#src.model.backbone.mit.Mlp.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . fc1 ( inputs ) fmap = self . square_reshape ( fmap ) fmap = self . depth_conv ( fmap ) fmap = self . wide_reshape ( fmap ) fmap = self . gelu ( fmap ) return self . fc2 ( fmap )","title":"call()"},{"location":"models/backbone/mit/#src.model.backbone.mit.Mlp.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.Mlp.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"fc1_units\" : self . fc1_units , \"fc2_units\" : self . fc2_units , \"l2_regularization\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.OverlapPatchEmbed","text":"","title":"OverlapPatchEmbed"},{"location":"models/backbone/mit/#src.model.backbone.mit.OverlapPatchEmbed.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , height , width , channels = input_shape self . H = height // self . strides self . W = width // self . strides self . proj = Conv2D ( self . emb_dim , kernel_size = self . patch_size , strides = self . strides , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . reshape = Reshape ( target_shape = ( self . H * self . W , - 1 ))","title":"build()"},{"location":"models/backbone/mit/#src.model.backbone.mit.OverlapPatchEmbed.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . proj ( inputs ) fmap = self . reshape ( fmap ) return self . norm ( fmap )","title":"call()"},{"location":"models/backbone/mit/#src.model.backbone.mit.OverlapPatchEmbed.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.OverlapPatchEmbed.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"patch_size\" : self . patch_size , \"strides\" : self . strides , \"emb_dim\" : self . emb_dim , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.SquareReshape","text":"","title":"SquareReshape"},{"location":"models/backbone/mit/#src.model.backbone.mit.SquareReshape.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mit.py def build ( self , input_shape ) -> None : _ , tensors , _ = input_shape height = int ( tf . sqrt ( float ( tensors ))) width = int ( tf . sqrt ( float ( tensors ))) self . square_reshape = Reshape ( target_shape = ( height , width , - 1 ))","title":"build()"},{"location":"models/backbone/mit/#src.model.backbone.mit.SquareReshape.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : return self . square_reshape ( inputs )","title":"call()"},{"location":"models/backbone/mit/#src.model.backbone.mit.SquareReshape.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.SquareReshape.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () return config","title":"get_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.StochasticDepth","text":"","title":"StochasticDepth"},{"location":"models/backbone/mit/#src.model.backbone.mit.StochasticDepth.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mit.py def call ( self , inputs , training = None ) -> tf . Tensor : if training : keep_prob = tf . cast ( 1 - self . drop_prob , dtype = inputs . dtype ) shape = ( tf . shape ( inputs )[ 0 ],) + ( 1 ,) * ( len ( tf . shape ( inputs )) - 1 ) random_tensor = keep_prob + tf . random . uniform ( shape , 0 , 1 , dtype = inputs . dtype , ) random_tensor = tf . floor ( random_tensor ) return ( inputs / keep_prob ) * random_tensor return inputs","title":"call()"},{"location":"models/backbone/mit/#src.model.backbone.mit.StochasticDepth.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.StochasticDepth.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"drop_prob\" : self . drop_prob }) return config","title":"get_config()"},{"location":"models/backbone/mit/#src.model.backbone.mit.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] [description] required patch_size List[int] [description] required strides List[int] [description] required emb_dims List[int] [description] required num_heads List[int] [description] required mlp_ratios List[int] [description] required proj_drop_prob float [description] required attn_drop_prob float [description] required stochastic_depth_rate float [description] required attn_reduction_ratios List[int] [description] required depths List[int] [description] required backbone_name str [description] required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mit.py def get_backbone ( img_shape : List [ int ], patch_size : List [ int ], strides : List [ int ], emb_dims : List [ int ], num_heads : List [ int ], mlp_ratios : List [ int ], proj_drop_prob : float , attn_drop_prob : float , stochastic_depth_rate : float , attn_reduction_ratios : List [ int ], depths : List [ int ], backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): [description] patch_size (List[int]): [description] strides (List[int]): [description] emb_dims (List[int]): [description] num_heads (List[int]): [description] mlp_ratios (List[int]): [description] proj_drop_prob (float): [description] attn_drop_prob (float): [description] stochastic_depth_rate (float): [description] attn_reduction_ratios (List[int]): [description] depths (List[int]): [description] backbone_name (str): [description] Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , patch_size = patch_size , strides = strides , emb_dims = emb_dims , num_heads = num_heads , mlp_ratios = mlp_ratios , proj_drop_prob = proj_drop_prob , attn_drop_prob = attn_drop_prob , stochastic_depth_rate = stochastic_depth_rate , attn_reduction_ratios = attn_reduction_ratios , depths = depths , ) endpoint_layers = [ \"reshape_stage1\" , \"reshape_stage2\" , \"reshape_stage3\" , \"reshape_stage4\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/mit/#src.model.backbone.mit.get_feature_extractor","text":"Instantiate a MiT model. Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mit.py def get_feature_extractor ( img_shape : List [ int ], patch_size : List [ int ], strides : List [ int ], emb_dims : List [ int ], num_heads : List [ int ], mlp_ratios : List [ int ], proj_drop_prob : float , attn_drop_prob : float , stochastic_depth_rate : float , attn_reduction_ratios : List [ int ], depths : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a MiT model. Returns: A `tf.keras` model. \"\"\" dpr = [ rates for rates in np . linspace ( 0 , stochastic_depth_rate , np . sum ( depths ))] img_input = Input ( img_shape ) fmap = OverlapPatchEmbed ( patch_size = patch_size [ 0 ], strides = strides [ 0 ], emb_dim = emb_dims [ 0 ], )( img_input ) # stage 1 cur = 0 for idx0 in range ( depths [ 0 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 0 ], num_heads = num_heads [ 0 ], mlp_ratio = mlp_ratios [ 0 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 0 ], stochastic_depth_rate = dpr [ cur + idx0 ], name = f \"block_ { idx0 } _stage_1\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage1\" )( fmap ) # stage 2 fmap = OverlapPatchEmbed ( patch_size = patch_size [ 1 ], strides = strides [ 1 ], emb_dim = emb_dims [ 1 ], )( fmap ) cur += depths [ 0 ] for idx1 in range ( depths [ 1 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 1 ], num_heads = num_heads [ 1 ], mlp_ratio = mlp_ratios [ 1 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 1 ], stochastic_depth_rate = dpr [ cur + idx1 ], name = f \"block_ { idx1 } _stage_2\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage2\" )( fmap ) # stage 3 fmap = OverlapPatchEmbed ( patch_size = patch_size [ 2 ], strides = strides [ 2 ], emb_dim = emb_dims [ 2 ], )( fmap ) cur += depths [ 1 ] for idx2 in range ( depths [ 2 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 2 ], num_heads = num_heads [ 2 ], mlp_ratio = mlp_ratios [ 2 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 2 ], stochastic_depth_rate = dpr [ cur + idx2 ], name = f \"block_ { idx2 } _stage_3\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage3\" )( fmap ) # stage 4 fmap = OverlapPatchEmbed ( patch_size = patch_size [ 3 ], strides = strides [ 3 ], emb_dim = emb_dims [ 3 ], )( fmap ) cur += depths [ 2 ] for idx3 in range ( depths [ 3 ]): fmap = FFNAttentionBlock ( units = emb_dims [ 3 ], num_heads = num_heads [ 3 ], mlp_ratio = mlp_ratios [ 3 ], attn_drop_prob = attn_drop_prob , proj_drop_prob = proj_drop_prob , attn_reduction_ratio = attn_reduction_ratios [ 3 ], stochastic_depth_rate = dpr [ cur + idx3 ], name = f \"block_ { idx3 } _stage_4\" , )( fmap ) fmap = LayerNormalization ()( fmap ) fmap = SquareReshape ( name = \"reshape_stage4\" )( fmap ) return Model ( img_input , fmap )","title":"get_feature_extractor()"},{"location":"models/backbone/mobilenetv2/","text":"MobileNetV2: Inverted Residuals and Linear Bottlenecks Abstract In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters ArXiv link get_backbone ( img_shape , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mobilenetv2.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , ) endpoint_layers = [ \"inv_bottleneck_2_2\" , \"inv_bottleneck_3_3\" , \"inv_bottleneck_5_3\" , \"inv_bottleneck_6_3\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , ) get_feature_extractor ( img_shape ) Instantiate a Mobilenetv2 model. Parameters: Name Type Description Default img_shape List[int] Input shape of the images in the dataset. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mobilenetv2.py def get_feature_extractor ( img_shape : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a Mobilenetv2 model. Args: img_shape (List[int]): Input shape of the images in the dataset. Returns: A `tf.keras` model. \"\"\" channels = [ 32 , 16 , 24 , 32 , 64 , 96 , 160 , 320 ] img_input = Input ( img_shape ) img = Conv2D ( filters = channels [ 0 ], kernel_size = ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = True , )( img_input ) img = InvertedResidualBottleneck2D ( expansion_rate = 1 , filters = channels [ 1 ], strides = 1 , skip_connection = False , name = \"inv_bottleneck_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[1], # expansion_factor=1, # strides=(1, 1), # skip_connection=False, # name=\"inv_bottleneck_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 2 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_2_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[2], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_2_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 2 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_2_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[2], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_2_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 3 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_3_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[3], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_3_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 3 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_3_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[3], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_3_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 3 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_3_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[3], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_3_3\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_4_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_4_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_4_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_4_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_4_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_4_3\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_4_4\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_4_4\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 5 ], strides = 1 , skip_connection = False , name = \"inv_bottleneck_5_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[5], # expansion_factor=6, # strides=(1, 1), # skip_connection=False, # name=\"inv_bottleneck_5_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 5 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_5_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[5], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_5_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 5 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_5_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[5], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_5_3\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 6 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_6_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[6], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_6_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 6 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_6_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[6], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_6_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 6 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_6_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[6], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_6_3\", # ) img = Conv2D ( filters = 1280 , kernel_size = 1 , strides = 2 , padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = True , )( img ) return Model ( img_input , img ) inverted_residual_bottleneck ( fmap , filters , expansion_factor , strides , skip_connection , name ) Inverted Residual Bottleneck, the backbone of the GhostNet model. Architecture Parameters: Name Type Description Default fmap tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the second Conv2D layer. required expansion_factor int Integer by which multiply the number of channels \\(C\\) of the input feature map to define the number of filters in the first Conv2D . required strides Tuple[int, int] Stride parameter of the DepthwiseConv2D layers, used to downsample. required skip_connection bool Determine wheter or not add a skip connection to the module. required name str Name of the module. required Returns: Type Description Tensor Output feature map. Source code in src/model/backbone/mobilenetv2.py def inverted_residual_bottleneck ( fmap : tf . Tensor , filters : int , expansion_factor : int , strides : Tuple [ int , int ], skip_connection : bool , name : str , ) -> tf . Tensor : \"\"\"Inverted Residual Bottleneck, the backbone of the GhostNet model. Architecture: ![Architecture](./images/inv_residual_bottleneck.svg) Args: fmap (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the second `Conv2D` layer. expansion_factor (int): Integer by which multiply the number of channels $C$ of the input feature map to define the number of filters in the first `Conv2D`. strides (Tuple[int, int]): Stride parameter of the `DepthwiseConv2D` layers, used to downsample. skip_connection (bool): Determine wheter or not add a skip connection to the module. name (str): Name of the module. Returns: Output feature map. \"\"\" in_channels = backend . int_shape ( fmap )[ - 1 ] img = Conv2D ( filters = expansion_factor * in_channels , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , name = f \"conv1_ { name } \" , )( fmap ) img = BatchNormalization ( name = f \"bn1_ { name } \" )( img ) img = ReLU ( max_value = 6 , name = f \"relu1_ { name } \" )( img ) img = DepthwiseConv2D ( kernel_size = ( 3 , 3 ), strides = strides , padding = \"same\" , depth_multiplier = 1 , depthwise_initializer = \"he_normal\" , use_bias = False , name = f \"depthconv1_ { name } \" , )( img ) img = BatchNormalization ( name = f \"bn2_ { name } \" )( img ) img = ReLU ( max_value = 6 , name = f \"relu2_ { name } \" )( img ) img = Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , name = f \"conv2_ { name } \" , )( img ) img = BatchNormalization ( name = f \"bn3_ { name } \" )( img ) if skip_connection : img = Add ( name = f \"skip_connection_ { name } \" )([ img , fmap ]) return img","title":"MobileNetv2"},{"location":"models/backbone/mobilenetv2/#mobilenetv2-inverted-residuals-and-linear-bottlenecks","text":"Abstract In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters ArXiv link","title":"MobileNetV2: Inverted Residuals and Linear Bottlenecks"},{"location":"models/backbone/mobilenetv2/#src.model.backbone.mobilenetv2.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mobilenetv2.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , ) endpoint_layers = [ \"inv_bottleneck_2_2\" , \"inv_bottleneck_3_3\" , \"inv_bottleneck_5_3\" , \"inv_bottleneck_6_3\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/mobilenetv2/#src.model.backbone.mobilenetv2.get_feature_extractor","text":"Instantiate a Mobilenetv2 model. Parameters: Name Type Description Default img_shape List[int] Input shape of the images in the dataset. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mobilenetv2.py def get_feature_extractor ( img_shape : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a Mobilenetv2 model. Args: img_shape (List[int]): Input shape of the images in the dataset. Returns: A `tf.keras` model. \"\"\" channels = [ 32 , 16 , 24 , 32 , 64 , 96 , 160 , 320 ] img_input = Input ( img_shape ) img = Conv2D ( filters = channels [ 0 ], kernel_size = ( 3 , 3 ), strides = ( 2 , 2 ), padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = True , )( img_input ) img = InvertedResidualBottleneck2D ( expansion_rate = 1 , filters = channels [ 1 ], strides = 1 , skip_connection = False , name = \"inv_bottleneck_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[1], # expansion_factor=1, # strides=(1, 1), # skip_connection=False, # name=\"inv_bottleneck_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 2 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_2_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[2], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_2_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 2 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_2_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[2], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_2_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 3 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_3_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[3], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_3_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 3 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_3_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[3], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_3_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 3 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_3_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[3], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_3_3\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_4_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_4_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_4_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_4_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_4_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_4_3\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 4 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_4_4\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[4], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_4_4\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 5 ], strides = 1 , skip_connection = False , name = \"inv_bottleneck_5_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[5], # expansion_factor=6, # strides=(1, 1), # skip_connection=False, # name=\"inv_bottleneck_5_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 5 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_5_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[5], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_5_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 5 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_5_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[5], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_5_3\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 6 ], strides = 2 , skip_connection = False , name = \"inv_bottleneck_6_1\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[6], # expansion_factor=6, # strides=(2, 2), # skip_connection=False, # name=\"inv_bottleneck_6_1\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 6 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_6_2\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[6], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_6_2\", # ) img = InvertedResidualBottleneck2D ( expansion_rate = 6 , filters = channels [ 6 ], strides = 1 , skip_connection = True , name = \"inv_bottleneck_6_3\" , )( img ) # img = inverted_residual_bottleneck( # img, # filters=channels[6], # expansion_factor=6, # strides=(1, 1), # skip_connection=True, # name=\"inv_bottleneck_6_3\", # ) img = Conv2D ( filters = 1280 , kernel_size = 1 , strides = 2 , padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = True , )( img ) return Model ( img_input , img )","title":"get_feature_extractor()"},{"location":"models/backbone/mobilenetv2/#src.model.backbone.mobilenetv2.inverted_residual_bottleneck","text":"Inverted Residual Bottleneck, the backbone of the GhostNet model. Architecture Parameters: Name Type Description Default fmap tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters int Number of filters used in the second Conv2D layer. required expansion_factor int Integer by which multiply the number of channels \\(C\\) of the input feature map to define the number of filters in the first Conv2D . required strides Tuple[int, int] Stride parameter of the DepthwiseConv2D layers, used to downsample. required skip_connection bool Determine wheter or not add a skip connection to the module. required name str Name of the module. required Returns: Type Description Tensor Output feature map. Source code in src/model/backbone/mobilenetv2.py def inverted_residual_bottleneck ( fmap : tf . Tensor , filters : int , expansion_factor : int , strides : Tuple [ int , int ], skip_connection : bool , name : str , ) -> tf . Tensor : \"\"\"Inverted Residual Bottleneck, the backbone of the GhostNet model. Architecture: ![Architecture](./images/inv_residual_bottleneck.svg) Args: fmap (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters (int): Number of filters used in the second `Conv2D` layer. expansion_factor (int): Integer by which multiply the number of channels $C$ of the input feature map to define the number of filters in the first `Conv2D`. strides (Tuple[int, int]): Stride parameter of the `DepthwiseConv2D` layers, used to downsample. skip_connection (bool): Determine wheter or not add a skip connection to the module. name (str): Name of the module. Returns: Output feature map. \"\"\" in_channels = backend . int_shape ( fmap )[ - 1 ] img = Conv2D ( filters = expansion_factor * in_channels , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , name = f \"conv1_ { name } \" , )( fmap ) img = BatchNormalization ( name = f \"bn1_ { name } \" )( img ) img = ReLU ( max_value = 6 , name = f \"relu1_ { name } \" )( img ) img = DepthwiseConv2D ( kernel_size = ( 3 , 3 ), strides = strides , padding = \"same\" , depth_multiplier = 1 , depthwise_initializer = \"he_normal\" , use_bias = False , name = f \"depthconv1_ { name } \" , )( img ) img = BatchNormalization ( name = f \"bn2_ { name } \" )( img ) img = ReLU ( max_value = 6 , name = f \"relu2_ { name } \" )( img ) img = Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_normal\" , use_bias = False , name = f \"conv2_ { name } \" , )( img ) img = BatchNormalization ( name = f \"bn3_ { name } \" )( img ) if skip_connection : img = Add ( name = f \"skip_connection_ { name } \" )([ img , fmap ]) return img","title":"inverted_residual_bottleneck()"},{"location":"models/backbone/mobilevit/","text":"MobileViT MobileViT2D ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mobilevit.py def build ( self , input_shape ) -> None : _ , height , width , _ = input_shape num_patches = int ( height * width // self . patch_size ** 2 ) self . unfold = Reshape ( target_shape = ( self . patch_size ** 2 , num_patches , self . filters ), ) self . fold = Reshape ( target_shape = ( height , width , self . filters )) self . conv3x3_1 = Conv2D ( filters = self . filters , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv3x3_2 = Conv2D ( filters = self . filters , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv1x1_1 = Conv2D ( filters = self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv1x1_2 = Conv2D ( filters = self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . concat = Concatenate ( axis =- 1 ) self . transformer_block = [ Transformer ( fc1_units = self . expansion_rate * self . emb_dim , fc2_units = self . filters , num_heads = self . num_heads , emb_dim = self . emb_dim , ) for _ in range ( self . repetitions ) ] call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mobilevit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . conv3x3_1 ( inputs ) fmap = self . conv1x1_1 ( fmap ) fmap = self . unfold ( fmap ) for block in self . transformer_block : fmap = block ( fmap ) fmap = self . fold ( fmap ) fmap = self . conv1x1_2 ( fmap ) fmap = self . concat ([ fmap , inputs ]) return self . conv3x3_2 ( fmap ) from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mobilevit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mobilevit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"expansion_rate\" : self . expansion_rate , \"filters\" : self . filters , \"patch_size\" : self . patch_size , \"num_heads\" : self . num_heads , \"emb_dim\" : self . emb_dim , \"repetitions\" : self . repetitions , \"l2_regul\" : self . l2_regul , }, ) return config Transformer ( Layer ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mobilevit.py def build ( self , input_shape ) -> None : self . norm1 = LayerNormalization () self . norm2 = LayerNormalization () self . mhsa = MultiHeadAttention ( num_heads = self . num_heads , key_dim = self . emb_dim , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc1 = Dense ( self . fc1_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc2 = Dense ( self . fc2_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mobilevit.py def call ( self , inputs , training = None ) -> tf . Tensor : mhsa_in = self . norm1 ( inputs ) mhsa_out = self . mhsa ( mhsa_in , mhsa_in ) mhsa_out = inputs + mhsa_out fc_in = self . norm2 ( mhsa_out ) fc_in = self . fc1 ( fc_in ) fc_in = self . act ( fc_in ) fc_out = self . fc2 ( fc_in ) return mhsa_out + fc_out from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mobilevit.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mobilevit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"fc1_units\" : self . fc1_units , \"fc2_units\" : self . fc2_units , \"num_heads\" : self . num_heads , \"emb_dim\" : self . emb_dim , \"l2_regul\" : self . l2_regul , }, ) return config get_backbone ( img_shape , expansion_rate , filters , emb_dim , repetitions , num_heads , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] [description] required expansion_rate List[int] [description] required filters List[int] [description] required emb_dim List[int] [description] required repetitions List[int] [description] required num_heads int [description] required backbone_name str [description] required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mobilevit.py def get_backbone ( img_shape : List [ int ], expansion_rate : List [ int ], filters : List [ int ], emb_dim : List [ int ], repetitions : List [ int ], num_heads : int , backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): [description] expansion_rate (List[int]): [description] filters (List[int]): [description] emb_dim (List[int]): [description] repetitions (List[int]): [description] num_heads (int): [description] backbone_name (str): [description] Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , expansion_rate = expansion_rate , filters = filters , emb_dim = emb_dim , repetitions = repetitions , num_heads = num_heads , ) endpoint_layers = [ \"ivrb3\" , \"MobileViT2D_block2\" , \"MobileViT2D_block3\" , \"conv_output\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , )","title":"MobileViT"},{"location":"models/backbone/mobilevit/#mobilevit","text":"","title":"MobileViT"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.MobileViT2D","text":"","title":"MobileViT2D"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.MobileViT2D.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mobilevit.py def build ( self , input_shape ) -> None : _ , height , width , _ = input_shape num_patches = int ( height * width // self . patch_size ** 2 ) self . unfold = Reshape ( target_shape = ( self . patch_size ** 2 , num_patches , self . filters ), ) self . fold = Reshape ( target_shape = ( height , width , self . filters )) self . conv3x3_1 = Conv2D ( filters = self . filters , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv3x3_2 = Conv2D ( filters = self . filters , kernel_size = 3 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv1x1_1 = Conv2D ( filters = self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv1x1_2 = Conv2D ( filters = self . filters , kernel_size = 1 , strides = 1 , padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . concat = Concatenate ( axis =- 1 ) self . transformer_block = [ Transformer ( fc1_units = self . expansion_rate * self . emb_dim , fc2_units = self . filters , num_heads = self . num_heads , emb_dim = self . emb_dim , ) for _ in range ( self . repetitions ) ]","title":"build()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.MobileViT2D.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mobilevit.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap = self . conv3x3_1 ( inputs ) fmap = self . conv1x1_1 ( fmap ) fmap = self . unfold ( fmap ) for block in self . transformer_block : fmap = block ( fmap ) fmap = self . fold ( fmap ) fmap = self . conv1x1_2 ( fmap ) fmap = self . concat ([ fmap , inputs ]) return self . conv3x3_2 ( fmap )","title":"call()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.MobileViT2D.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mobilevit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.MobileViT2D.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mobilevit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"expansion_rate\" : self . expansion_rate , \"filters\" : self . filters , \"patch_size\" : self . patch_size , \"num_heads\" : self . num_heads , \"emb_dim\" : self . emb_dim , \"repetitions\" : self . repetitions , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.Transformer","text":"","title":"Transformer"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.Transformer.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/backbone/mobilevit.py def build ( self , input_shape ) -> None : self . norm1 = LayerNormalization () self . norm2 = LayerNormalization () self . mhsa = MultiHeadAttention ( num_heads = self . num_heads , key_dim = self . emb_dim , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc1 = Dense ( self . fc1_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . fc2 = Dense ( self . fc2_units , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), )","title":"build()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.Transformer.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/backbone/mobilevit.py def call ( self , inputs , training = None ) -> tf . Tensor : mhsa_in = self . norm1 ( inputs ) mhsa_out = self . mhsa ( mhsa_in , mhsa_in ) mhsa_out = inputs + mhsa_out fc_in = self . norm2 ( mhsa_out ) fc_in = self . fc1 ( fc_in ) fc_in = self . act ( fc_in ) fc_out = self . fc2 ( fc_in ) return mhsa_out + fc_out","title":"call()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.Transformer.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/backbone/mobilevit.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.Transformer.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/backbone/mobilevit.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"fc1_units\" : self . fc1_units , \"fc2_units\" : self . fc2_units , \"num_heads\" : self . num_heads , \"emb_dim\" : self . emb_dim , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/backbone/mobilevit/#src.model.backbone.mobilevit.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] [description] required expansion_rate List[int] [description] required filters List[int] [description] required emb_dim List[int] [description] required repetitions List[int] [description] required num_heads int [description] required backbone_name str [description] required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/mobilevit.py def get_backbone ( img_shape : List [ int ], expansion_rate : List [ int ], filters : List [ int ], emb_dim : List [ int ], repetitions : List [ int ], num_heads : int , backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): [description] expansion_rate (List[int]): [description] filters (List[int]): [description] emb_dim (List[int]): [description] repetitions (List[int]): [description] num_heads (int): [description] backbone_name (str): [description] Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , expansion_rate = expansion_rate , filters = filters , emb_dim = emb_dim , repetitions = repetitions , num_heads = num_heads , ) endpoint_layers = [ \"ivrb3\" , \"MobileViT2D_block2\" , \"MobileViT2D_block3\" , \"conv_output\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/resnet101/","text":"ResNet101 get_backbone ( img_shape , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet101.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet101 ( include_top = False , input_shape = img_shape ) c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in [ \"conv2_block3_out\" , \"conv3_block4_out\" , \"conv4_block23_out\" , \"conv5_block3_out\" , ] ] height = img_shape [ 1 ] logger . info ( f \"c2_output OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c3_output OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c4_output OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c5_output OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"ResNet101"},{"location":"models/backbone/resnet101/#resnet101","text":"","title":"ResNet101"},{"location":"models/backbone/resnet101/#src.model.backbone.resnet101.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet101.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet101 ( include_top = False , input_shape = img_shape ) c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in [ \"conv2_block3_out\" , \"conv3_block4_out\" , \"conv4_block23_out\" , \"conv5_block3_out\" , ] ] height = img_shape [ 1 ] logger . info ( f \"c2_output OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c3_output OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c4_output OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c5_output OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/resnet101v2/","text":"ResNet101 V2 get_backbone ( img_shape , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet101v2.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet101V2 ( include_top = False , input_shape = img_shape ) endpoint_layers = [ \"conv2_block3_preact_relu\" , \"conv3_block4_preact_relu\" , \"conv4_block23_preact_relu\" , \"post_relu\" , ] c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"c2_output OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c3_output OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c4_output OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c5_output OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"ResNet101v2"},{"location":"models/backbone/resnet101v2/#resnet101-v2","text":"","title":"ResNet101 V2"},{"location":"models/backbone/resnet101v2/#src.model.backbone.resnet101v2.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet101v2.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet101V2 ( include_top = False , input_shape = img_shape ) endpoint_layers = [ \"conv2_block3_preact_relu\" , \"conv3_block4_preact_relu\" , \"conv4_block23_preact_relu\" , \"post_relu\" , ] c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"c2_output OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c3_output OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c4_output OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c5_output OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/resnet50/","text":"ResNet 50 get_backbone ( img_shape , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet50.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet50 ( include_top = False , input_shape = img_shape ) endpoint_layers = [ \"conv2_block3_out\" , \"conv3_block4_out\" , \"conv4_block6_out\" , \"conv5_block3_out\" , ] c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"c2_output OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c3_output OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c4_output OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c5_output OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"ResNet50"},{"location":"models/backbone/resnet50/#resnet-50","text":"","title":"ResNet 50"},{"location":"models/backbone/resnet50/#src.model.backbone.resnet50.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet50.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet50 ( include_top = False , input_shape = img_shape ) endpoint_layers = [ \"conv2_block3_out\" , \"conv3_block4_out\" , \"conv4_block6_out\" , \"conv5_block3_out\" , ] c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"c2_output OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c3_output OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c4_output OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"c5_output OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/resnet50v2/","text":"ResNet50 V2 get_backbone ( img_shape , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet50v2.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet50V2 ( include_top = False , input_shape = img_shape ) endpoint_layers = [ \"conv2_block3_preact_relu\" , \"conv3_block4_preact_relu\" , \"conv4_block6_preact_relu\" , \"conv5_block3_out\" , ] c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"conv2_block3_preact_relu OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" , ) logger . info ( f \"conv3_block4_preact_relu OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" , ) logger . info ( f \"conv4_block6_preact_relu OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" , ) logger . info ( f \"conv5_block3_out OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"ResNet50v2"},{"location":"models/backbone/resnet50v2/#resnet50-v2","text":"","title":"ResNet50 V2"},{"location":"models/backbone/resnet50v2/#src.model.backbone.resnet50v2.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/resnet50v2.py def get_backbone ( img_shape : List [ int ], backbone_name : str ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = keras . applications . ResNet50V2 ( include_top = False , input_shape = img_shape ) endpoint_layers = [ \"conv2_block3_preact_relu\" , \"conv3_block4_preact_relu\" , \"conv4_block6_preact_relu\" , \"conv5_block3_out\" , ] c2_output , c3_output , c4_output , c5_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"conv2_block3_preact_relu OS : { int ( height / c2_output . shape . as_list ()[ 1 ]) } \" , ) logger . info ( f \"conv3_block4_preact_relu OS : { int ( height / c3_output . shape . as_list ()[ 1 ]) } \" , ) logger . info ( f \"conv4_block6_preact_relu OS : { int ( height / c4_output . shape . as_list ()[ 1 ]) } \" , ) logger . info ( f \"conv5_block3_out OS : { int ( height / c5_output . shape . as_list ()[ 1 ]) } \" ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ c2_output , c3_output , c4_output , c5_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/vovnet/","text":"VoVNet : An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection Abstract As DenseNet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. Although feature reuse enables DenseNet to produce strong features with a small number of model parameters and FLOPs, the detector with DenseNet backbone shows rather slow speed and low energy efficiency. We find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. To solve the inefficiency of DenseNet, we propose an energy and computation efficient architecture called VoVNet comprised of One-Shot Aggregation (OSA). The OSA not only adopts the strength of DenseNet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. To validate the effectiveness of VoVNet as a backbone network, we design both lightweight and large-scale VoVNet and apply them to one-stage and two-stage object detectors. Our VoVNet based detectors outperform DenseNet based ones with 2x faster speed and the energy consumptions are reduced by 1.6x - 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet backbone with faster speed and better energy efficiency. In particular, the small object detection performance has been significantly improved over DenseNet and ResNet. ArXiv link get_backbone ( img_shape , filters_conv3x3 , filters_conv1x1 , block_repetitions , backbone_name ) Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required filters_conv3x3 List[int] List the number of filters used for the 3x3 Conv2D in each OSA block. required filters_conv1x1 List[int] List the number of filters used for the 1x1 Conv2D in each OSA block. required block_repetitions List[int] Determine the number of OSA modules to repeat in each block. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/vovnet.py def get_backbone ( img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. filters_conv3x3 (List[int]): List the number of filters used for the 3x3 `Conv2D` in each OSA block. filters_conv1x1 (List[int]): List the number of filters used for the 1x1 `Conv2D` in each OSA block. block_repetitions (List[int]): Determine the number of OSA modules to repeat in each block. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , filters_conv3x3 = filters_conv3x3 , filters_conv1x1 = filters_conv1x1 , block_repetitions = block_repetitions , ) endpoint_layers = [ \"maxpool_block1_out\" , \"maxpool_block2_out\" , \"maxpool_block3_out\" , \"maxpool_block4_out\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , ) get_feature_extractor ( img_shape , filters_conv3x3 , filters_conv1x1 , block_repetitions ) Instantiate a VoVNet model. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required filters_conv3x3 List[int] List the number of filters used for the 3x3 Conv2D in each OSA block. required filters_conv1x1 List[int] List the number of filters used for the 1x1 Conv2D in each OSA block. required block_repetitions List[int] Determine the number of OSA modules to repeat in each block. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/vovnet.py def get_feature_extractor ( img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a VoVNet model. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. filters_conv3x3 (List[int]): List the number of filters used for the 3x3 `Conv2D` in each OSA block. filters_conv1x1 (List[int]): List the number of filters used for the 1x1 `Conv2D` in each OSA block. block_repetitions (List[int]): Determine the number of OSA modules to repeat in each block. Returns: A `tf.keras` model. \"\"\" # input block img_input = Input ( img_shape ) fmap = conv_bn_relu ( tensor = img_input , filters = 64 , kernel_size = ( 3 , 3 ), strides = ( 2 , 2 ), name = \"stem_stage_1\" , ) fmap = conv_bn_relu ( tensor = fmap , filters = 64 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = \"stem_stage_2\" , ) fmap = conv_bn_relu ( tensor = fmap , filters = 128 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = \"stem_stage_3\" , ) for idx0 in range ( block_repetitions [ 0 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 0 ], filters_conv1x1 = filters_conv1x1 [ 0 ], block_name = f \"1_ { idx0 } \" , ) fmap = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block1_out\" )( fmap ) for idx1 in range ( block_repetitions [ 1 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 1 ], filters_conv1x1 = filters_conv1x1 [ 1 ], block_name = f \"2_ { idx1 } \" , ) fmap = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block2_out\" )( fmap ) for idx2 in range ( block_repetitions [ 2 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 2 ], filters_conv1x1 = filters_conv1x1 [ 2 ], block_name = f \"3_ { idx2 } \" , ) fmap = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block3_out\" )( fmap ) for idx3 in range ( block_repetitions [ 3 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 3 ], filters_conv1x1 = filters_conv1x1 [ 3 ], block_name = f \"4_ { idx3 } \" , ) fmap_out = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block4_out\" )( fmap ) return Model ( img_input , fmap_out , name = \"VoVNet\" ) osa_module ( tensor , filters_conv3x3 , filters_conv1x1 , block_name ) One-Shot Aggregation module, the backbone of the VoVNet model. Architecture Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters_conv3x3 int Numbers of filters used in the 3x3 Conv2D layers. required filters_conv1x1 int Numbers of filters used in the 1x1 Conv2D layer. required block_name str Name of the module. required Returns: Type Description Tensor Output feature map, size = \\((H,W,\\mathrm{filters\\_ conv1x1})\\) . Source code in src/model/backbone/vovnet.py def osa_module ( tensor : tf . Tensor , filters_conv3x3 : int , filters_conv1x1 : int , block_name : str , ) -> tf . Tensor : \"\"\"One-Shot Aggregation module, the backbone of the VoVNet model. Architecture: ![Architecture](./images/osa_module.svg) Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters_conv3x3 (int): Numbers of filters used in the 3x3 `Conv2D` layers. filters_conv1x1 (int): Numbers of filters used in the 1x1 `Conv2D` layer. block_name (str): Name of the module. Returns: Output feature map, size = $(H,W,\\mathrm{filters\\_ conv1x1})$. \"\"\" fmap1 = conv_bn_relu ( tensor = tensor , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_1_block_ { block_name } \" , ) fmap2 = conv_bn_relu ( tensor = fmap1 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_2_block_ { block_name } \" , ) fmap3 = conv_bn_relu ( tensor = fmap2 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_3_block_ { block_name } \" , ) fmap4 = conv_bn_relu ( tensor = fmap3 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_4_block_ { block_name } \" , ) fmap5 = conv_bn_relu ( tensor = fmap4 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_5_block_ { block_name } \" , ) fmap = Concatenate ( axis =- 1 , name = f \"concat_ { block_name } \" )( [ fmap1 , fmap2 , fmap3 , fmap4 , fmap5 ], ) return conv_bn_relu ( tensor = fmap , filters = filters_conv1x1 , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), name = f \"conv1_out_block_ { block_name } \" , ) Various configurations available as backbones VoVNet27 1 2 3 4 5 6 _target_ : model.backbone.vovnet.get_backbone img_shape : ${datasets.params.img_shape} filters_conv3x3 : [ 64 , 80 , 96 , 112 ] filters_conv1x1 : [ 128 , 256 , 384 , 512 ] block_repetitions : [ 1 , 1 , 1 , 1 ] backbone_name : VoVNet27 VoVNet39 1 2 3 4 5 6 _target_ : model.backbone.vovnet.get_backbone img_shape : ${datasets.params.img_shape} filters_conv3x3 : [ 128 , 256 , 192 , 224 ] filters_conv1x1 : [ 256 , 512 , 768 , 1024 ] block_repetitions : [ 1 , 1 , 2 , 2 ] backbone_name : VoVNet39 VoVNet57 1 2 3 4 5 6 _target_ : model.backbone.vovnet.get_backbone img_shape : ${datasets.params.img_shape} filters_conv3x3 : [ 128 , 256 , 192 , 224 ] filters_conv1x1 : [ 256 , 512 , 768 , 1024 ] block_repetitions : [ 1 , 1 , 4 , 3 ] backbone_name : VoVNet57","title":"VoVNet"},{"location":"models/backbone/vovnet/#vovnet-an-energy-and-gpu-computation-efficient-backbone-network-for-real-time-object-detection","text":"Abstract As DenseNet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. Although feature reuse enables DenseNet to produce strong features with a small number of model parameters and FLOPs, the detector with DenseNet backbone shows rather slow speed and low energy efficiency. We find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. To solve the inefficiency of DenseNet, we propose an energy and computation efficient architecture called VoVNet comprised of One-Shot Aggregation (OSA). The OSA not only adopts the strength of DenseNet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. To validate the effectiveness of VoVNet as a backbone network, we design both lightweight and large-scale VoVNet and apply them to one-stage and two-stage object detectors. Our VoVNet based detectors outperform DenseNet based ones with 2x faster speed and the energy consumptions are reduced by 1.6x - 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet backbone with faster speed and better energy efficiency. In particular, the small object detection performance has been significantly improved over DenseNet and ResNet. ArXiv link","title":"VoVNet : An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection"},{"location":"models/backbone/vovnet/#src.model.backbone.vovnet.get_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required filters_conv3x3 List[int] List the number of filters used for the 3x3 Conv2D in each OSA block. required filters_conv1x1 List[int] List the number of filters used for the 1x1 Conv2D in each OSA block. required block_repetitions List[int] Determine the number of OSA modules to repeat in each block. required backbone_name str Name of the backbone. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/vovnet.py def get_backbone ( img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], backbone_name : str , ) -> tf . keras . Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. filters_conv3x3 (List[int]): List the number of filters used for the 3x3 `Conv2D` in each OSA block. filters_conv1x1 (List[int]): List the number of filters used for the 1x1 `Conv2D` in each OSA block. block_repetitions (List[int]): Determine the number of OSA modules to repeat in each block. backbone_name (str): Name of the backbone. Returns: A `tf.keras` model. \"\"\" backbone = get_feature_extractor ( img_shape = img_shape , filters_conv3x3 = filters_conv3x3 , filters_conv1x1 = filters_conv1x1 , block_repetitions = block_repetitions , ) endpoint_layers = [ \"maxpool_block1_out\" , \"maxpool_block2_out\" , \"maxpool_block3_out\" , \"maxpool_block4_out\" , ] os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in endpoint_layers ] height = img_shape [ 1 ] logger . info ( f \"os4_output OS : { int ( height / os4_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os8_output OS : { int ( height / os8_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os16_output OS : { int ( height / os16_output . shape . as_list ()[ 1 ]) } \" ) logger . info ( f \"os32_output OS : { int ( height / os32_output . shape . as_list ()[ 1 ]) } \" ) return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = backbone_name , )","title":"get_backbone()"},{"location":"models/backbone/vovnet/#src.model.backbone.vovnet.get_feature_extractor","text":"Instantiate a VoVNet model. Parameters: Name Type Description Default img_shape List[int] Input shape of the images/masks in the dataset. required filters_conv3x3 List[int] List the number of filters used for the 3x3 Conv2D in each OSA block. required filters_conv1x1 List[int] List the number of filters used for the 1x1 Conv2D in each OSA block. required block_repetitions List[int] Determine the number of OSA modules to repeat in each block. required Returns: Type Description Model A tf.keras model. Source code in src/model/backbone/vovnet.py def get_feature_extractor ( img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], ) -> tf . keras . Model : \"\"\"Instantiate a VoVNet model. Args: img_shape (List[int]): Input shape of the images/masks in the dataset. filters_conv3x3 (List[int]): List the number of filters used for the 3x3 `Conv2D` in each OSA block. filters_conv1x1 (List[int]): List the number of filters used for the 1x1 `Conv2D` in each OSA block. block_repetitions (List[int]): Determine the number of OSA modules to repeat in each block. Returns: A `tf.keras` model. \"\"\" # input block img_input = Input ( img_shape ) fmap = conv_bn_relu ( tensor = img_input , filters = 64 , kernel_size = ( 3 , 3 ), strides = ( 2 , 2 ), name = \"stem_stage_1\" , ) fmap = conv_bn_relu ( tensor = fmap , filters = 64 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = \"stem_stage_2\" , ) fmap = conv_bn_relu ( tensor = fmap , filters = 128 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = \"stem_stage_3\" , ) for idx0 in range ( block_repetitions [ 0 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 0 ], filters_conv1x1 = filters_conv1x1 [ 0 ], block_name = f \"1_ { idx0 } \" , ) fmap = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block1_out\" )( fmap ) for idx1 in range ( block_repetitions [ 1 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 1 ], filters_conv1x1 = filters_conv1x1 [ 1 ], block_name = f \"2_ { idx1 } \" , ) fmap = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block2_out\" )( fmap ) for idx2 in range ( block_repetitions [ 2 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 2 ], filters_conv1x1 = filters_conv1x1 [ 2 ], block_name = f \"3_ { idx2 } \" , ) fmap = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block3_out\" )( fmap ) for idx3 in range ( block_repetitions [ 3 ]): fmap = osa_module ( tensor = fmap , filters_conv3x3 = filters_conv3x3 [ 3 ], filters_conv1x1 = filters_conv1x1 [ 3 ], block_name = f \"4_ { idx3 } \" , ) fmap_out = MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block4_out\" )( fmap ) return Model ( img_input , fmap_out , name = \"VoVNet\" )","title":"get_feature_extractor()"},{"location":"models/backbone/vovnet/#src.model.backbone.vovnet.osa_module","text":"One-Shot Aggregation module, the backbone of the VoVNet model. Architecture Parameters: Name Type Description Default tensor tf.Tensor Input feature map of the module, size = \\((H,W,C)\\) . required filters_conv3x3 int Numbers of filters used in the 3x3 Conv2D layers. required filters_conv1x1 int Numbers of filters used in the 1x1 Conv2D layer. required block_name str Name of the module. required Returns: Type Description Tensor Output feature map, size = \\((H,W,\\mathrm{filters\\_ conv1x1})\\) . Source code in src/model/backbone/vovnet.py def osa_module ( tensor : tf . Tensor , filters_conv3x3 : int , filters_conv1x1 : int , block_name : str , ) -> tf . Tensor : \"\"\"One-Shot Aggregation module, the backbone of the VoVNet model. Architecture: ![Architecture](./images/osa_module.svg) Args: tensor (tf.Tensor): Input feature map of the module, size = $(H,W,C)$. filters_conv3x3 (int): Numbers of filters used in the 3x3 `Conv2D` layers. filters_conv1x1 (int): Numbers of filters used in the 1x1 `Conv2D` layer. block_name (str): Name of the module. Returns: Output feature map, size = $(H,W,\\mathrm{filters\\_ conv1x1})$. \"\"\" fmap1 = conv_bn_relu ( tensor = tensor , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_1_block_ { block_name } \" , ) fmap2 = conv_bn_relu ( tensor = fmap1 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_2_block_ { block_name } \" , ) fmap3 = conv_bn_relu ( tensor = fmap2 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_3_block_ { block_name } \" , ) fmap4 = conv_bn_relu ( tensor = fmap3 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_4_block_ { block_name } \" , ) fmap5 = conv_bn_relu ( tensor = fmap4 , filters = filters_conv3x3 , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), name = f \"conv3_5_block_ { block_name } \" , ) fmap = Concatenate ( axis =- 1 , name = f \"concat_ { block_name } \" )( [ fmap1 , fmap2 , fmap3 , fmap4 , fmap5 ], ) return conv_bn_relu ( tensor = fmap , filters = filters_conv1x1 , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), name = f \"conv1_out_block_ { block_name } \" , )","title":"osa_module()"},{"location":"models/backbone/vovnet/#various-configurations-available-as-backbones","text":"","title":"Various configurations available as backbones"},{"location":"models/backbone/vovnet/#vovnet27","text":"1 2 3 4 5 6 _target_ : model.backbone.vovnet.get_backbone img_shape : ${datasets.params.img_shape} filters_conv3x3 : [ 64 , 80 , 96 , 112 ] filters_conv1x1 : [ 128 , 256 , 384 , 512 ] block_repetitions : [ 1 , 1 , 1 , 1 ] backbone_name : VoVNet27","title":"VoVNet27"},{"location":"models/backbone/vovnet/#vovnet39","text":"1 2 3 4 5 6 _target_ : model.backbone.vovnet.get_backbone img_shape : ${datasets.params.img_shape} filters_conv3x3 : [ 128 , 256 , 192 , 224 ] filters_conv1x1 : [ 256 , 512 , 768 , 1024 ] block_repetitions : [ 1 , 1 , 2 , 2 ] backbone_name : VoVNet39","title":"VoVNet39"},{"location":"models/backbone/vovnet/#vovnet57","text":"1 2 3 4 5 6 _target_ : model.backbone.vovnet.get_backbone img_shape : ${datasets.params.img_shape} filters_conv3x3 : [ 128 , 256 , 192 , 224 ] filters_conv1x1 : [ 256 , 512 , 768 , 1024 ] block_repetitions : [ 1 , 1 , 4 , 3 ] backbone_name : VoVNet57","title":"VoVNet57"},{"location":"models/heads/allmlp/","text":"SegFormer get_segmentation_module ( units , n_classes , backbone , name ) Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/all_mlp.py def get_segmentation_module ( units : int , n_classes : int , backbone : tf . keras . Model , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" l2_regul = 1e-4 bil = \"bilinear\" he = \"he_uniform\" os4_output , os8_output , os16_output , os32_output = backbone . outputs fmap1 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os4_output ) fmap2 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os8_output ) fmap2 = UpSampling2D ( size = ( 2 , 2 ), interpolation = bil )( fmap2 ) fmap3 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os16_output ) fmap3 = UpSampling2D ( size = ( 4 , 4 ), interpolation = bil )( fmap3 ) fmap4 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os32_output ) fmap4 = UpSampling2D ( size = ( 8 , 8 ), interpolation = bil )( fmap4 ) fmap = Concatenate ( axis =- 1 )([ fmap1 , fmap2 , fmap3 , fmap4 ]) fmap = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( fmap ) fmap = Dense ( n_classes , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( fmap ) fmap = UpSampling2D ( size = ( 4 , 4 ), interpolation = bil )( fmap ) out = Activation ( \"softmax\" )( fmap ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"SegFormer"},{"location":"models/heads/allmlp/#segformer","text":"","title":"SegFormer"},{"location":"models/heads/allmlp/#src.model.all_mlp.get_segmentation_module","text":"Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/all_mlp.py def get_segmentation_module ( units : int , n_classes : int , backbone : tf . keras . Model , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" l2_regul = 1e-4 bil = \"bilinear\" he = \"he_uniform\" os4_output , os8_output , os16_output , os32_output = backbone . outputs fmap1 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os4_output ) fmap2 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os8_output ) fmap2 = UpSampling2D ( size = ( 2 , 2 ), interpolation = bil )( fmap2 ) fmap3 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os16_output ) fmap3 = UpSampling2D ( size = ( 4 , 4 ), interpolation = bil )( fmap3 ) fmap4 = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( os32_output ) fmap4 = UpSampling2D ( size = ( 8 , 8 ), interpolation = bil )( fmap4 ) fmap = Concatenate ( axis =- 1 )([ fmap1 , fmap2 , fmap3 , fmap4 ]) fmap = Dense ( units , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( fmap ) fmap = Dense ( n_classes , kernel_initializer = he , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )( fmap ) fmap = UpSampling2D ( size = ( 4 , 4 ), interpolation = bil )( fmap ) out = Activation ( \"softmax\" )( fmap ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"get_segmentation_module()"},{"location":"models/heads/fpn/","text":"Panoptic Feature Pyramid Networks Abstract The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation. ArXiv link ICCV Presentation Architecture FeaturePyramidNetwork ( Layer ) Description of FeaturePyramidNetwork The Feature Pyramid Networks head. Architecture Attributes: Name Type Description conv1 type Conv2D layer. conv2 type Conv2D layer. conv3 type Conv2D layer. conv4 type Conv2D layer. upsample type Upsampling2D layer. Inheritance tf.keras.layers.Layer: Returns: Type Description List[tf.Tensor] A list of feature maps, of dimensions \\([(OS4, 256), (OS8, 256), (OS16, 256), (OS32, 256)]\\) . __init__ ( self , filters = 256 , kernel_size = 1 , strides = 1 , padding = 'same' , kernel_initializer = 'he_uniform' , l2_regul = 0.0001 , * args , ** kwargs ) special Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. Defaults to 256. 256 kernel_size tuple Size of the convolution kernels in each Conv2D layers. Defaults to 1. 1 strides tuple Stride parameter in each Conv2D layers. Defaults to 1. 1 padding str Paddinf parameter in each Conv2D layers. Defaults to \"same\". 'same' kernel_initializer str Kernel initialization method used in each Conv2D layers. Defaults to \"he_uniform\". 'he_uniform' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/feature_pyramids.py def __init__ ( self , filters : int = 256 , kernel_size : int = 1 , strides : int = 1 , padding : str = \"same\" , kernel_initializer : str = \"he_uniform\" , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. Defaults to 256. kernel_size (tuple, optional): Size of the convolution kernels in each `Conv2D` layers. Defaults to 1. strides (tuple, optional): Stride parameter in each `Conv2D` layers. Defaults to 1. padding (str, optional): Paddinf parameter in each `Conv2D` layers. Defaults to \"same\". kernel_initializer (str, optional): Kernel initialization method used in each `Conv2D` layers. Defaults to \"he_uniform\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . strides = strides self . padding = padding self . kernel_initializer = kernel_initializer self . l2_regul = l2_regul build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/feature_pyramids.py def build ( self , input_shape ) -> None : self . conv1 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv2 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv3 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv4 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . upsample = UpSampling2D ( size = ( 2 , 2 ), interpolation = \"bilinear\" ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description List[tensorflow.python.framework.ops.Tensor] A tensor or list/tuple of tensors. Source code in src/model/layers/feature_pyramids.py def call ( self , inputs , training = None ) -> List [ tf . Tensor ]: c2_output , c3_output , c4_output , c5_output = inputs p2_output = self . conv1 ( c2_output ) p3_output = self . conv2 ( c3_output ) p4_output = self . conv3 ( c4_output ) p5_output = self . conv4 ( c5_output ) p4_output = p4_output + self . upsample ( p5_output ) p3_output = p3_output + self . upsample ( p4_output ) p2_output = p2_output + self . upsample ( p3_output ) return [ p2_output , p3_output , p4_output , p5_output ] from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/feature_pyramids.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/feature_pyramids.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"padding\" : self . padding , \"kernel_initializer\" : self . kernel_initializer , \"l2_regul\" : self . l2_regul , }, ) return config SemanticHeadFPN ( Layer ) Description of SemanticHeadFPN The segmentation head added to the FPN. Architecture Attributes: Name Type Description conv1 type Conv2D-GroupNormalization-ReLU layer. conv2 type Conv2D-GroupNormalization-ReLU layer. conv3 type Conv2D-GroupNormalization-ReLU layer. conv4 type Conv2D-GroupNormalization-ReLU layer. conv5 type Conv2D-GroupNormalization-ReLU layer. conv6 type Conv2D-GroupNormalization-ReLU layer. conv7 type Conv2D-GroupNormalization-ReLU layer. upsample type Upsampling2D layer. concat type Concatenate layer. Inheritance tf.keras.layers.Layer Returns: Type Description tf.Tensor An output feature map of size \\(OS4\\) , with 512 filters. __init__ ( self , filters = 128 , kernel_size = 1 , strides = 1 , padding = 'same' , kernel_initializer = 'he_uniform' , l2_regul = 0.0001 , * args , ** kwargs ) special Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. Defaults to 128. 128 kernel_size tuple Size of the convolution kernels in each Conv2D layers. Defaults to 1. 1 strides tuple Stride parameter in each Conv2D layers. Defaults to 1. 1 padding str Paddinf parameter in each Conv2D layers. Defaults to \"same\". 'same' kernel_initializer str Kernel initialization method used in each Conv2D layers. Defaults to \"he_uniform\". 'he_uniform' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/feature_pyramids.py def __init__ ( self , filters : int = 128 , kernel_size : int = 1 , strides : int = 1 , padding : str = \"same\" , kernel_initializer : str = \"he_uniform\" , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. Defaults to 128. kernel_size (tuple, optional): Size of the convolution kernels in each `Conv2D` layers. Defaults to 1. strides (tuple, optional): Stride parameter in each `Conv2D` layers. Defaults to 1. padding (str, optional): Paddinf parameter in each `Conv2D` layers. Defaults to \"same\". kernel_initializer (str, optional): Kernel initialization method used in each `Conv2D` layers. Defaults to \"he_uniform\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . strides = strides self . padding = padding self . kernel_initializer = kernel_initializer self . l2_regul = l2_regul build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/feature_pyramids.py def build ( self , input_shape ) -> None : self . conv1 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg1\" , ) self . conv2 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg2\" , ) self . conv3 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg3\" , ) self . conv4 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg4\" , ) self . conv5 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg5\" , ) self . conv6 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg6\" , ) self . conv7 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg7\" , ) self . upsample = UpSampling2D ( size = ( 2 , 2 ), interpolation = \"bilinear\" ) self . concat = Concatenate ( axis =- 1 ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/feature_pyramids.py def call ( self , inputs , training = None ) -> tf . Tensor : p2_output , p3_output , p4_output , p5_output = inputs p5_output = self . upsample ( self . conv1 ( p5_output )) p5_output = self . upsample ( self . conv2 ( p5_output )) fmap5 = self . upsample ( self . conv3 ( p5_output )) p4_output = self . upsample ( self . conv4 ( p4_output )) fmap4 = self . upsample ( self . conv5 ( p4_output )) fmap3 = self . upsample ( self . conv6 ( p3_output )) fmap2 = self . conv7 ( p2_output ) return self . concat ([ fmap5 , fmap4 , fmap3 , fmap2 ]) from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/feature_pyramids.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/feature_pyramids.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"padding\" : self . padding , \"kernel_initializer\" : self . kernel_initializer , \"l2_regul\" : self . l2_regul , }, ) return config get_segmentation_module ( n_classes , backbone , name ) Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/fpn.py def get_segmentation_module ( n_classes : int , backbone : tf . keras . Model , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" c_outputs = backbone . outputs p_outputs = FeaturePyramidNetwork ()( c_outputs ) fmap = SemanticHeadFPN ()( p_outputs ) fmap = Conv2D ( filters = n_classes , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_uniform\" , )( fmap ) fmap = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" )( fmap ) out = Activation ( \"softmax\" )( fmap ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"FPN"},{"location":"models/heads/fpn/#panoptic-feature-pyramid-networks","text":"Abstract The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation. ArXiv link ICCV Presentation Architecture","title":"Panoptic Feature Pyramid Networks"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.FeaturePyramidNetwork","text":"Description of FeaturePyramidNetwork The Feature Pyramid Networks head. Architecture Attributes: Name Type Description conv1 type Conv2D layer. conv2 type Conv2D layer. conv3 type Conv2D layer. conv4 type Conv2D layer. upsample type Upsampling2D layer. Inheritance tf.keras.layers.Layer: Returns: Type Description List[tf.Tensor] A list of feature maps, of dimensions \\([(OS4, 256), (OS8, 256), (OS16, 256), (OS32, 256)]\\) .","title":"FeaturePyramidNetwork"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.FeaturePyramidNetwork.__init__","text":"Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. Defaults to 256. 256 kernel_size tuple Size of the convolution kernels in each Conv2D layers. Defaults to 1. 1 strides tuple Stride parameter in each Conv2D layers. Defaults to 1. 1 padding str Paddinf parameter in each Conv2D layers. Defaults to \"same\". 'same' kernel_initializer str Kernel initialization method used in each Conv2D layers. Defaults to \"he_uniform\". 'he_uniform' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/feature_pyramids.py def __init__ ( self , filters : int = 256 , kernel_size : int = 1 , strides : int = 1 , padding : str = \"same\" , kernel_initializer : str = \"he_uniform\" , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. Defaults to 256. kernel_size (tuple, optional): Size of the convolution kernels in each `Conv2D` layers. Defaults to 1. strides (tuple, optional): Stride parameter in each `Conv2D` layers. Defaults to 1. padding (str, optional): Paddinf parameter in each `Conv2D` layers. Defaults to \"same\". kernel_initializer (str, optional): Kernel initialization method used in each `Conv2D` layers. Defaults to \"he_uniform\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . strides = strides self . padding = padding self . kernel_initializer = kernel_initializer self . l2_regul = l2_regul","title":"__init__()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.FeaturePyramidNetwork.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/feature_pyramids.py def build ( self , input_shape ) -> None : self . conv1 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv2 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv3 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . conv4 = Conv2D ( filters = self . filters , kernel_size = self . kernel_size , strides = self . strides , padding = self . padding , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ) self . upsample = UpSampling2D ( size = ( 2 , 2 ), interpolation = \"bilinear\" )","title":"build()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.FeaturePyramidNetwork.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description List[tensorflow.python.framework.ops.Tensor] A tensor or list/tuple of tensors. Source code in src/model/layers/feature_pyramids.py def call ( self , inputs , training = None ) -> List [ tf . Tensor ]: c2_output , c3_output , c4_output , c5_output = inputs p2_output = self . conv1 ( c2_output ) p3_output = self . conv2 ( c3_output ) p4_output = self . conv3 ( c4_output ) p5_output = self . conv4 ( c5_output ) p4_output = p4_output + self . upsample ( p5_output ) p3_output = p3_output + self . upsample ( p4_output ) p2_output = p2_output + self . upsample ( p3_output ) return [ p2_output , p3_output , p4_output , p5_output ]","title":"call()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.FeaturePyramidNetwork.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/feature_pyramids.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.FeaturePyramidNetwork.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/feature_pyramids.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"padding\" : self . padding , \"kernel_initializer\" : self . kernel_initializer , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.SemanticHeadFPN","text":"Description of SemanticHeadFPN The segmentation head added to the FPN. Architecture Attributes: Name Type Description conv1 type Conv2D-GroupNormalization-ReLU layer. conv2 type Conv2D-GroupNormalization-ReLU layer. conv3 type Conv2D-GroupNormalization-ReLU layer. conv4 type Conv2D-GroupNormalization-ReLU layer. conv5 type Conv2D-GroupNormalization-ReLU layer. conv6 type Conv2D-GroupNormalization-ReLU layer. conv7 type Conv2D-GroupNormalization-ReLU layer. upsample type Upsampling2D layer. concat type Concatenate layer. Inheritance tf.keras.layers.Layer Returns: Type Description tf.Tensor An output feature map of size \\(OS4\\) , with 512 filters.","title":"SemanticHeadFPN"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.SemanticHeadFPN.__init__","text":"Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. Defaults to 128. 128 kernel_size tuple Size of the convolution kernels in each Conv2D layers. Defaults to 1. 1 strides tuple Stride parameter in each Conv2D layers. Defaults to 1. 1 padding str Paddinf parameter in each Conv2D layers. Defaults to \"same\". 'same' kernel_initializer str Kernel initialization method used in each Conv2D layers. Defaults to \"he_uniform\". 'he_uniform' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/feature_pyramids.py def __init__ ( self , filters : int = 128 , kernel_size : int = 1 , strides : int = 1 , padding : str = \"same\" , kernel_initializer : str = \"he_uniform\" , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. Defaults to 128. kernel_size (tuple, optional): Size of the convolution kernels in each `Conv2D` layers. Defaults to 1. strides (tuple, optional): Stride parameter in each `Conv2D` layers. Defaults to 1. padding (str, optional): Paddinf parameter in each `Conv2D` layers. Defaults to \"same\". kernel_initializer (str, optional): Kernel initialization method used in each `Conv2D` layers. Defaults to \"he_uniform\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . strides = strides self . padding = padding self . kernel_initializer = kernel_initializer self . l2_regul = l2_regul","title":"__init__()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.SemanticHeadFPN.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/feature_pyramids.py def build ( self , input_shape ) -> None : self . conv1 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg1\" , ) self . conv2 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg2\" , ) self . conv3 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg3\" , ) self . conv4 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg4\" , ) self . conv5 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg5\" , ) self . conv6 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg6\" , ) self . conv7 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], name = \"seg7\" , ) self . upsample = UpSampling2D ( size = ( 2 , 2 ), interpolation = \"bilinear\" ) self . concat = Concatenate ( axis =- 1 )","title":"build()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.SemanticHeadFPN.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/feature_pyramids.py def call ( self , inputs , training = None ) -> tf . Tensor : p2_output , p3_output , p4_output , p5_output = inputs p5_output = self . upsample ( self . conv1 ( p5_output )) p5_output = self . upsample ( self . conv2 ( p5_output )) fmap5 = self . upsample ( self . conv3 ( p5_output )) p4_output = self . upsample ( self . conv4 ( p4_output )) fmap4 = self . upsample ( self . conv5 ( p4_output )) fmap3 = self . upsample ( self . conv6 ( p3_output )) fmap2 = self . conv7 ( p2_output ) return self . concat ([ fmap5 , fmap4 , fmap3 , fmap2 ])","title":"call()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.SemanticHeadFPN.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/feature_pyramids.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/heads/fpn/#src.model.layers.feature_pyramids.SemanticHeadFPN.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/feature_pyramids.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"padding\" : self . padding , \"kernel_initializer\" : self . kernel_initializer , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/heads/fpn/#src.model.fpn.get_segmentation_module","text":"Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/fpn.py def get_segmentation_module ( n_classes : int , backbone : tf . keras . Model , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" c_outputs = backbone . outputs p_outputs = FeaturePyramidNetwork ()( c_outputs ) fmap = SemanticHeadFPN ()( p_outputs ) fmap = Conv2D ( filters = n_classes , kernel_size = ( 1 , 1 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_uniform\" , )( fmap ) fmap = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" )( fmap ) out = Activation ( \"softmax\" )( fmap ) return keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"get_segmentation_module()"},{"location":"models/heads/jpu/","text":"FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Abstract Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract high-resolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting high-resolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. ArXiv link Architecture JointPyramidUpsampling ( Layer ) Description of JointPyramidUpsampling Joint Pyramid Upsampling module. Architecture Attributes: Name Type Description conv1 type Conv2D-GroupNormalization-ReLU layer. conv2 type Conv2D-GroupNormalization-ReLU layer. conv3 type Conv2D-GroupNormalization-ReLU layer. conv4 type Conv2D-GroupNormalization-ReLU layer. upsample type Upsampling2D layer. concat type Concatenate layer. sepconv1 type SeparableConv2D-GroupNormalization-ReLU layer. sepconv2 type SeparableConv2D-GroupNormalization-ReLU layer. sepconv4 type SeparableConv2D-GroupNormalization-ReLU layer. sepconv8 type SeparableConv2D-GroupNormalization-ReLU layer. Inheritance tf.keras.layers.Layer: Returns: Type Description tf.Tensor Output feature map, \\((H,W,C)\\) . __init__ ( self , filters = 256 , kernel_size = 3 , strides = 1 , padding = 'same' , kernel_initializer = 'he_uniform' , l2_regul = 0.0001 , * args , ** kwargs ) special Initilization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. Defaults to 256. 256 kernel_size tuple Size of the convolution kernels in each Conv2D layers. Defaults to 3. 3 strides tuple Stride parameter in each Conv2D layers. Defaults to 1. 1 padding str Paddinf parameter in each Conv2D layers. Defaults to \"same\". 'same' kernel_initializer str Kernel initialization method used in each Conv2D layers. Defaults to \"he_uniform\". 'he_uniform' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/joint_pyramid_upsampling.py def __init__ ( self , filters : int = 256 , kernel_size : int = 3 , strides : int = 1 , padding : str = \"same\" , kernel_initializer : str = \"he_uniform\" , l2_regul : float = 1e-4 , * args : List [ Any ], ** kwargs : Dict [ str , Any ], ) -> None : \"\"\"Initilization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. Defaults to 256. kernel_size (tuple, optional): Size of the convolution kernels in each `Conv2D` layers. Defaults to 3. strides (tuple, optional): Stride parameter in each `Conv2D` layers. Defaults to 1. padding (str, optional): Paddinf parameter in each `Conv2D` layers. Defaults to \"same\". kernel_initializer (str, optional): Kernel initialization method used in each `Conv2D` layers. Defaults to \"he_uniform\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . strides = strides self . padding = padding self . kernel_initializer = kernel_initializer self . l2_regul = l2_regul build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/joint_pyramid_upsampling.py def build ( self , input_shape ) -> None : self . conv1 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . conv2 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . conv3 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . conv4 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . upsample = UpSampling2D ( size = ( 2 , 2 ), interpolation = \"bilinear\" ) self . upsample4 = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" ) self . concat = Concatenate ( axis =- 1 ) self . sepconv1 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], ) self . sepconv2 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], ) self . sepconv4 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], ) self . sepconv8 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/joint_pyramid_upsampling.py def call ( self , inputs , training = None ) -> tf . Tensor : c3_output , c4_output , c5_output = inputs fmap3 = self . conv1 ( c3_output ) fmap4 = self . upsample ( self . conv2 ( c4_output )) fmap5 = self . upsample4 ( self . conv3 ( c5_output )) fmap1 = self . concat ([ fmap3 , fmap4 , fmap5 ]) sep_fmap1 = self . sepconv1 ( fmap1 ) sep_fmap2 = self . sepconv2 ( fmap1 ) sep_fmap4 = self . sepconv4 ( fmap1 ) sep_fmap8 = self . sepconv8 ( fmap1 ) fmap2 = self . concat ([ sep_fmap1 , sep_fmap2 , sep_fmap4 , sep_fmap8 ]) return self . conv4 ( fmap2 ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/joint_pyramid_upsampling.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"padding\" : self . padding , \"kernel_initializer\" : self . kernel_initializer , \"l2_regul\" : self . l2_regul , }, ) return config ASPP ( Layer ) Description of ASPP. Attributes: Name Type Description conv1 type Conv2D-BatchNormalization-ReLU layer. conv2 type Conv2D-BatchNormalization-ReLU layer. conv3 type Conv2D-BatchNormalization-ReLU layer. conv4 type Conv2D-BatchNormalization-ReLU layer. conv5 type Conv2D-BatchNormalization-ReLU layer. conv6 type Conv2D-BatchNormalization-ReLU layer. Inheritance tf.keras.layers.Layer: __init__ ( self , filters , l2_regul = 0.0001 , * args , ** kwargs ) special Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/aspp.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . dilation_rate = [ 6 , 12 , 18 ] self . concat = Concatenate ( axis =- 1 ) self . conv1 = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv2 = Sequential ( [ Conv2D ( filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 0 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv3 = Sequential ( [ Conv2D ( filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 1 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv4 = Sequential ( [ Conv2D ( filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 2 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv5 = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv6 = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/aspp.py def build ( self , input_shape ) -> None : _ , height , width , * _ = input_shape self . pooling = AveragePooling2D ( pool_size = ( height , width )) self . upsample = UpSampling2D ( size = ( height , width ), interpolation = \"bilinear\" ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/aspp.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap1 = self . conv1 ( inputs ) fmap2 = self . conv2 ( inputs ) fmap3 = self . conv3 ( inputs ) fmap4 = self . conv4 ( inputs ) fmap_pool = self . pooling ( inputs ) fmap_pool = self . conv5 ( fmap_pool ) fmap_pool = self . upsample ( fmap_pool ) fmap = self . concat ([ fmap_pool , fmap1 , fmap2 , fmap3 , fmap4 ]) return self . conv6 ( fmap ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/aspp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , \"dilation_rate\" : self . dilation_rate , }, ) return config decoder ( fmap_aspp , endpoint , img_height , img_width , filters ) Decoder part of the segmentation model. Parameters: Name Type Description Default fmap_aspp tf.Tensor Input feature map coming from the ASPP module. required endpoint tf.Tensor Input feature map coming from the backbone model, OS4. required img_height int Height of the images in the dataset. required img_width int Width of the images in the dataset. required filters int Number of filters used in each conv_gn_relu layers. required Returns: Type Description Tensor Output feature map. Source code in src/model/jpu.py def decoder ( fmap_aspp : tf . Tensor , endpoint : tf . Tensor , img_height : int , img_width : int , filters : int , ) -> tf . Tensor : \"\"\"Decoder part of the segmentation model. Args: fmap_aspp (tf.Tensor): Input feature map coming from the ASPP module. endpoint (tf.Tensor): Input feature map coming from the backbone model, OS4. img_height (int): Height of the images in the dataset. img_width (int): Width of the images in the dataset. filters (int): Number of filters used in each `conv_gn_relu` layers. Returns: Output feature map. \"\"\" fmap_a = upsampling ( fmap_aspp , img_height / 4 , img_width / 4 ) fmap_b = conv_gn_relu ( endpoint , filters = filters , kernel_size = 1 ) fmap = Concatenate ( axis =- 1 )([ fmap_a , fmap_b ]) return conv_gn_relu ( fmap , filters = filters , kernel_size = 3 ) get_segmentation_module ( n_classes , img_shape , backbone , name ) Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required img_shape List[int] Input shape of the images/masks in the dataset. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/jpu.py def get_segmentation_module ( n_classes : int , img_shape : List [ int ], backbone : tf . keras . Model , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. img_shape (List[int]): Input shape of the images/masks in the dataset. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" img_height , img_width = img_shape [: 2 ] endpoints = backbone . outputs # JPU Module fmap = JointPyramidUpsampling ()( endpoints [ 1 :]) # ASPP Head fmap = ASPP ( filters = 128 )( fmap ) fmap = decoder ( fmap , endpoints [ 0 ], img_height , img_width , 128 ) fmap = Conv2D ( n_classes , ( 3 , 3 ), activation = \"softmax\" , padding = \"same\" , name = \"output_layer\" , )( fmap ) out = upsampling ( fmap , img_height , img_width ) return tf . keras . Model ( inputs = backbone . input , outputs = out , name = name ) upsampling ( fmap , height , width ) Upsampling module. Upsample features maps to the original height, width of the images in the dataset. Get the height, width of the input feature map, and the height width of the original images in the dataset to compute the scale to upsample the feature map. Parameters: Name Type Description Default fmap tf.Tensor Input feature map of the module. required height Union[int, float] Height of the images in the dataset. required width Union[int, float] Width of the images in the dataset. required Returns: Type Description Tensor Output feature map, size \\((H,W,C)\\) . Source code in src/model/jpu.py def upsampling ( fmap : tf . Tensor , height : Union [ int , float ], width : Union [ int , float ], ) -> tf . Tensor : \"\"\"Upsampling module. Upsample features maps to the original height, width of the images in the dataset. Get the height, width of the input feature map, and the height width of the original images in the dataset to compute the scale to upsample the feature map. Args: fmap (tf.Tensor): Input feature map of the module. height (Union[int, float]): Height of the images in the dataset. width (Union[int, float]): Width of the images in the dataset. Returns: Output feature map, size $(H,W,C)$. \"\"\" h_fmap , w_fmap = fmap . shape . as_list ()[ 1 : 3 ] scale = ( int ( height // h_fmap ), int ( width // w_fmap )) return UpSampling2D ( size = scale , interpolation = \"bilinear\" )( fmap )","title":"JPU"},{"location":"models/heads/jpu/#fastfcn-rethinking-dilated-convolution-in-the-backbone-for-semantic-segmentation","text":"Abstract Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract high-resolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting high-resolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. ArXiv link Architecture","title":"FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation"},{"location":"models/heads/jpu/#src.model.layers.joint_pyramid_upsampling.JointPyramidUpsampling","text":"Description of JointPyramidUpsampling Joint Pyramid Upsampling module. Architecture Attributes: Name Type Description conv1 type Conv2D-GroupNormalization-ReLU layer. conv2 type Conv2D-GroupNormalization-ReLU layer. conv3 type Conv2D-GroupNormalization-ReLU layer. conv4 type Conv2D-GroupNormalization-ReLU layer. upsample type Upsampling2D layer. concat type Concatenate layer. sepconv1 type SeparableConv2D-GroupNormalization-ReLU layer. sepconv2 type SeparableConv2D-GroupNormalization-ReLU layer. sepconv4 type SeparableConv2D-GroupNormalization-ReLU layer. sepconv8 type SeparableConv2D-GroupNormalization-ReLU layer. Inheritance tf.keras.layers.Layer: Returns: Type Description tf.Tensor Output feature map, \\((H,W,C)\\) .","title":"JointPyramidUpsampling"},{"location":"models/heads/jpu/#src.model.layers.joint_pyramid_upsampling.JointPyramidUpsampling.__init__","text":"Initilization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. Defaults to 256. 256 kernel_size tuple Size of the convolution kernels in each Conv2D layers. Defaults to 3. 3 strides tuple Stride parameter in each Conv2D layers. Defaults to 1. 1 padding str Paddinf parameter in each Conv2D layers. Defaults to \"same\". 'same' kernel_initializer str Kernel initialization method used in each Conv2D layers. Defaults to \"he_uniform\". 'he_uniform' l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/joint_pyramid_upsampling.py def __init__ ( self , filters : int = 256 , kernel_size : int = 3 , strides : int = 1 , padding : str = \"same\" , kernel_initializer : str = \"he_uniform\" , l2_regul : float = 1e-4 , * args : List [ Any ], ** kwargs : Dict [ str , Any ], ) -> None : \"\"\"Initilization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. Defaults to 256. kernel_size (tuple, optional): Size of the convolution kernels in each `Conv2D` layers. Defaults to 3. strides (tuple, optional): Stride parameter in each `Conv2D` layers. Defaults to 1. padding (str, optional): Paddinf parameter in each `Conv2D` layers. Defaults to \"same\". kernel_initializer (str, optional): Kernel initialization method used in each `Conv2D` layers. Defaults to \"he_uniform\". l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . strides = strides self . padding = padding self . kernel_initializer = kernel_initializer self . l2_regul = l2_regul","title":"__init__()"},{"location":"models/heads/jpu/#src.model.layers.joint_pyramid_upsampling.JointPyramidUpsampling.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/joint_pyramid_upsampling.py def build ( self , input_shape ) -> None : self . conv1 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . conv2 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . conv3 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . conv4 = Sequential ( [ Conv2D ( filters = self . filters , kernel_size = self . kernel_size , padding = self . padding , use_bias = False , kernel_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), ), tfa . layers . GroupNormalization (), ReLU (), ], ) self . upsample = UpSampling2D ( size = ( 2 , 2 ), interpolation = \"bilinear\" ) self . upsample4 = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" ) self . concat = Concatenate ( axis =- 1 ) self . sepconv1 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], ) self . sepconv2 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], ) self . sepconv4 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], ) self . sepconv8 = Sequential ( [ SeparableConv2D ( filters = self . filters , depth_multiplier = 1 , kernel_size = self . kernel_size , padding = self . padding , strides = self . strides , dilation_rate = 1 , depthwise_initializer = self . kernel_initializer , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = self . l2_regul ), use_bias = False , ), BatchNormalization (), ReLU (), ], )","title":"build()"},{"location":"models/heads/jpu/#src.model.layers.joint_pyramid_upsampling.JointPyramidUpsampling.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/joint_pyramid_upsampling.py def call ( self , inputs , training = None ) -> tf . Tensor : c3_output , c4_output , c5_output = inputs fmap3 = self . conv1 ( c3_output ) fmap4 = self . upsample ( self . conv2 ( c4_output )) fmap5 = self . upsample4 ( self . conv3 ( c5_output )) fmap1 = self . concat ([ fmap3 , fmap4 , fmap5 ]) sep_fmap1 = self . sepconv1 ( fmap1 ) sep_fmap2 = self . sepconv2 ( fmap1 ) sep_fmap4 = self . sepconv4 ( fmap1 ) sep_fmap8 = self . sepconv8 ( fmap1 ) fmap2 = self . concat ([ sep_fmap1 , sep_fmap2 , sep_fmap4 , sep_fmap8 ]) return self . conv4 ( fmap2 )","title":"call()"},{"location":"models/heads/jpu/#src.model.layers.joint_pyramid_upsampling.JointPyramidUpsampling.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/joint_pyramid_upsampling.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"padding\" : self . padding , \"kernel_initializer\" : self . kernel_initializer , \"l2_regul\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/heads/jpu/#src.model.layers.aspp.ASPP","text":"Description of ASPP. Attributes: Name Type Description conv1 type Conv2D-BatchNormalization-ReLU layer. conv2 type Conv2D-BatchNormalization-ReLU layer. conv3 type Conv2D-BatchNormalization-ReLU layer. conv4 type Conv2D-BatchNormalization-ReLU layer. conv5 type Conv2D-BatchNormalization-ReLU layer. conv6 type Conv2D-BatchNormalization-ReLU layer. Inheritance tf.keras.layers.Layer:","title":"ASPP"},{"location":"models/heads/jpu/#src.model.layers.aspp.ASPP.__init__","text":"Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/aspp.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . dilation_rate = [ 6 , 12 , 18 ] self . concat = Concatenate ( axis =- 1 ) self . conv1 = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv2 = Sequential ( [ Conv2D ( filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 0 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv3 = Sequential ( [ Conv2D ( filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 1 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv4 = Sequential ( [ Conv2D ( filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 2 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv5 = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv6 = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], )","title":"__init__()"},{"location":"models/heads/jpu/#src.model.layers.aspp.ASPP.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/aspp.py def build ( self , input_shape ) -> None : _ , height , width , * _ = input_shape self . pooling = AveragePooling2D ( pool_size = ( height , width )) self . upsample = UpSampling2D ( size = ( height , width ), interpolation = \"bilinear\" )","title":"build()"},{"location":"models/heads/jpu/#src.model.layers.aspp.ASPP.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/aspp.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap1 = self . conv1 ( inputs ) fmap2 = self . conv2 ( inputs ) fmap3 = self . conv3 ( inputs ) fmap4 = self . conv4 ( inputs ) fmap_pool = self . pooling ( inputs ) fmap_pool = self . conv5 ( fmap_pool ) fmap_pool = self . upsample ( fmap_pool ) fmap = self . concat ([ fmap_pool , fmap1 , fmap2 , fmap3 , fmap4 ]) return self . conv6 ( fmap )","title":"call()"},{"location":"models/heads/jpu/#src.model.layers.aspp.ASPP.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/aspp.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , \"dilation_rate\" : self . dilation_rate , }, ) return config","title":"get_config()"},{"location":"models/heads/jpu/#src.model.jpu.decoder","text":"Decoder part of the segmentation model. Parameters: Name Type Description Default fmap_aspp tf.Tensor Input feature map coming from the ASPP module. required endpoint tf.Tensor Input feature map coming from the backbone model, OS4. required img_height int Height of the images in the dataset. required img_width int Width of the images in the dataset. required filters int Number of filters used in each conv_gn_relu layers. required Returns: Type Description Tensor Output feature map. Source code in src/model/jpu.py def decoder ( fmap_aspp : tf . Tensor , endpoint : tf . Tensor , img_height : int , img_width : int , filters : int , ) -> tf . Tensor : \"\"\"Decoder part of the segmentation model. Args: fmap_aspp (tf.Tensor): Input feature map coming from the ASPP module. endpoint (tf.Tensor): Input feature map coming from the backbone model, OS4. img_height (int): Height of the images in the dataset. img_width (int): Width of the images in the dataset. filters (int): Number of filters used in each `conv_gn_relu` layers. Returns: Output feature map. \"\"\" fmap_a = upsampling ( fmap_aspp , img_height / 4 , img_width / 4 ) fmap_b = conv_gn_relu ( endpoint , filters = filters , kernel_size = 1 ) fmap = Concatenate ( axis =- 1 )([ fmap_a , fmap_b ]) return conv_gn_relu ( fmap , filters = filters , kernel_size = 3 )","title":"decoder()"},{"location":"models/heads/jpu/#src.model.jpu.get_segmentation_module","text":"Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required img_shape List[int] Input shape of the images/masks in the dataset. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/jpu.py def get_segmentation_module ( n_classes : int , img_shape : List [ int ], backbone : tf . keras . Model , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. img_shape (List[int]): Input shape of the images/masks in the dataset. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" img_height , img_width = img_shape [: 2 ] endpoints = backbone . outputs # JPU Module fmap = JointPyramidUpsampling ()( endpoints [ 1 :]) # ASPP Head fmap = ASPP ( filters = 128 )( fmap ) fmap = decoder ( fmap , endpoints [ 0 ], img_height , img_width , 128 ) fmap = Conv2D ( n_classes , ( 3 , 3 ), activation = \"softmax\" , padding = \"same\" , name = \"output_layer\" , )( fmap ) out = upsampling ( fmap , img_height , img_width ) return tf . keras . Model ( inputs = backbone . input , outputs = out , name = name )","title":"get_segmentation_module()"},{"location":"models/heads/jpu/#src.model.jpu.upsampling","text":"Upsampling module. Upsample features maps to the original height, width of the images in the dataset. Get the height, width of the input feature map, and the height width of the original images in the dataset to compute the scale to upsample the feature map. Parameters: Name Type Description Default fmap tf.Tensor Input feature map of the module. required height Union[int, float] Height of the images in the dataset. required width Union[int, float] Width of the images in the dataset. required Returns: Type Description Tensor Output feature map, size \\((H,W,C)\\) . Source code in src/model/jpu.py def upsampling ( fmap : tf . Tensor , height : Union [ int , float ], width : Union [ int , float ], ) -> tf . Tensor : \"\"\"Upsampling module. Upsample features maps to the original height, width of the images in the dataset. Get the height, width of the input feature map, and the height width of the original images in the dataset to compute the scale to upsample the feature map. Args: fmap (tf.Tensor): Input feature map of the module. height (Union[int, float]): Height of the images in the dataset. width (Union[int, float]): Width of the images in the dataset. Returns: Output feature map, size $(H,W,C)$. \"\"\" h_fmap , w_fmap = fmap . shape . as_list ()[ 1 : 3 ] scale = ( int ( height // h_fmap ), int ( width // w_fmap )) return UpSampling2D ( size = scale , interpolation = \"bilinear\" )( fmap )","title":"upsampling()"},{"location":"models/heads/ksac/","text":"See More Than Once -- Kernel-Sharing Atrous Convolution for Semantic Segmentation Abstract The state-of-the-art semantic segmentation solutions usually leverage different receptive fields via multiple parallel branches to handle objects with different sizes. However, employing separate kernels for individual branches degrades the generalization and representation abilities of the network, and the number of parameters increases linearly in the number of branches. To tackle this problem, we propose a novel network structure namely Kernel-Sharing Atrous Convolution (KSAC), where branches of different receptive fields share the same kernel, i.e., let a single kernel see the input feature maps more than once with different receptive fields, to facilitate communication among branches and perform feature augmentation inside the network. Experiments conducted on the benchmark PASCAL VOC 2012 dataset show that the proposed sharing strategy can not only boost a network s generalization and representation abilities but also reduce the model complexity significantly. Specifically, on the validation set, whe compared with DeepLabV3+ equipped with MobileNetv2 backbone, 33% of parameters are reduced together with an mIOU improvement of 0.6%. When Xception is used as the backbone, the mIOU is elevated from 83.34% to 85.96% with about 10M parameters saved. In addition, different from the widely used ASPP structure, our proposed KSAC is able to further improve the mIOU by taking benefit of wider context with larger atrous rates. Finally, our KSAC achieves mIOUs of 88.1% and 45.47% on the PASCAL VOC 2012 test set and ADE20K dataset, respectively. Our full code will be released on the Github. ArXiv link Architecture KSAConv2D ( Layer ) Description of KSAConv2D Attributes: Name Type Description relu type ReLU function. conv1 type Conv2D layer. conv2 type Conv2D layer. conv3 type Conv2D layer. concat type Concatenate layer. Inheritance tf.keras.layers.Layer: __init__ ( self , filters , * args , ** kwargs ) special Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D and SharedDilatedConv layers. required Source code in src/model/layers/shared_kernels.py def __init__ ( self , filters : int , * args : List [ Any ], ** kwargs : Dict [ Any , Any ], ) -> None : \"\"\"Initialization of the class. Args: filters (int): Number of filters in each `Conv2D` and `SharedDilatedConv` layers. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . relu = ReLU () self . conv1 = Conv2D ( filters = filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_uniform\" , use_bias = False , ) self . conv2 = Conv2D ( filters = filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_uniform\" , use_bias = False , ) self . conv3 = Conv2D ( filters = filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_uniform\" , use_bias = False , ) self . concat = Concatenate ( axis =- 1 ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/shared_kernels.py def build ( self , input_shape ) -> None : _ , height , width , * _ = input_shape self . shared_conv = SharedDilatedConv ( filters = self . filters , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), kernel_initializer = \"he_uniform\" , use_bias = False , ) self . bn1 = BatchNormalization () self . bn2 = BatchNormalization () self . pooling = AveragePooling2D ( pool_size = ( height , width )) self . upsample = UpSampling2D ( size = ( height , width ), interpolation = \"bilinear\" ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/shared_kernels.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap1 = self . conv1 ( inputs ) fmap1 = self . bn1 ( fmap1 ) fmap2 = self . shared_conv ( inputs ) fmap3 = self . pooling ( inputs ) fmap3 = self . conv2 ( fmap3 ) fmap3 = self . upsample ( fmap3 ) fmap = self . concat ([ fmap1 , fmap2 , fmap3 ]) fmap = self . conv3 ( fmap ) fmap = self . bn2 ( fmap ) return self . relu ( fmap ) from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/shared_kernels.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/shared_kernels.py def get_config ( self ) -> Dict [ str , Any ]: base_config = super () . get_config () return { ** base_config , \"filters\" : self . filters , } SharedDilatedConv ( Layer ) Description of SharedDilatedConv Attributes: Name Type Description w type Weight of the tf.nn.conv2d operation. b type Bias of the tf.nn.conv2d operation. dilation_rates type Dilations rates used for the tf.nn.conv2d operation. relu type Inheritance tf.keras.layers.Layer __init__ ( self , filters , kernel_size , strides , kernel_initializer , use_bias , bias_initializer = None , * args , ** kwargs ) special [summary] Parameters: Name Type Description Default filters int Number of filters in each tf.nn.conv2d operations. required kernel_size tuple Size of the convolution kernels in each tf.nn.conv2d operation. required strides tuple Stride parameter in each tf.nn.conv2d operation. required kernel_initializer str Kernel initialization method used in each tf.nn.conv2d operation. required use_bias bool Determine whetther or not use bias. required bias_initializer [type] Bias initialization method used in each tf.nn.conv2d operation.. Defaults to None. None Source code in src/model/layers/shared_kernels.py def __init__ ( self , filters : int , kernel_size : Tuple [ int , int ], strides : Tuple [ int , int ], kernel_initializer : str , use_bias : bool , bias_initializer = None , * args , ** kwargs , ) -> None : \"\"\"[summary] Args: filters (int, optional): Number of filters in each `tf.nn.conv2d` operations. kernel_size (tuple, optional): Size of the convolution kernels in each `tf.nn.conv2d` operation. strides (tuple, optional): Stride parameter in each `tf.nn.conv2d` operation. kernel_initializer (str, optional): Kernel initialization method used in each `tf.nn.conv2d` operation. use_bias (bool): Determine whetther or not use bias. bias_initializer ([type], optional): Bias initialization method used in each `tf.nn.conv2d` operation.. Defaults to None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . kernel_initializer = kernel_initializer self . bias_initializer = bias_initializer self . strides = strides self . use_bias = use_bias self . kernel = None self . b = None self . dilation_rates = [ 6 , 12 , 18 ] self . relu = ReLU () build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/shared_kernels.py def build ( self , input_shape ) -> None : * _ , n_channels = input_shape self . kernel = self . add_weight ( name = \"kernel\" , shape = ( * self . kernel_size , n_channels , self . filters ), initializer = self . kernel_initializer , trainable = True , dtype = \"float32\" , ) if self . use_bias : self . b = self . add_weight ( name = \"bias\" , shape = ( self . filters ,), initializer = self . bias_initializer , trainable = True , dtype = \"float32\" , ) else : self . b = None self . bn1 = BatchNormalization () self . bn2 = BatchNormalization () self . bn3 = BatchNormalization () call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/shared_kernels.py def call ( self , inputs , training = None ) -> tf . Tensor : x1 = tf . nn . conv2d ( inputs , self . kernel , padding = \"SAME\" , strides = self . strides , dilations = self . dilation_rates [ 0 ], ) if self . use_bias : x1 = x1 + self . b x1 = self . relu ( self . bn1 ( x1 )) x2 = tf . nn . conv2d ( inputs , self . kernel , padding = \"SAME\" , strides = self . strides , dilations = self . dilation_rates [ 1 ], ) if self . use_bias : x2 = x2 + self . b x2 = self . relu ( self . bn2 ( x2 )) x3 = tf . nn . conv2d ( inputs , self . kernel , padding = \"SAME\" , strides = self . strides , dilations = self . dilation_rates [ 2 ], ) if self . use_bias : x3 = x3 + self . b x3 = self . relu ( self . bn2 ( x3 )) return x1 + x2 + x3 from_config ( config ) classmethod Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/shared_kernels.py @classmethod def from_config ( cls , config ): return cls ( ** config ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/shared_kernels.py def get_config ( self ) -> Dict [ str , Any ]: base_config = super () . get_config () return { ** base_config , \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"kernel_initializer\" : self . kernel_initializer , \"bias_initializer\" : self . bias_initializer , \"use_bias\" : self . use_bias , } decoder ( fmap1 , fmap2 , filters ) [summary] Parameters: Name Type Description Default fmap1 [type] [description] required fmap2 [type] [description] required filters [type] [description] required Returns: Type Description [type] [description] Source code in src/model/ksac.py def decoder ( fmap1 , fmap2 , filters ): \"\"\"[summary] Args: fmap1 ([type]): [description] fmap2 ([type]): [description] filters ([type]): [description] Returns: [type]: [description] \"\"\" fmap1 = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" )( fmap1 ) fmap2 = conv_bn_relu ( tensor = fmap2 , filters = filters , kernel_size = 1 , name = \"decoder1\" ) fmap = Concatenate ( axis =- 1 )([ fmap1 , fmap2 ]) return conv_bn_relu ( tensor = fmap , filters = filters , kernel_size = 3 , name = \"decoder2\" ) get_segmentation_module ( n_classes , backbone , name , ksac_filters , decoder_filters ) Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/ksac.py def get_segmentation_module ( n_classes : int , backbone : tf . keras . Model , name : str , ksac_filters : int , decoder_filters : int , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" c2_output , _ , c4_output , _ = backbone . outputs fm = KSAConv2D ( filters = ksac_filters )( c4_output ) fmap = decoder ( fm , c2_output , filters = decoder_filters ) fmap = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" )( fmap ) fmap = Conv2D ( filters = n_classes , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_uniform\" , )( fmap ) out = Activation ( \"softmax\" )( fmap ) return tf . keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"KSAC"},{"location":"models/heads/ksac/#see-more-than-once-kernel-sharing-atrous-convolution-for-semantic-segmentation","text":"Abstract The state-of-the-art semantic segmentation solutions usually leverage different receptive fields via multiple parallel branches to handle objects with different sizes. However, employing separate kernels for individual branches degrades the generalization and representation abilities of the network, and the number of parameters increases linearly in the number of branches. To tackle this problem, we propose a novel network structure namely Kernel-Sharing Atrous Convolution (KSAC), where branches of different receptive fields share the same kernel, i.e., let a single kernel see the input feature maps more than once with different receptive fields, to facilitate communication among branches and perform feature augmentation inside the network. Experiments conducted on the benchmark PASCAL VOC 2012 dataset show that the proposed sharing strategy can not only boost a network s generalization and representation abilities but also reduce the model complexity significantly. Specifically, on the validation set, whe compared with DeepLabV3+ equipped with MobileNetv2 backbone, 33% of parameters are reduced together with an mIOU improvement of 0.6%. When Xception is used as the backbone, the mIOU is elevated from 83.34% to 85.96% with about 10M parameters saved. In addition, different from the widely used ASPP structure, our proposed KSAC is able to further improve the mIOU by taking benefit of wider context with larger atrous rates. Finally, our KSAC achieves mIOUs of 88.1% and 45.47% on the PASCAL VOC 2012 test set and ADE20K dataset, respectively. Our full code will be released on the Github. ArXiv link Architecture","title":"See More Than Once -- Kernel-Sharing Atrous Convolution for Semantic Segmentation"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.KSAConv2D","text":"Description of KSAConv2D Attributes: Name Type Description relu type ReLU function. conv1 type Conv2D layer. conv2 type Conv2D layer. conv3 type Conv2D layer. concat type Concatenate layer. Inheritance tf.keras.layers.Layer:","title":"KSAConv2D"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.KSAConv2D.__init__","text":"Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D and SharedDilatedConv layers. required Source code in src/model/layers/shared_kernels.py def __init__ ( self , filters : int , * args : List [ Any ], ** kwargs : Dict [ Any , Any ], ) -> None : \"\"\"Initialization of the class. Args: filters (int): Number of filters in each `Conv2D` and `SharedDilatedConv` layers. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . relu = ReLU () self . conv1 = Conv2D ( filters = filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_uniform\" , use_bias = False , ) self . conv2 = Conv2D ( filters = filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_uniform\" , use_bias = False , ) self . conv3 = Conv2D ( filters = filters , kernel_size = 1 , strides = 1 , padding = \"same\" , kernel_initializer = \"he_uniform\" , use_bias = False , ) self . concat = Concatenate ( axis =- 1 )","title":"__init__()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.KSAConv2D.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/shared_kernels.py def build ( self , input_shape ) -> None : _ , height , width , * _ = input_shape self . shared_conv = SharedDilatedConv ( filters = self . filters , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), kernel_initializer = \"he_uniform\" , use_bias = False , ) self . bn1 = BatchNormalization () self . bn2 = BatchNormalization () self . pooling = AveragePooling2D ( pool_size = ( height , width )) self . upsample = UpSampling2D ( size = ( height , width ), interpolation = \"bilinear\" )","title":"build()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.KSAConv2D.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/shared_kernels.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap1 = self . conv1 ( inputs ) fmap1 = self . bn1 ( fmap1 ) fmap2 = self . shared_conv ( inputs ) fmap3 = self . pooling ( inputs ) fmap3 = self . conv2 ( fmap3 ) fmap3 = self . upsample ( fmap3 ) fmap = self . concat ([ fmap1 , fmap2 , fmap3 ]) fmap = self . conv3 ( fmap ) fmap = self . bn2 ( fmap ) return self . relu ( fmap )","title":"call()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.KSAConv2D.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/shared_kernels.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.KSAConv2D.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/shared_kernels.py def get_config ( self ) -> Dict [ str , Any ]: base_config = super () . get_config () return { ** base_config , \"filters\" : self . filters , }","title":"get_config()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.SharedDilatedConv","text":"Description of SharedDilatedConv Attributes: Name Type Description w type Weight of the tf.nn.conv2d operation. b type Bias of the tf.nn.conv2d operation. dilation_rates type Dilations rates used for the tf.nn.conv2d operation. relu type Inheritance tf.keras.layers.Layer","title":"SharedDilatedConv"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.SharedDilatedConv.__init__","text":"[summary] Parameters: Name Type Description Default filters int Number of filters in each tf.nn.conv2d operations. required kernel_size tuple Size of the convolution kernels in each tf.nn.conv2d operation. required strides tuple Stride parameter in each tf.nn.conv2d operation. required kernel_initializer str Kernel initialization method used in each tf.nn.conv2d operation. required use_bias bool Determine whetther or not use bias. required bias_initializer [type] Bias initialization method used in each tf.nn.conv2d operation.. Defaults to None. None Source code in src/model/layers/shared_kernels.py def __init__ ( self , filters : int , kernel_size : Tuple [ int , int ], strides : Tuple [ int , int ], kernel_initializer : str , use_bias : bool , bias_initializer = None , * args , ** kwargs , ) -> None : \"\"\"[summary] Args: filters (int, optional): Number of filters in each `tf.nn.conv2d` operations. kernel_size (tuple, optional): Size of the convolution kernels in each `tf.nn.conv2d` operation. strides (tuple, optional): Stride parameter in each `tf.nn.conv2d` operation. kernel_initializer (str, optional): Kernel initialization method used in each `tf.nn.conv2d` operation. use_bias (bool): Determine whetther or not use bias. bias_initializer ([type], optional): Bias initialization method used in each `tf.nn.conv2d` operation.. Defaults to None. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . kernel_size = kernel_size self . kernel_initializer = kernel_initializer self . bias_initializer = bias_initializer self . strides = strides self . use_bias = use_bias self . kernel = None self . b = None self . dilation_rates = [ 6 , 12 , 18 ] self . relu = ReLU ()","title":"__init__()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.SharedDilatedConv.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/shared_kernels.py def build ( self , input_shape ) -> None : * _ , n_channels = input_shape self . kernel = self . add_weight ( name = \"kernel\" , shape = ( * self . kernel_size , n_channels , self . filters ), initializer = self . kernel_initializer , trainable = True , dtype = \"float32\" , ) if self . use_bias : self . b = self . add_weight ( name = \"bias\" , shape = ( self . filters ,), initializer = self . bias_initializer , trainable = True , dtype = \"float32\" , ) else : self . b = None self . bn1 = BatchNormalization () self . bn2 = BatchNormalization () self . bn3 = BatchNormalization ()","title":"build()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.SharedDilatedConv.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/shared_kernels.py def call ( self , inputs , training = None ) -> tf . Tensor : x1 = tf . nn . conv2d ( inputs , self . kernel , padding = \"SAME\" , strides = self . strides , dilations = self . dilation_rates [ 0 ], ) if self . use_bias : x1 = x1 + self . b x1 = self . relu ( self . bn1 ( x1 )) x2 = tf . nn . conv2d ( inputs , self . kernel , padding = \"SAME\" , strides = self . strides , dilations = self . dilation_rates [ 1 ], ) if self . use_bias : x2 = x2 + self . b x2 = self . relu ( self . bn2 ( x2 )) x3 = tf . nn . conv2d ( inputs , self . kernel , padding = \"SAME\" , strides = self . strides , dilations = self . dilation_rates [ 2 ], ) if self . use_bias : x3 = x3 + self . b x3 = self . relu ( self . bn2 ( x3 )) return x1 + x2 + x3","title":"call()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.SharedDilatedConv.from_config","text":"Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Type Description Default config A Python dictionary, typically the output of get_config. required Returns: Type Description A layer instance. Source code in src/model/layers/shared_kernels.py @classmethod def from_config ( cls , config ): return cls ( ** config )","title":"from_config()"},{"location":"models/heads/ksac/#src.model.layers.shared_kernels.SharedDilatedConv.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/shared_kernels.py def get_config ( self ) -> Dict [ str , Any ]: base_config = super () . get_config () return { ** base_config , \"filters\" : self . filters , \"kernel_size\" : self . kernel_size , \"strides\" : self . strides , \"kernel_initializer\" : self . kernel_initializer , \"bias_initializer\" : self . bias_initializer , \"use_bias\" : self . use_bias , }","title":"get_config()"},{"location":"models/heads/ksac/#src.model.ksac.decoder","text":"[summary] Parameters: Name Type Description Default fmap1 [type] [description] required fmap2 [type] [description] required filters [type] [description] required Returns: Type Description [type] [description] Source code in src/model/ksac.py def decoder ( fmap1 , fmap2 , filters ): \"\"\"[summary] Args: fmap1 ([type]): [description] fmap2 ([type]): [description] filters ([type]): [description] Returns: [type]: [description] \"\"\" fmap1 = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" )( fmap1 ) fmap2 = conv_bn_relu ( tensor = fmap2 , filters = filters , kernel_size = 1 , name = \"decoder1\" ) fmap = Concatenate ( axis =- 1 )([ fmap1 , fmap2 ]) return conv_bn_relu ( tensor = fmap , filters = filters , kernel_size = 3 , name = \"decoder2\" )","title":"decoder()"},{"location":"models/heads/ksac/#src.model.ksac.get_segmentation_module","text":"Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/ksac.py def get_segmentation_module ( n_classes : int , backbone : tf . keras . Model , name : str , ksac_filters : int , decoder_filters : int , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" c2_output , _ , c4_output , _ = backbone . outputs fm = KSAConv2D ( filters = ksac_filters )( c4_output ) fmap = decoder ( fm , c2_output , filters = decoder_filters ) fmap = UpSampling2D ( size = ( 4 , 4 ), interpolation = \"bilinear\" )( fmap ) fmap = Conv2D ( filters = n_classes , kernel_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = \"same\" , kernel_initializer = \"he_uniform\" , )( fmap ) out = Activation ( \"softmax\" )( fmap ) return tf . keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"get_segmentation_module()"},{"location":"models/heads/ocnet/","text":"OCNet: Object Context for Semantic Segmentation Abstract In this paper, we address the semantic segmentation task with a new context aggregation scheme named object context, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise. We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices. To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling and atrous spatial pyramid pooling. We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff ArXiv link Architecture ASPP_OC ( Layer ) Description of ASPP_OC Attributes: Name Type Description dilation_rate type Dilations rates used for the Conv2D layers. isa_block1 type ISA2D layer. isa_block2 type ISA2D layer. isa_block3 type ISA2D layer. isa_block4 type ISA2D layer. concat type Concatenate layer. conv1 type Conv2D-BatchNormalization-ReLU layer. conv2 type Conv2D-BatchNormalization-ReLU layer. conv3 type Conv2D-BatchNormalization-ReLU layer. conv4 type Conv2D-BatchNormalization-ReLU layer. conv5 type Conv2D-BatchNormalization-ReLU layer. conv6 type Conv2D-BatchNormalization-ReLU layer. Inheritance tf.keras.layers.Layer: __init__ ( self , filters , l2_regul = 0.0001 , * args , ** kwargs ) special Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/object_context.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . dilation_rate = [ 6 , 12 , 18 ] self . isa_block1 = ISA2D ( 8 , 8 ) self . isa_block2 = ISA2D ( 8 , 8 ) self . isa_block3 = ISA2D ( 8 , 8 ) self . isa_block4 = ISA2D ( 8 , 8 ) self . concat = Concatenate ( axis =- 1 ) self . conv1 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv2 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 0 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv3 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 1 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv4 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 2 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv5 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv6 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/object_context.py def build ( self , input_shape ) -> None : _ , height , width , * _ = input_shape self . pooling = AveragePooling2D ( pool_size = ( height , width )) self . upsample = UpSampling2D ( size = ( height , width ), interpolation = \"bilinear\" ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap1 = self . conv1 ( inputs ) fmap1 = self . isa_block1 ( fmap1 ) fmap2 = self . conv2 ( inputs ) fmap2 = self . isa_block2 ( fmap2 ) fmap3 = self . conv3 ( inputs ) fmap3 = self . isa_block3 ( fmap3 ) fmap4 = self . conv4 ( inputs ) fmap4 = self . isa_block4 ( fmap4 ) fmap_pool = self . pooling ( inputs ) fmap_pool = self . conv5 ( fmap_pool ) fmap_pool = self . upsample ( fmap_pool ) fmap = self . concat ([ fmap_pool , fmap1 , fmap2 , fmap3 , fmap4 ]) return self . conv6 ( fmap ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , \"dilation_rate\" : self . dilation_rate , }, ) return config BaseOC ( Layer ) Description of Base_OC_Module Attributes: Name Type Description isa_block type ISA2D layer. concat type Concatenate layer. conv type Conv2D-BatchNormalization-ReLU layer. Inheritance tf.keras.layers.Layer: __init__ ( self , filters , l2_regul = 0.0001 , * args , ** kwargs ) special Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/object_context.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . isa_block = ISA2D ( 8 , 8 ) self . concat = Concatenate ( axis =- 1 ) self . conv = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : attention = self . isa_block ( inputs ) fmap = self . concat ([ attention , inputs ]) return self . conv ( fmap ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul }) return config ISA2D ( Layer ) Description of ISA2D Inheritance tf.keras.layers.Layer: __init__ ( self , P_h , P_w , * args , ** kwargs ) special Initilization of the class. Parameters: Name Type Description Default P_h int Number of partitions wanted for the height axis. required P_w int Number of partitions wanted for the width axis. required Source code in src/model/layers/object_context.py def __init__ ( self , P_h : int , P_w : int , * args , ** kwargs ) -> None : \"\"\"Initilization of the class. Args: P_h (int): Number of partitions wanted for the height axis. P_w (int): Number of partitions wanted for the width axis. \"\"\" super () . __init__ ( * args , ** kwargs ) self . P_h = P_h self . P_w = P_w build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/object_context.py def build ( self , input_shape ) -> None : self . attention1 = SelfAttention2D ( input_shape [ - 1 ]) self . attention2 = SelfAttention2D ( input_shape [ - 1 ]) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : _ , H , W , C = tf . keras . backend . int_shape ( inputs ) Q_h , Q_w = H // self . P_h , W // self . P_w # global relation fmap = tf . reshape ( inputs , [ - 1 , Q_h , self . P_h , Q_w , self . P_w , C ]) fmap = Permute (( 4 , 1 , 2 , 3 , 5 ))( fmap ) fmap = tf . reshape ( fmap , [ - 1 , Q_h , Q_w , C ]) fmap = self . attention1 ( fmap ) # local relation fmap = tf . reshape ( fmap , [ - 1 , self . P_h , self . P_w , Q_h , Q_w , C ]) fmap = Permute (( 3 , 4 , 1 , 2 , 5 ))( fmap ) fmap = tf . reshape ( fmap , [ - 1 , self . P_h , self . P_w , C ]) fmap = self . attention2 ( fmap ) # reshape fmap = tf . reshape ( fmap , [ - 1 , Q_h , Q_w , self . P_h , self . P_w , C ]) fmap = Permute (( 3 , 1 , 2 , 4 , 5 ))( fmap ) return tf . reshape ( fmap , [ - 1 , H , W , C ]) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"P_h\" : self . P_h , \"P_w\" : self . P_w }) return config SelfAttention2D ( Layer ) Description of SelfAttention2D Attributes: Name Type Description softmax type Softmax function. theta type [Conv2D-BatchNormalization-ReLU]x2 layer. phi type [Conv2D-BatchNormalization-ReLU]x2 layer. gamma type Conv2D layer. rho type Conv2D layer. Inheritance tf.keras.layers.Layer: __init__ ( self , filters , l2_regul = 0.0001 , * args , ** kwargs ) special Initilization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/object_context.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs ) -> None : \"\"\"Initilization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . regul = tf . sqrt ( 2 / self . filters ) self . softmax = tf . keras . layers . Activation ( \"softmax\" ) self . theta = Sequential ( [ Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . phi = Sequential ( [ Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . gamma = Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ) self . rho = Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ) call ( self , inputs , training = None ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : out_phi = self . phi ( inputs ) out_theta = self . theta ( inputs ) out_gamma = self . gamma ( inputs ) out_product1 = self . regul * tf . matmul ( out_theta , out_phi , transpose_b = True ) w = self . softmax ( out_product1 ) out_product2 = tf . matmul ( w , out_gamma ) return self . rho ( out_product2 ) get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , }, ) return config get_segmentation_module ( n_classes , backbone , architecture , filters , name ) Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required architecture str Choice of architecture for the segmentation head : base_oc or aspp_ocnet . required filters int Numbers of filters used in the segmentation head. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/ocnet.py def get_segmentation_module ( n_classes : int , backbone : tf . keras . Model , architecture : str , filters : int , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. architecture (str): Choice of architecture for the segmentation head : `base_oc` or `aspp_ocnet`. filters (int): Numbers of filters used in the segmentation head. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" fmap = backbone . outputs [ 1 ] if architecture == \"base_oc\" : fmap = conv_bn_relu ( fmap , filters = 1024 , kernel_size = 3 , name = \"pre_OCP_conv\" ) fmap = BaseOC ( filters = filters )( fmap ) elif architecture == \"aspp_ocnet\" : fmap = ASPP_OC ( filters = filters )( fmap ) fmap = Conv2D ( filters = n_classes , kernel_size = ( 3 , 3 ), padding = \"same\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = 1e-4 ), kernel_initializer = \"he_uniform\" , use_bias = False , )( fmap ) fmap = Activation ( \"softmax\" )( fmap ) out = UpSampling2D ( size = ( 8 , 8 ), interpolation = \"bilinear\" )( fmap ) return tf . keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"OCNet"},{"location":"models/heads/ocnet/#ocnet-object-context-for-semantic-segmentation","text":"Abstract In this paper, we address the semantic segmentation task with a new context aggregation scheme named object context, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise. We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices. To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling and atrous spatial pyramid pooling. We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff ArXiv link Architecture","title":"OCNet: Object Context for Semantic Segmentation"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ASPP_OC","text":"Description of ASPP_OC Attributes: Name Type Description dilation_rate type Dilations rates used for the Conv2D layers. isa_block1 type ISA2D layer. isa_block2 type ISA2D layer. isa_block3 type ISA2D layer. isa_block4 type ISA2D layer. concat type Concatenate layer. conv1 type Conv2D-BatchNormalization-ReLU layer. conv2 type Conv2D-BatchNormalization-ReLU layer. conv3 type Conv2D-BatchNormalization-ReLU layer. conv4 type Conv2D-BatchNormalization-ReLU layer. conv5 type Conv2D-BatchNormalization-ReLU layer. conv6 type Conv2D-BatchNormalization-ReLU layer. Inheritance tf.keras.layers.Layer:","title":"ASPP_OC"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ASPP_OC.__init__","text":"Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/object_context.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs , ) -> None : \"\"\"Initialization of the class. Args: filters (int): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . dilation_rate = [ 6 , 12 , 18 ] self . isa_block1 = ISA2D ( 8 , 8 ) self . isa_block2 = ISA2D ( 8 , 8 ) self . isa_block3 = ISA2D ( 8 , 8 ) self . isa_block4 = ISA2D ( 8 , 8 ) self . concat = Concatenate ( axis =- 1 ) self . conv1 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv2 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 0 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv3 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 1 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv4 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 3 , 3 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = self . dilation_rate [ 2 ], kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv5 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . conv6 = Sequential ( [ Conv2D ( filters = filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , dilation_rate = 1 , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], )","title":"__init__()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ASPP_OC.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/object_context.py def build ( self , input_shape ) -> None : _ , height , width , * _ = input_shape self . pooling = AveragePooling2D ( pool_size = ( height , width )) self . upsample = UpSampling2D ( size = ( height , width ), interpolation = \"bilinear\" )","title":"build()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ASPP_OC.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : fmap1 = self . conv1 ( inputs ) fmap1 = self . isa_block1 ( fmap1 ) fmap2 = self . conv2 ( inputs ) fmap2 = self . isa_block2 ( fmap2 ) fmap3 = self . conv3 ( inputs ) fmap3 = self . isa_block3 ( fmap3 ) fmap4 = self . conv4 ( inputs ) fmap4 = self . isa_block4 ( fmap4 ) fmap_pool = self . pooling ( inputs ) fmap_pool = self . conv5 ( fmap_pool ) fmap_pool = self . upsample ( fmap_pool ) fmap = self . concat ([ fmap_pool , fmap1 , fmap2 , fmap3 , fmap4 ]) return self . conv6 ( fmap )","title":"call()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ASPP_OC.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , \"dilation_rate\" : self . dilation_rate , }, ) return config","title":"get_config()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.BaseOC","text":"Description of Base_OC_Module Attributes: Name Type Description isa_block type ISA2D layer. concat type Concatenate layer. conv type Conv2D-BatchNormalization-ReLU layer. Inheritance tf.keras.layers.Layer:","title":"BaseOC"},{"location":"models/heads/ocnet/#src.model.layers.object_context.BaseOC.__init__","text":"Initialization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/object_context.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs ) -> None : \"\"\"Initialization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . isa_block = ISA2D ( 8 , 8 ) self . concat = Concatenate ( axis =- 1 ) self . conv = Sequential ( [ Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], )","title":"__init__()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.BaseOC.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : attention = self . isa_block ( inputs ) fmap = self . concat ([ attention , inputs ]) return self . conv ( fmap )","title":"call()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.BaseOC.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul }) return config","title":"get_config()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ISA2D","text":"Description of ISA2D Inheritance tf.keras.layers.Layer:","title":"ISA2D"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ISA2D.__init__","text":"Initilization of the class. Parameters: Name Type Description Default P_h int Number of partitions wanted for the height axis. required P_w int Number of partitions wanted for the width axis. required Source code in src/model/layers/object_context.py def __init__ ( self , P_h : int , P_w : int , * args , ** kwargs ) -> None : \"\"\"Initilization of the class. Args: P_h (int): Number of partitions wanted for the height axis. P_w (int): Number of partitions wanted for the width axis. \"\"\" super () . __init__ ( * args , ** kwargs ) self . P_h = P_h self . P_w = P_w","title":"__init__()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ISA2D.build","text":"Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Type Description Default input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). required Source code in src/model/layers/object_context.py def build ( self , input_shape ) -> None : self . attention1 = SelfAttention2D ( input_shape [ - 1 ]) self . attention2 = SelfAttention2D ( input_shape [ - 1 ])","title":"build()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ISA2D.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : _ , H , W , C = tf . keras . backend . int_shape ( inputs ) Q_h , Q_w = H // self . P_h , W // self . P_w # global relation fmap = tf . reshape ( inputs , [ - 1 , Q_h , self . P_h , Q_w , self . P_w , C ]) fmap = Permute (( 4 , 1 , 2 , 3 , 5 ))( fmap ) fmap = tf . reshape ( fmap , [ - 1 , Q_h , Q_w , C ]) fmap = self . attention1 ( fmap ) # local relation fmap = tf . reshape ( fmap , [ - 1 , self . P_h , self . P_w , Q_h , Q_w , C ]) fmap = Permute (( 3 , 4 , 1 , 2 , 5 ))( fmap ) fmap = tf . reshape ( fmap , [ - 1 , self . P_h , self . P_w , C ]) fmap = self . attention2 ( fmap ) # reshape fmap = tf . reshape ( fmap , [ - 1 , Q_h , Q_w , self . P_h , self . P_w , C ]) fmap = Permute (( 3 , 1 , 2 , 4 , 5 ))( fmap ) return tf . reshape ( fmap , [ - 1 , H , W , C ])","title":"call()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.ISA2D.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ({ \"P_h\" : self . P_h , \"P_w\" : self . P_w }) return config","title":"get_config()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.SelfAttention2D","text":"Description of SelfAttention2D Attributes: Name Type Description softmax type Softmax function. theta type [Conv2D-BatchNormalization-ReLU]x2 layer. phi type [Conv2D-BatchNormalization-ReLU]x2 layer. gamma type Conv2D layer. rho type Conv2D layer. Inheritance tf.keras.layers.Layer:","title":"SelfAttention2D"},{"location":"models/heads/ocnet/#src.model.layers.object_context.SelfAttention2D.__init__","text":"Initilization of the class. Parameters: Name Type Description Default filters int Number of filters in each Conv2D layers. required l2_regul float Value of the constraint used for the \\(L_2\\) regularization. Defaults to 1e-4. 0.0001 Source code in src/model/layers/object_context.py def __init__ ( self , filters : int , l2_regul : float = 1e-4 , * args , ** kwargs ) -> None : \"\"\"Initilization of the class. Args: filters (int, optional): Number of filters in each `Conv2D` layers. l2_regul (float, optional): Value of the constraint used for the $L_2$ regularization. Defaults to 1e-4. \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters = filters self . l2_regul = l2_regul self . regul = tf . sqrt ( 2 / self . filters ) self . softmax = tf . keras . layers . Activation ( \"softmax\" ) self . theta = Sequential ( [ Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . phi = Sequential ( [ Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ), BatchNormalization (), ReLU (), ], ) self . gamma = Conv2D ( filters // 2 , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), ) self . rho = Conv2D ( filters , kernel_size = ( 1 , 1 ), padding = \"same\" , use_bias = False , kernel_initializer = \"he_uniform\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = l2_regul ), )","title":"__init__()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.SelfAttention2D.call","text":"This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Type Description Default inputs Input tensor, or list/tuple of input tensors. required *args Additional positional arguments. Currently unused. required **kwargs Additional keyword arguments. Currently unused. required Returns: Type Description Tensor A tensor or list/tuple of tensors. Source code in src/model/layers/object_context.py def call ( self , inputs , training = None ) -> tf . Tensor : out_phi = self . phi ( inputs ) out_theta = self . theta ( inputs ) out_gamma = self . gamma ( inputs ) out_product1 = self . regul * tf . matmul ( out_theta , out_phi , transpose_b = True ) w = self . softmax ( out_product1 ) out_product2 = tf . matmul ( w , out_gamma ) return self . rho ( out_product2 )","title":"call()"},{"location":"models/heads/ocnet/#src.model.layers.object_context.SelfAttention2D.get_config","text":"Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Note that get_config() does not guarantee to return a fresh copy of dict every time it is called. The callers should make a copy of the returned dict if they want to modify it. Returns: Type Description Dict[str, Any] Python dictionary. Source code in src/model/layers/object_context.py def get_config ( self ) -> Dict [ str , Any ]: config = super () . get_config () config . update ( { \"filters\" : self . filters , \"l2_regularization\" : self . l2_regul , }, ) return config","title":"get_config()"},{"location":"models/heads/ocnet/#src.model.ocnet.get_segmentation_module","text":"Instantiate the segmentation head module for the segmentation task. Parameters: Name Type Description Default n_classes int Number of classes in the segmentation task. required backbone tf.keras.Model CNN used as backbone/feature extractor. required architecture str Choice of architecture for the segmentation head : base_oc or aspp_ocnet . required filters int Numbers of filters used in the segmentation head. required name str Name of the segmentation head module. required Returns: Type Description Model A semantic segmentation model. Source code in src/model/ocnet.py def get_segmentation_module ( n_classes : int , backbone : tf . keras . Model , architecture : str , filters : int , name : str , ) -> tf . keras . Model : \"\"\"Instantiate the segmentation head module for the segmentation task. Args: n_classes (int): Number of classes in the segmentation task. backbone (tf.keras.Model): CNN used as backbone/feature extractor. architecture (str): Choice of architecture for the segmentation head : `base_oc` or `aspp_ocnet`. filters (int): Numbers of filters used in the segmentation head. name (str): Name of the segmentation head module. Returns: A semantic segmentation model. \"\"\" fmap = backbone . outputs [ 1 ] if architecture == \"base_oc\" : fmap = conv_bn_relu ( fmap , filters = 1024 , kernel_size = 3 , name = \"pre_OCP_conv\" ) fmap = BaseOC ( filters = filters )( fmap ) elif architecture == \"aspp_ocnet\" : fmap = ASPP_OC ( filters = filters )( fmap ) fmap = Conv2D ( filters = n_classes , kernel_size = ( 3 , 3 ), padding = \"same\" , kernel_regularizer = tf . keras . regularizers . l2 ( l2 = 1e-4 ), kernel_initializer = \"he_uniform\" , use_bias = False , )( fmap ) fmap = Activation ( \"softmax\" )( fmap ) out = UpSampling2D ( size = ( 8 , 8 ), interpolation = \"bilinear\" )( fmap ) return tf . keras . Model ( inputs = [ backbone . inputs ], outputs = [ out ], name = name )","title":"get_segmentation_module()"},{"location":"training_loop/train/","text":"Boucle d'entra\u00eenement du mod\u00e8le train ( config ) Training loop. When you wrok with Hydra, all the logic of the funtion has to be contained in the function decorated by @hydra.main(...) . You can't define another function in same script asthe one containing the @hydra.main(...) decorated one. You can import functions from other scripts and use it in the @hydra.main(...) decorated function, that's fine, but you can't write other functions in the same script. Also, be careful to the root file : Hydra modify the root file. Path(__file__).parent.parent will return . , but this root will be located in the hydra folder, not the real root of the folder. This is to be expected , as the job of Hydra is to monitor and record each iteration of the training loop for reproducibility, hydra create a new folder for each training loop. The name of the folder were the configuration of the training loop has been configured to be related to the run name : If we have : run_name: ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S} The various files of this training loop will be stored in : dir: hydra/${now:%Y-%m-%d_%H-%M-%S} . To access to the root path of the folder, we use the hydra.utils.get_original_cwd() command. Parameters: Name Type Description Default config DictConfig The hydra configuration file used for the training loop. required Returns: Type Description Model A trained tf.keras model. Source code in src/train.py @logger . catch () @hydra . main ( config_path = \"../configs/\" , config_name = \"params.yaml\" ) def train ( config : DictConfig ) -> tf . keras . Model : \"\"\"Training loop. When you wrok with Hydra, all the logic of the funtion has to be contained in the function decorated by `@hydra.main(...)`. You can't define another function in same script asthe one containing the `@hydra.main(...)` decorated one. You can import functions from other scripts and use it in the `@hydra.main(...)` decorated function, that's fine, but you can't write other functions in the same script. Also, be careful to the root file : Hydra modify the root file. `Path(__file__).parent.parent` will return `.`, but this root will be located in the `hydra` folder, not the real root of the folder. **This is to be expected**, as the job of `Hydra` is to monitor and record each iteration of the training loop for reproducibility, `hydra` create a new folder for each training loop. The name of the folder were the configuration of the training loop has been configured to be related to the run name : If we have : * `run_name: ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S}` The various files of this training loop will be stored in : * `dir: hydra/${now:%Y-%m-%d_%H-%M-%S}`. To access to the root path of the folder, we use the `hydra.utils.get_original_cwd()` command. Args: config (DictConfig): The hydra configuration file used for the training loop. Returns: A trained tf.keras model. \"\"\" _ , repo_path = set_log_infos ( config ) mlflow . set_tracking_uri ( f \"file:// { repo_path } /mlruns\" ) mlflow . set_experiment ( config . mlflow . experiment_name ) set_seed ( config . prepare . seed ) logger . info ( \"Data loading\" ) logger . info ( f \"Root path of the folder : { repo_path } \" ) logger . info ( f \"MLFlow uri : { mlflow . get_tracking_uri () } \" ) with mlflow . start_run ( run_name = config . mlflow . run_name , ) as run : logger . info ( f \"Run infos : { run . info } \" ) mltensorflow . autolog ( every_n_iter = 1 ) mlflow . log_params ( flatten_omegaconf ( config )) logger . info ( \"Instantiate data pipeline.\" ) pipeline = instantiate ( config . pipeline ) ds = pipeline . create_train_dataset ( Path ( repo_path ) / config . datasets . prepared_dataset . train , config . datasets . params . batch_size , config . datasets . params . repetitions , config . datasets . params . augment , config . datasets . params . prefetch , ) ds_val = pipeline . create_test_dataset ( Path ( repo_path ) / config . datasets . prepared_dataset . val , config . datasets . params . batch_size , config . datasets . params . repetitions , config . datasets . params . prefetch , ) if config . lrd . activate : logger . info ( \"Found learning rate decay policy.\" ) lr = { \"learning_rate\" : instantiate ( config . lr_decay )} else : lr = { \"learning_rate\" : config . training . lr } logger . info ( \"Instantiate optimizer\" ) optimizer = instantiate ( config . optimizer , ** lr ) logger . info ( \"Instantiate loss\" ) loss = instantiate ( config . losses ) logger . info ( \"Instantiate metrics\" ) metric = instantiate ( config . metrics ) logger . info ( \"Instantiate model\" ) if config . start . from_saved_model : logger . info ( \"Start training back from a saved model.\" ) model = load_model ( Path ( repo_path ) / config . start . saved_model_dir ) else : logger . info ( \"Start training from scratch.\" ) backbone = { \"backbone\" : instantiate ( config . backbone )} model = instantiate ( config . segmentation_model , ** backbone ) logger . info ( \"Compiling model\" ) model . compile ( optimizer = optimizer , loss = loss , metrics = [ metric ], ) callbacks = [ tf . keras . callbacks . ModelCheckpoint ( f \"callback_ { config . mlflow . run_name } \" , monitor = \"val_mean_iou\" , mode = \"max\" , save_best_only = True , save_weights_only = False , ), ] logger . info ( \"Start training\" ) model . summary () model . fit ( ds , epochs = config . training . epochs , validation_data = ds_val , callbacks = callbacks , ) save_model ( model , f \" { config . mlflow . run_name } \" )","title":"Training loop"},{"location":"training_loop/train/#boucle-dentrainement-du-modele","text":"","title":"Boucle d'entra\u00eenement du mod\u00e8le"},{"location":"training_loop/train/#src.train.train","text":"Training loop. When you wrok with Hydra, all the logic of the funtion has to be contained in the function decorated by @hydra.main(...) . You can't define another function in same script asthe one containing the @hydra.main(...) decorated one. You can import functions from other scripts and use it in the @hydra.main(...) decorated function, that's fine, but you can't write other functions in the same script. Also, be careful to the root file : Hydra modify the root file. Path(__file__).parent.parent will return . , but this root will be located in the hydra folder, not the real root of the folder. This is to be expected , as the job of Hydra is to monitor and record each iteration of the training loop for reproducibility, hydra create a new folder for each training loop. The name of the folder were the configuration of the training loop has been configured to be related to the run name : If we have : run_name: ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S} The various files of this training loop will be stored in : dir: hydra/${now:%Y-%m-%d_%H-%M-%S} . To access to the root path of the folder, we use the hydra.utils.get_original_cwd() command. Parameters: Name Type Description Default config DictConfig The hydra configuration file used for the training loop. required Returns: Type Description Model A trained tf.keras model. Source code in src/train.py @logger . catch () @hydra . main ( config_path = \"../configs/\" , config_name = \"params.yaml\" ) def train ( config : DictConfig ) -> tf . keras . Model : \"\"\"Training loop. When you wrok with Hydra, all the logic of the funtion has to be contained in the function decorated by `@hydra.main(...)`. You can't define another function in same script asthe one containing the `@hydra.main(...)` decorated one. You can import functions from other scripts and use it in the `@hydra.main(...)` decorated function, that's fine, but you can't write other functions in the same script. Also, be careful to the root file : Hydra modify the root file. `Path(__file__).parent.parent` will return `.`, but this root will be located in the `hydra` folder, not the real root of the folder. **This is to be expected**, as the job of `Hydra` is to monitor and record each iteration of the training loop for reproducibility, `hydra` create a new folder for each training loop. The name of the folder were the configuration of the training loop has been configured to be related to the run name : If we have : * `run_name: ${backbone.backbone_name}_${segmentation_model.name}_${now:%Y-%m-%d_%H-%M-%S}` The various files of this training loop will be stored in : * `dir: hydra/${now:%Y-%m-%d_%H-%M-%S}`. To access to the root path of the folder, we use the `hydra.utils.get_original_cwd()` command. Args: config (DictConfig): The hydra configuration file used for the training loop. Returns: A trained tf.keras model. \"\"\" _ , repo_path = set_log_infos ( config ) mlflow . set_tracking_uri ( f \"file:// { repo_path } /mlruns\" ) mlflow . set_experiment ( config . mlflow . experiment_name ) set_seed ( config . prepare . seed ) logger . info ( \"Data loading\" ) logger . info ( f \"Root path of the folder : { repo_path } \" ) logger . info ( f \"MLFlow uri : { mlflow . get_tracking_uri () } \" ) with mlflow . start_run ( run_name = config . mlflow . run_name , ) as run : logger . info ( f \"Run infos : { run . info } \" ) mltensorflow . autolog ( every_n_iter = 1 ) mlflow . log_params ( flatten_omegaconf ( config )) logger . info ( \"Instantiate data pipeline.\" ) pipeline = instantiate ( config . pipeline ) ds = pipeline . create_train_dataset ( Path ( repo_path ) / config . datasets . prepared_dataset . train , config . datasets . params . batch_size , config . datasets . params . repetitions , config . datasets . params . augment , config . datasets . params . prefetch , ) ds_val = pipeline . create_test_dataset ( Path ( repo_path ) / config . datasets . prepared_dataset . val , config . datasets . params . batch_size , config . datasets . params . repetitions , config . datasets . params . prefetch , ) if config . lrd . activate : logger . info ( \"Found learning rate decay policy.\" ) lr = { \"learning_rate\" : instantiate ( config . lr_decay )} else : lr = { \"learning_rate\" : config . training . lr } logger . info ( \"Instantiate optimizer\" ) optimizer = instantiate ( config . optimizer , ** lr ) logger . info ( \"Instantiate loss\" ) loss = instantiate ( config . losses ) logger . info ( \"Instantiate metrics\" ) metric = instantiate ( config . metrics ) logger . info ( \"Instantiate model\" ) if config . start . from_saved_model : logger . info ( \"Start training back from a saved model.\" ) model = load_model ( Path ( repo_path ) / config . start . saved_model_dir ) else : logger . info ( \"Start training from scratch.\" ) backbone = { \"backbone\" : instantiate ( config . backbone )} model = instantiate ( config . segmentation_model , ** backbone ) logger . info ( \"Compiling model\" ) model . compile ( optimizer = optimizer , loss = loss , metrics = [ metric ], ) callbacks = [ tf . keras . callbacks . ModelCheckpoint ( f \"callback_ { config . mlflow . run_name } \" , monitor = \"val_mean_iou\" , mode = \"max\" , save_best_only = True , save_weights_only = False , ), ] logger . info ( \"Start training\" ) model . summary () model . fit ( ds , epochs = config . training . epochs , validation_data = ds_val , callbacks = callbacks , ) save_model ( model , f \" { config . mlflow . run_name } \" )","title":"train()"}]}