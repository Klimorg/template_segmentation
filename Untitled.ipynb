{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c96c43-745e-48e0-8767-c640065a379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def __init__(self, val2, *args, **kwargs):\n",
    "        self.val1 = 1\n",
    "        self.val2 = val2\n",
    "\n",
    "    def func_test(self, msg: str):\n",
    "        print(f\"salut {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c778270-063a-49aa-b525-e4d011e9fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = Test(val1=1, val2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d0a2f8-27b7-48dc-92fb-d8a621d8cd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffad335e-e1c8-464a-9cc1-4a842d0ff92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d4afb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salut moi\n"
     ]
    }
   ],
   "source": [
    "ts.func_test(\"moi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96075522-26d6-4280-aeda-1a0c9fefc5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperTest(Test):\n",
    "    def __init__(self, val3, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.val3 = val3\n",
    "\n",
    "    def func_retest(self, msg2):\n",
    "        self.func_test(msg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22ceb546-7ffa-4aa7-887f-7b22b0195fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = SuperTest(val2=2, val3=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c6fa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f91d2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salut remoi\n"
     ]
    }
   ],
   "source": [
    "ts.func_retest(\"remoi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5a9ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class BasePipeline(object):\n",
    "    \"\"\"\n",
    "    Base class used to create tensor datasets for TensorFlow.\n",
    "\n",
    "    Inheritance:\n",
    "        object: The base class of the class hierarchy, used only to enforce WPS306.\n",
    "        See https://wemake-python-stylegui.de/en/latest/pages/usage/violations/consistency.html#consistency.\n",
    "\n",
    "    Args:\n",
    "        n_classes (int): Number of classes in the dataset.\n",
    "        img_shape (Tuple[int,int,int]): Dimension of the image, format is (H,W,C).\n",
    "        random_seed (int): Fixed random seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int,\n",
    "        img_shape: Tuple[int, int, int],\n",
    "        random_seed: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization of the class Tensorize.\n",
    "\n",
    "        Initialize the class, the number of classes in the datasets, the shape of the\n",
    "        images and the random seed.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.random_seed = random_seed\n",
    "        self.AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    def load_images(self, data_frame: pd.DataFrame, column_name: str) -> List[str]:\n",
    "        \"\"\"Load the images as a list.\n",
    "\n",
    "        Take the dataframe containing the observations and the masks and the return the\n",
    "        column containing the observations as a list.\n",
    "\n",
    "        Args:\n",
    "            data_frame (pd.DataFrame): Dataframe containing the dataset.\n",
    "            column_name (str): The name of the column containing the observations.\n",
    "\n",
    "        Returns:\n",
    "            The list of observations deduced from the dataframe.\n",
    "        \"\"\"\n",
    "        return data_frame[column_name].tolist()\n",
    "\n",
    "    @tf.function\n",
    "    def parse_image_and_mask(\n",
    "        self,\n",
    "        image: str,\n",
    "        mask: str,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Transform image and mask.\n",
    "\n",
    "        Parse image and mask to go from path to a resized np.ndarray.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The path of the image to parse.\n",
    "            mask (str): The mask of the image.\n",
    "\n",
    "        Returns:\n",
    "            A np.ndarray corresponding to the image and the corresponding one-hot mask.\n",
    "        \"\"\"\n",
    "        resized_dims = [self.img_shape[0], self.img_shape[1]]\n",
    "        # convert the mask to one-hot encoding\n",
    "        # decode image\n",
    "        image = tf.io.read_file(image)\n",
    "        # Don't use tf.image.decode_image,\n",
    "        # or the output shape will be undefined\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "        # This will convert to float values in [0, 1]\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize(\n",
    "            image,\n",
    "            resized_dims,\n",
    "            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "        )\n",
    "\n",
    "        mask = tf.io.read_file(mask)\n",
    "        # Don't use tf.image.decode_image,\n",
    "        # or the output shape will be undefined\n",
    "        mask = tf.io.decode_png(mask, channels=1)\n",
    "        mask = tf.image.resize(\n",
    "            mask,\n",
    "            resized_dims,\n",
    "            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "        )\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def train_preprocess(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Augmentation preprocess, if needed.\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): The image to augment.\n",
    "            mask (np.ndarray): The corresponding mask.\n",
    "\n",
    "        Returns:\n",
    "            The augmented pair.\n",
    "        \"\"\"\n",
    "\n",
    "        aug = A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.Transpose(p=0.5),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        augmented = aug(image=image, mask=mask)\n",
    "\n",
    "        image = augmented[\"image\"]\n",
    "        mask = augmented[\"mask\"]\n",
    "\n",
    "        image = tf.cast(x=image, dtype=tf.float32)\n",
    "        mask = tf.cast(x=mask, dtype=tf.float32)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    @tf.function\n",
    "    def apply_augments(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        mask: np.ndarray,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply augmentation (roations, transposition, flips), if needed.\n",
    "\n",
    "        Args:\n",
    "            image (np.ndarray): A numpy array representing an image of the dataset.\n",
    "            mask (np.ndarray): A numpy array representing a mask of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            An augmented pair (image, mask).\n",
    "        \"\"\"\n",
    "\n",
    "        image, mask = tf.numpy_function(\n",
    "            func=self.train_preprocess,\n",
    "            inp=[image, mask],\n",
    "            Tout=[tf.float32, tf.float32],\n",
    "        )\n",
    "\n",
    "        img_shape = [self.img_shape[0], self.img_shape[1], 3]\n",
    "        mask_shape = [self.img_shape[0], self.img_shape[1], 1]\n",
    "\n",
    "        image = tf.ensure_shape(image, shape=img_shape)\n",
    "        mask = tf.ensure_shape(mask, shape=mask_shape)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def create_test_dataset(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        batch: int,\n",
    "        repet: int,\n",
    "        prefetch: int,\n",
    "    ) -> tf.data.Dataset:\n",
    "        \"\"\"Creation of a tensor dataset for TensorFlow.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path where the csv file containing the dataframe is\n",
    "                located.\n",
    "            batch (int): Batch size, usually 32.\n",
    "            repet (int): How many times the dataset has to be repeated.\n",
    "            prefetch (int): How many batch the CPU has to prepare in advance for the\n",
    "                GPU.\n",
    "            augment (bool): Does the dataset has to be augmented or no.\n",
    "\n",
    "        Returns:\n",
    "            A batch of observations and masks.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(data_path)\n",
    "        features = self.load_images(data_frame=df, column_name=\"filename\")\n",
    "        masks = self.load_images(data_frame=df, column_name=\"mask\")\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((features, masks))\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(len(features), seed=self.random_seed)\n",
    "        dataset = dataset.repeat(repet)\n",
    "        dataset = dataset.map(\n",
    "            self.parse_image_and_mask,\n",
    "            num_parallel_calls=self.AUTOTUNE,\n",
    "        )\n",
    "        dataset = dataset.batch(batch)\n",
    "        return dataset.prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df6304aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(BasePipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization of the class Tensorize.\n",
    "\n",
    "        Initialize the class, the number of classes in the datasets, the shape of the\n",
    "        images and the random seed.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def create_train_dataset(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        batch: int,\n",
    "        repet: int,\n",
    "        prefetch: int,\n",
    "        augment: bool,\n",
    "    ) -> tf.data.Dataset:\n",
    "        \"\"\"Creation of a tensor dataset for TensorFlow.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path where the csv file containing the dataframe is\n",
    "                located.\n",
    "            batch (int): Batch size, usually 32.\n",
    "            repet (int): How many times the dataset has to be repeated.\n",
    "            prefetch (int): How many batch the CPU has to prepare in advance for the\n",
    "                GPU.\n",
    "            augment (bool): Does the dataset has to be augmented or no.\n",
    "\n",
    "        Returns:\n",
    "            A batch of observations and masks.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(data_path)\n",
    "        features = self.load_images(data_frame=df, column_name=\"filename\")\n",
    "        masks = self.load_images(data_frame=df, column_name=\"mask\")\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((features, masks))\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(len(features), seed=self.random_seed)\n",
    "        dataset = dataset.repeat(repet)\n",
    "        dataset = dataset.map(\n",
    "            self.parse_image_and_mask,\n",
    "            num_parallel_calls=self.AUTOTUNE,\n",
    "        )\n",
    "        if augment:\n",
    "            dataset = dataset.map(self.apply_augments, num_parallel_calls=self.AUTOTUNE)\n",
    "        dataset = dataset.batch(batch)\n",
    "        return dataset.prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78d72275",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BaseDataset(n_classes=4, img_shape=[224, 224, 3], random_seed=42)\n",
    "\n",
    "ds_train = ds.create_train_dataset(\n",
    "    data_path=\"datas/prepared_dataset/test.csv\",\n",
    "    batch=9,\n",
    "    repet=1,\n",
    "    prefetch=1,\n",
    "    augment=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99c5f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 224, 224, None), (None, 224, 224, 1)), types: (tf.float32, tf.uint8)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c450171",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/workspaces/segmentation_ecco/datas/raw_dataset/ML/masks/cor1_mask_384_640.png; No such file or directory\n\t [[{{node PartitionedCall/ReadFile_1}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d735553c2097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Let's preview 9 samples from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /workspaces/segmentation_ecco/datas/raw_dataset/ML/masks/cor1_mask_384_640.png; No such file or directory\n\t [[{{node PartitionedCall/ReadFile_1}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's preview 9 samples from the dataset\n",
    "image_batch, label_batch = next(iter(ds_train))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.title(np.max(label_batch[i]))\n",
    "    plt.imshow(image_batch[i])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e56733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
